{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Introduction: ML System Components (Beyond the Model)\n",
    "\n",
    "---\n",
    "\n",
    "## What the Chapter Says\n",
    "\n",
    "A production ML system is **far more than just the model**. The chapter explicitly lists these components:\n",
    "\n",
    "- **Data collection** + verification + feature extraction\n",
    "- **Evaluation pipeline**\n",
    "- **Monitoring**\n",
    "- **Serving infrastructure**\n",
    "- **Process management tools**\n",
    "- **Resource management**\n",
    "- **Analysis tools**\n",
    "- **Configuration**\n",
    "\n",
    "The model (\"ML Code\") is often a tiny fraction of the overall system.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta Interview Signal\n",
    "\n",
    "| Level | Expectations |\n",
    "|-------|-------------|\n",
    "| **E5** | Can articulate the full system beyond the model. Understands why each component exists. Can draw the component diagram and explain data flow. |\n",
    "| **E6** | Discusses **scale implications** for each component (e.g., monitoring at 1B users). Identifies **failure modes** (stale features, evaluation drift). Proposes **iteration velocity** improvements (faster retraining, automated config). |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Diagram: Production ML System Components\n",
    "\n",
    "This diagram matches the chapter's conceptual anchor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "ax.set_title('Production ML System Components (Chapter Diagram)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Component boxes - matching the chapter's component list\n",
    "components = [\n",
    "    # (x, y, width, height, label, color)\n",
    "    (1, 7.5, 2.5, 1.2, 'Data\\nCollection', '#E3F2FD'),\n",
    "    (4, 7.5, 2.5, 1.2, 'Data\\nVerification', '#E3F2FD'),\n",
    "    (7, 7.5, 2.5, 1.2, 'Feature\\nExtraction', '#E3F2FD'),\n",
    "    (10.5, 7.5, 2.5, 1.2, 'Feature\\nStore', '#E3F2FD'),\n",
    "    \n",
    "    (5.5, 5, 3, 1.5, 'ML CODE', '#FFCDD2'),  # The model - small box!\n",
    "    \n",
    "    (1, 2.5, 2.5, 1.2, 'Serving\\nInfrastructure', '#C8E6C9'),\n",
    "    (4, 2.5, 2.5, 1.2, 'Monitoring', '#C8E6C9'),\n",
    "    (7, 2.5, 2.5, 1.2, 'Evaluation\\nPipeline', '#C8E6C9'),\n",
    "    (10.5, 2.5, 2.5, 1.2, 'Analysis\\nTools', '#C8E6C9'),\n",
    "    \n",
    "    (1, 0.5, 2.5, 1.2, 'Configuration', '#FFF9C4'),\n",
    "    (4, 0.5, 2.5, 1.2, 'Process\\nManagement', '#FFF9C4'),\n",
    "    (7, 0.5, 2.5, 1.2, 'Resource\\nManagement', '#FFF9C4'),\n",
    "]\n",
    "\n",
    "for (x, y, w, h, label, color) in components:\n",
    "    rect = mpatches.FancyBboxPatch((x, y), w, h, boxstyle='round,pad=0.05',\n",
    "                                    facecolor=color, edgecolor='black', linewidth=1.5)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x + w/2, y + h/2, label, ha='center', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Arrows showing data flow\n",
    "arrow_style = dict(arrowstyle='->', color='gray', lw=1.5)\n",
    "ax.annotate('', xy=(4, 8.1), xytext=(3.5, 8.1), arrowprops=arrow_style)\n",
    "ax.annotate('', xy=(7, 8.1), xytext=(6.5, 8.1), arrowprops=arrow_style)\n",
    "ax.annotate('', xy=(10.5, 8.1), xytext=(9.5, 8.1), arrowprops=arrow_style)\n",
    "ax.annotate('', xy=(7, 6.5), xytext=(7, 7.5), arrowprops=arrow_style)  # Features to ML Code\n",
    "ax.annotate('', xy=(7, 3.7), xytext=(7, 5), arrowprops=arrow_style)    # ML Code to Evaluation\n",
    "\n",
    "# Legend\n",
    "ax.text(0.5, 9.5, 'Blue = Data Pipeline | Red = ML Code (tiny!) | Green = Ops | Yellow = Management',\n",
    "        fontsize=10, style='italic')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASCII Version (for quick whiteboard recall)\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                        PRODUCTION ML SYSTEM                                  │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                              │\n",
    "│   ┌──────────────┐   ┌──────────────┐   ┌──────────────┐   ┌─────────────┐  │\n",
    "│   │    Data      │──▶│    Data      │──▶│   Feature    │──▶│   Feature   │  │\n",
    "│   │  Collection  │   │ Verification │   │  Extraction  │   │    Store    │  │\n",
    "│   └──────────────┘   └──────────────┘   └──────────────┘   └─────────────┘  │\n",
    "│                                               │                              │\n",
    "│                                               ▼                              │\n",
    "│                                    ┌─────────────────┐                       │\n",
    "│                                    │    ML CODE      │  ◀── Small fraction!  │\n",
    "│                                    └─────────────────┘                       │\n",
    "│                                               │                              │\n",
    "│                                               ▼                              │\n",
    "│   ┌──────────────┐   ┌──────────────┐   ┌──────────────┐   ┌─────────────┐  │\n",
    "│   │   Serving    │◀──│  Monitoring  │◀──│  Evaluation  │──▶│  Analysis   │  │\n",
    "│   │Infrastructure│   │              │   │   Pipeline   │   │    Tools    │  │\n",
    "│   └──────────────┘   └──────────────┘   └──────────────┘   └─────────────┘  │\n",
    "│                                                                              │\n",
    "│   ┌──────────────┐   ┌──────────────┐   ┌──────────────┐                     │\n",
    "│   │Configuration │   │   Process    │   │   Resource   │                     │\n",
    "│   │              │   │  Management  │   │  Management  │                     │\n",
    "│   └──────────────┘   └──────────────┘   └──────────────┘                     │\n",
    "│                                                                              │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hands-On: Simulating Component Interactions\n",
    "\n",
    "Let's build a minimal simulation of the component pipeline for a **Feed Ranking** system (like/dislike prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# ============================================================\n",
    "# COMPONENT 1: Data Collection\n",
    "# ============================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPONENT: Data Collection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "n_interactions = 10000\n",
    "\n",
    "raw_data = pd.DataFrame({\n",
    "    'user_id': np.random.randint(1, 1001, n_interactions),\n",
    "    'post_id': np.random.randint(1, 5001, n_interactions),\n",
    "    'action': np.random.choice(['like', 'dislike', 'view', 'share', None], \n",
    "                               n_interactions, p=[0.15, 0.05, 0.6, 0.05, 0.15]),\n",
    "    'timestamp': [datetime.now() - timedelta(hours=np.random.randint(0, 720)) \n",
    "                  for _ in range(n_interactions)],\n",
    "    'device': np.random.choice(['iOS', 'Android', 'Web', None], n_interactions, p=[0.4, 0.35, 0.2, 0.05])\n",
    "})\n",
    "\n",
    "print(f\"Collected {len(raw_data)} raw interactions\")\n",
    "print(f\"Sample:\\n{raw_data.head()}\")\n",
    "print(f\"\\nMissing values:\\n{raw_data.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPONENT 2: Data Verification\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPONENT: Data Verification\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def verify_data(df):\n",
    "    \"\"\"Check data quality - missing values, duplicates, anomalies\"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    # Check for nulls\n",
    "    null_counts = df.isnull().sum()\n",
    "    for col, count in null_counts.items():\n",
    "        if count > 0:\n",
    "            issues.append(f\"Column '{col}' has {count} null values ({count/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    dup_count = df.duplicated().sum()\n",
    "    if dup_count > 0:\n",
    "        issues.append(f\"Found {dup_count} duplicate rows\")\n",
    "    \n",
    "    return issues\n",
    "\n",
    "verification_issues = verify_data(raw_data)\n",
    "print(\"Verification Report:\")\n",
    "for issue in verification_issues:\n",
    "    print(f\"  [!] {issue}\")\n",
    "\n",
    "# Clean data\n",
    "clean_data = raw_data.dropna(subset=['action']).copy()\n",
    "clean_data['device'] = clean_data['device'].fillna('Unknown')\n",
    "print(f\"\\nAfter cleaning: {len(clean_data)} rows (dropped {len(raw_data) - len(clean_data)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPONENT 3: Feature Extraction\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPONENT: Feature Extraction\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# User-level features\n",
    "user_features = clean_data.groupby('user_id').agg({\n",
    "    'post_id': 'count',\n",
    "    'action': lambda x: (x == 'like').sum() / len(x),  # like rate\n",
    "}).rename(columns={'post_id': 'total_actions', 'action': 'like_rate'})\n",
    "\n",
    "# Post-level features  \n",
    "post_features = clean_data.groupby('post_id').agg({\n",
    "    'user_id': 'nunique',\n",
    "    'action': lambda x: (x == 'like').sum(),\n",
    "}).rename(columns={'user_id': 'unique_viewers', 'action': 'total_likes'})\n",
    "\n",
    "print(f\"Extracted features for {len(user_features)} users and {len(post_features)} posts\")\n",
    "print(f\"\\nUser features sample:\\n{user_features.head()}\")\n",
    "print(f\"\\nPost features sample:\\n{post_features.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPONENT 4: Feature Store (simulated)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPONENT: Feature Store\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class SimpleFeatureStore:\n",
    "    \"\"\"Simulates a feature store with versioning\"\"\"\n",
    "    def __init__(self):\n",
    "        self.features = {}\n",
    "        self.metadata = {}\n",
    "    \n",
    "    def register(self, name, df, description):\n",
    "        self.features[name] = df\n",
    "        self.metadata[name] = {\n",
    "            'description': description,\n",
    "            'shape': df.shape,\n",
    "            'created_at': datetime.now(),\n",
    "            'version': 1\n",
    "        }\n",
    "        print(f\"Registered: {name} - {df.shape}\")\n",
    "    \n",
    "    def get(self, name):\n",
    "        return self.features.get(name)\n",
    "    \n",
    "    def list_features(self):\n",
    "        for name, meta in self.metadata.items():\n",
    "            print(f\"  {name}: {meta['description']} | shape={meta['shape']}\")\n",
    "\n",
    "feature_store = SimpleFeatureStore()\n",
    "feature_store.register('user_engagement', user_features, 'User-level engagement metrics')\n",
    "feature_store.register('post_popularity', post_features, 'Post-level popularity metrics')\n",
    "\n",
    "print(\"\\nFeature Store Contents:\")\n",
    "feature_store.list_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPONENT 5: ML Code (the small part!)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPONENT: ML Code (The Small Part!)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create training data: predict like vs not-like\n",
    "training_data = clean_data[clean_data['action'].isin(['like', 'dislike', 'view'])].copy()\n",
    "training_data['label'] = (training_data['action'] == 'like').astype(int)\n",
    "\n",
    "# Merge features\n",
    "training_data = training_data.merge(user_features, on='user_id', how='left')\n",
    "training_data = training_data.merge(post_features, on='post_id', how='left')\n",
    "training_data = training_data.fillna(0)\n",
    "\n",
    "X = training_data[['total_actions', 'like_rate', 'unique_viewers', 'total_likes']]\n",
    "y = training_data['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Model trained on {len(X_train)} samples\")\n",
    "print(f\"Test accuracy: {model.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPONENT 6: Evaluation Pipeline\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPONENT: Evaluation Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "metrics = {\n",
    "    'precision': precision_score(y_test, y_pred),\n",
    "    'recall': recall_score(y_test, y_pred),\n",
    "    'f1': f1_score(y_test, y_pred),\n",
    "    'roc_auc': roc_auc_score(y_test, y_proba)\n",
    "}\n",
    "\n",
    "print(\"Offline Evaluation Metrics:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPONENT 7: Monitoring\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPONENT: Monitoring\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class ModelMonitor:\n",
    "    \"\"\"Simulates production monitoring\"\"\"\n",
    "    def __init__(self, baseline_metrics):\n",
    "        self.baseline = baseline_metrics\n",
    "        self.alerts = []\n",
    "    \n",
    "    def check_metrics(self, current_metrics, threshold=0.1):\n",
    "        \"\"\"Alert if metrics drift beyond threshold\"\"\"\n",
    "        for metric, current in current_metrics.items():\n",
    "            baseline = self.baseline.get(metric, current)\n",
    "            drift = abs(current - baseline) / baseline if baseline > 0 else 0\n",
    "            status = \"OK\" if drift < threshold else \"ALERT\"\n",
    "            print(f\"  {metric}: baseline={baseline:.3f}, current={current:.3f}, drift={drift:.1%} [{status}]\")\n",
    "            if drift >= threshold:\n",
    "                self.alerts.append(f\"{metric} drifted by {drift:.1%}\")\n",
    "\n",
    "monitor = ModelMonitor(metrics)\n",
    "\n",
    "# Simulate slightly degraded metrics\n",
    "simulated_current = {k: v * np.random.uniform(0.85, 1.05) for k, v in metrics.items()}\n",
    "print(\"Monitoring Check:\")\n",
    "monitor.check_metrics(simulated_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPONENT 8: Serving Infrastructure (simulated)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPONENT: Serving Infrastructure\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import time\n",
    "\n",
    "class ModelServer:\n",
    "    \"\"\"Simulates serving infrastructure\"\"\"\n",
    "    def __init__(self, model, feature_store):\n",
    "        self.model = model\n",
    "        self.feature_store = feature_store\n",
    "        self.request_count = 0\n",
    "        self.latencies = []\n",
    "    \n",
    "    def predict(self, user_id, post_id):\n",
    "        start = time.time()\n",
    "        \n",
    "        # Fetch features (simulated lookup)\n",
    "        user_feats = self.feature_store.get('user_engagement')\n",
    "        post_feats = self.feature_store.get('post_popularity')\n",
    "        \n",
    "        # Get features or defaults\n",
    "        u = user_feats.loc[user_id] if user_id in user_feats.index else pd.Series({'total_actions': 0, 'like_rate': 0.5})\n",
    "        p = post_feats.loc[post_id] if post_id in post_feats.index else pd.Series({'unique_viewers': 0, 'total_likes': 0})\n",
    "        \n",
    "        features = [[u['total_actions'], u['like_rate'], p['unique_viewers'], p['total_likes']]]\n",
    "        prob = self.model.predict_proba(features)[0][1]\n",
    "        \n",
    "        latency_ms = (time.time() - start) * 1000\n",
    "        self.latencies.append(latency_ms)\n",
    "        self.request_count += 1\n",
    "        \n",
    "        return {'probability': prob, 'latency_ms': latency_ms}\n",
    "\n",
    "server = ModelServer(model, feature_store)\n",
    "\n",
    "# Simulate requests\n",
    "print(\"Simulating 5 prediction requests:\")\n",
    "for i in range(5):\n",
    "    result = server.predict(user_id=np.random.randint(1, 100), post_id=np.random.randint(1, 500))\n",
    "    print(f\"  Request {i+1}: prob={result['probability']:.3f}, latency={result['latency_ms']:.2f}ms\")\n",
    "\n",
    "print(f\"\\nTotal requests served: {server.request_count}\")\n",
    "print(f\"Average latency: {np.mean(server.latencies):.2f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPONENT 9: Configuration & Resource Management (overview)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPONENT: Configuration & Resource Management\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "system_config = {\n",
    "    'model': {\n",
    "        'version': 'v1.2.3',\n",
    "        'type': 'LogisticRegression',\n",
    "        'features': ['total_actions', 'like_rate', 'unique_viewers', 'total_likes']\n",
    "    },\n",
    "    'serving': {\n",
    "        'max_latency_ms': 50,\n",
    "        'replicas': 10,\n",
    "        'batch_size': 1\n",
    "    },\n",
    "    'training': {\n",
    "        'schedule': 'daily',\n",
    "        'data_lookback_days': 30\n",
    "    },\n",
    "    'monitoring': {\n",
    "        'alert_threshold': 0.1,\n",
    "        'metrics': ['precision', 'recall', 'latency_p99']\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"System Configuration:\")\n",
    "import json\n",
    "print(json.dumps(system_config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Framework Steps (Chapter Order)\n",
    "\n",
    "The chapter defines exactly this order:\n",
    "\n",
    "1. **Clarifying requirements**\n",
    "2. **Framing the problem as an ML task**\n",
    "3. **Data preparation**\n",
    "4. **Model development**\n",
    "5. **Evaluation**\n",
    "6. **Deployment and serving**\n",
    "7. **Monitoring and infrastructure**\n",
    "\n",
    "Each subsequent notebook will cover one or more of these steps in depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual representation of the framework\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.axis('off')\n",
    "\n",
    "steps = [\n",
    "    '1. Clarify\\nRequirements',\n",
    "    '2. Frame as\\nML Task',\n",
    "    '3. Data\\nPreparation',\n",
    "    '4. Model\\nDevelopment',\n",
    "    '5. Evaluation',\n",
    "    '6. Deployment\\n& Serving',\n",
    "    '7. Monitoring\\n& Infra'\n",
    "]\n",
    "\n",
    "colors = ['#BBDEFB', '#C8E6C9', '#FFF9C4', '#FFCCBC', '#E1BEE7', '#B2DFDB', '#F8BBD9']\n",
    "\n",
    "for i, (step, color) in enumerate(zip(steps, colors)):\n",
    "    x = i * 1.6 + 0.5\n",
    "    rect = mpatches.FancyBboxPatch((x, 1), 1.4, 2, boxstyle='round,pad=0.05',\n",
    "                                    facecolor=color, edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x + 0.7, 2, step, ha='center', va='center', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    if i < len(steps) - 1:\n",
    "        ax.annotate('', xy=(x + 1.5, 2), xytext=(x + 1.4, 2),\n",
    "                   arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
    "\n",
    "ax.set_xlim(0, 12)\n",
    "ax.set_ylim(0, 4)\n",
    "ax.set_title('ML System Design Framework (Chapter Order)', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Tradeoffs (Chapter-Aligned)\n",
    "\n",
    "| Component | Tradeoff | E5 Understanding | E6 Addition |\n",
    "|-----------|----------|------------------|-------------|\n",
    "| Data Collection | More data vs. cost/latency | Know that more data improves models | Consider data freshness SLAs, ingestion at petabyte scale |\n",
    "| Feature Store | Freshness vs. compute | Batch features are cheaper | Real-time features for time-sensitive predictions |\n",
    "| Serving | Latency vs. accuracy | Simpler models = faster | Model compression, caching strategies at scale |\n",
    "| Monitoring | Coverage vs. alert fatigue | Track key metrics | Define actionable alerts, avoid noise |\n",
    "| Configuration | Flexibility vs. complexity | Version models and configs | Gradual rollouts, feature flags for safety |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Meta Interview Signal (Detailed)\n",
    "\n",
    "### E5 Answer Expectations\n",
    "\n",
    "- Can draw the full system diagram from memory\n",
    "- Explains why each component exists (not just what it does)\n",
    "- Understands the data flow from collection → serving\n",
    "- Articulates that \"ML Code\" is small compared to the infrastructure\n",
    "\n",
    "### E6 Additions\n",
    "\n",
    "- **Scale**: \"At Meta scale with billions of users, the feature store alone handles X QPS...\"\n",
    "- **Failure modes**: \"If the feature pipeline is delayed, we fall back to cached features from the last successful run\"\n",
    "- **Iteration velocity**: \"We can ship a new model version within hours by decoupling training from serving\"\n",
    "- **Feedback loops**: \"User engagement signals flow back into training data, creating a flywheel\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Interview Drills\n",
    "\n",
    "### Drill 1: Whiteboard the System\n",
    "Draw the production ML system diagram from memory. Include all 9 components mentioned in the chapter.\n",
    "\n",
    "### Drill 2: Component Failure Scenarios\n",
    "For each component, describe:\n",
    "- What happens if it fails?\n",
    "- What's the fallback?\n",
    "- How do you detect the failure?\n",
    "\n",
    "### Drill 3: Scale Discussion\n",
    "Pick the Feed Ranking use case. Walk through each component and explain how it changes at:\n",
    "- 1M users\n",
    "- 100M users  \n",
    "- 1B users\n",
    "\n",
    "### Drill 4: The \"ML Code is Small\" Argument\n",
    "Your interviewer challenges: \"If ML code is so small, why do we hire ML engineers?\"\n",
    "Prepare a 2-minute response.\n",
    "\n",
    "### Drill 5: Framework Steps\n",
    "Recite the 7 framework steps in order. For each, give a one-sentence summary of its purpose."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

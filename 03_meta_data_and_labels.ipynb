{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta ML System Design: Data & Labels\n",
    "\n",
    "**Target Level:** Meta E5 â†’ E6  \n",
    "**Core Skill:** Understanding where labels come from and their biases  \n",
    "**Interview Focus:** Implicit feedback, label quality, data collection strategies\n",
    "\n",
    "---\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "At Meta, **you rarely have clean labels.** Almost all ML systems learn from implicit user behavior:\n",
    "\n",
    "- Clicks, not \"this was relevant\"\n",
    "- Watch time, not \"I enjoyed this\"\n",
    "- Hides, not \"this was bad\"\n",
    "\n",
    "Understanding the gap between **what you observe** and **what you want to learn** is crucial for building systems that actually work.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Label Sources at Meta Scale\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                           LABEL SOURCE HIERARCHY                            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "EXPLICIT LABELS (Rare, High Quality)\n",
    "â”œâ”€â”€ User ratings (1-5 stars)\n",
    "â”œâ”€â”€ Surveys (\"Was this helpful?\")\n",
    "â”œâ”€â”€ Human annotation (labelers)\n",
    "â””â”€â”€ User reports (spam, harmful content)\n",
    "         â”‚\n",
    "         â”‚  â†“ Decreasing quality, increasing volume\n",
    "         â–¼\n",
    "IMPLICIT LABELS (Common, Noisy)\n",
    "â”œâ”€â”€ Strong signals\n",
    "â”‚   â”œâ”€â”€ Purchase / conversion\n",
    "â”‚   â”œâ”€â”€ Share to friends\n",
    "â”‚   â”œâ”€â”€ Save for later\n",
    "â”‚   â””â”€â”€ Long dwell time (>30s)\n",
    "â”‚\n",
    "â”œâ”€â”€ Medium signals\n",
    "â”‚   â”œâ”€â”€ Like / reaction\n",
    "â”‚   â”œâ”€â”€ Comment\n",
    "â”‚   â”œâ”€â”€ Follow author\n",
    "â”‚   â””â”€â”€ Complete video\n",
    "â”‚\n",
    "â”œâ”€â”€ Weak signals\n",
    "â”‚   â”œâ”€â”€ Click\n",
    "â”‚   â”œâ”€â”€ Scroll past slowly\n",
    "â”‚   â””â”€â”€ Short dwell time\n",
    "â”‚\n",
    "â””â”€â”€ Negative signals\n",
    "    â”œâ”€â”€ Hide post\n",
    "    â”œâ”€â”€ Unfollow author\n",
    "    â”œâ”€â”€ Mark as spam\n",
    "    â””â”€â”€ Report content\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Implicit Feedback Challenge\n",
    "\n",
    "| What You Observe | What It Might Mean | What It Might NOT Mean |\n",
    "|------------------|-------------------|------------------------|\n",
    "| User clicked | Interested in content | Actually liked it |\n",
    "| User didn't click | Not interested | Didn't see it (scrolled past) |\n",
    "| User watched 10s | Engaged with video | Fell asleep, phone in pocket |\n",
    "| User liked | Positive reaction | Social obligation, habit |\n",
    "| User hid post | Negative reaction | Saw it too many times |\n",
    "\n",
    "**Key insight:** Every implicit label has false positives and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Demonstrate the noise in implicit labels\n",
    "\n",
    "np.random.seed(42)\n",
    "n_items = 1000\n",
    "\n",
    "# True relevance (what we wish we knew)\n",
    "true_relevance = np.random.uniform(0, 1, n_items)\n",
    "\n",
    "# Different label types with different noise characteristics\n",
    "def generate_implicit_label(true_relevance, noise_level, threshold=0.5):\n",
    "    \"\"\"Generate noisy binary label from true relevance.\"\"\"\n",
    "    noisy_signal = true_relevance + np.random.normal(0, noise_level, len(true_relevance))\n",
    "    return (noisy_signal > threshold).astype(int)\n",
    "\n",
    "labels = pd.DataFrame({\n",
    "    'true_relevance': true_relevance,\n",
    "    'click': generate_implicit_label(true_relevance, noise_level=0.3, threshold=0.3),  # High noise, low threshold\n",
    "    'like': generate_implicit_label(true_relevance, noise_level=0.2, threshold=0.5),   # Medium noise\n",
    "    'share': generate_implicit_label(true_relevance, noise_level=0.15, threshold=0.7), # Low noise, high threshold\n",
    "    'purchase': generate_implicit_label(true_relevance, noise_level=0.1, threshold=0.8), # Very low noise, very high threshold\n",
    "})\n",
    "\n",
    "# Show correlation with true relevance\n",
    "print(\"Correlation of different signals with TRUE relevance:\\n\")\n",
    "for col in ['click', 'like', 'share', 'purchase']:\n",
    "    corr = np.corrcoef(labels['true_relevance'], labels[col])[0, 1]\n",
    "    volume = labels[col].mean()\n",
    "    print(f\"  {col:12s}: correlation = {corr:.3f}, volume = {volume:.1%}\")\n",
    "\n",
    "print(\"\\nâ†’ Stronger signals have higher correlation but lower volume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the signal-volume tradeoff\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(14, 3))\n",
    "\n",
    "for ax, signal in zip(axes, ['click', 'like', 'share', 'purchase']):\n",
    "    positives = labels[labels[signal] == 1]['true_relevance']\n",
    "    negatives = labels[labels[signal] == 0]['true_relevance']\n",
    "    \n",
    "    ax.hist(negatives, bins=20, alpha=0.5, label=f'No {signal}', density=True)\n",
    "    ax.hist(positives, bins=20, alpha=0.5, label=f'{signal}', density=True)\n",
    "    ax.set_xlabel('True Relevance')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title(f'{signal.capitalize()} Signal\\n(volume: {labels[signal].mean():.1%})')\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observation: Stronger signals (share, purchase) separate relevant from irrelevant better,\")\n",
    "print(\"but they're much rarer, leading to class imbalance challenges.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ğŸ§  Meta Interview Signal**\n",
    ">\n",
    "> **Strong E5 Answer:** Explains the tradeoff between signal quality and volume. Mentions that clicks are noisy but abundant, while shares are clean but rare.\n",
    ">\n",
    "> **E6 Answer Adds:** Discusses multi-task learning to leverage bothâ€”train a shared representation on abundant clicks, then specialize for sparse shares. Mentions transfer learning from dense to sparse tasks.\n",
    ">\n",
    "> **Common Pitfall:** Using only one signal without discussing its limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Class Imbalance Problem\n",
    "\n",
    "At Meta scale, positive actions are **extremely rare**:\n",
    "\n",
    "| Action | Typical Rate | At 1B impressions/day |\n",
    "|--------|--------------|----------------------|\n",
    "| Click | 1-5% | 10-50M clicks |\n",
    "| Like | 0.1-1% | 1-10M likes |\n",
    "| Comment | 0.01-0.1% | 100K-1M comments |\n",
    "| Share | 0.001-0.01% | 10K-100K shares |\n",
    "| Purchase | 0.0001-0.001% | 1K-10K purchases |\n",
    "\n",
    "Even with 1 billion impressions, you might only have 10K purchases. This creates:\n",
    "\n",
    "1. **Modeling challenges:** Model can achieve 99.9% accuracy by predicting \"no purchase\" always\n",
    "2. **Training challenges:** Rare events may not be sampled enough\n",
    "3. **Evaluation challenges:** Standard metrics may be misleading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: Class imbalance effects\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate imbalanced data\n",
    "n_samples = 100000\n",
    "positive_rate = 0.001  # 0.1% positive rate\n",
    "\n",
    "# Features\n",
    "X = np.random.randn(n_samples, 5)\n",
    "\n",
    "# True labels (very imbalanced)\n",
    "# Positive class has slightly different feature distribution\n",
    "logits = 0.5 * X[:, 0] + 0.3 * X[:, 1] - 5  # -5 shifts the base rate to be very low\n",
    "probs = 1 / (1 + np.exp(-logits))\n",
    "y = np.random.binomial(1, probs)\n",
    "\n",
    "print(f\"Dataset: {n_samples:,} samples, {y.sum():,} positives ({y.mean():.2%})\")\n",
    "\n",
    "# Train a model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "y_prob = model.predict_proba(X)[:, 1]\n",
    "\n",
    "# Evaluate with different metrics\n",
    "print(\"\\nModel Performance:\")\n",
    "print(f\"  Accuracy:  {accuracy_score(y, y_pred):.4f}  â† Misleading! Just predicting 0 gives {1 - y.mean():.4f}\")\n",
    "print(f\"  Precision: {precision_score(y, y_pred, zero_division=0):.4f}\")\n",
    "print(f\"  Recall:    {recall_score(y, y_pred):.4f}\")\n",
    "print(f\"  F1:        {f1_score(y, y_pred):.4f}\")\n",
    "print(f\"  AUC-ROC:   {roc_auc_score(y, y_prob):.4f}  â† Better for ranking\")\n",
    "\n",
    "# Baseline: predict all zeros\n",
    "y_baseline = np.zeros_like(y)\n",
    "print(f\"\\n  Baseline (all zeros) accuracy: {accuracy_score(y, y_baseline):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Class Imbalance\n",
    "\n",
    "| Technique | How It Works | When to Use |\n",
    "|-----------|-------------|-------------|\n",
    "| **Negative downsampling** | Sample only 1-10% of negatives | Training efficiency, memory |\n",
    "| **Positive upsampling** | Repeat positives multiple times | Boost rare class signal |\n",
    "| **Class weights** | Weight positive samples higher in loss | When data fits in memory |\n",
    "| **Focal loss** | Down-weight easy negatives | When many \"easy\" negatives |\n",
    "| **Calibration** | Adjust predictions post-training | When you need true probabilities |\n",
    "\n",
    "**Meta's typical approach:** Negative downsampling during training (for efficiency), then calibration at serving time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: Negative downsampling with calibration\n",
    "\n",
    "def downsample_negatives(X, y, negative_sample_rate=0.1):\n",
    "    \"\"\"Keep all positives, sample fraction of negatives.\"\"\"\n",
    "    pos_mask = y == 1\n",
    "    neg_mask = y == 0\n",
    "    \n",
    "    # Keep all positives\n",
    "    X_pos, y_pos = X[pos_mask], y[pos_mask]\n",
    "    \n",
    "    # Sample negatives\n",
    "    n_neg_samples = int(neg_mask.sum() * negative_sample_rate)\n",
    "    neg_indices = np.random.choice(np.where(neg_mask)[0], n_neg_samples, replace=False)\n",
    "    X_neg, y_neg = X[neg_indices], y[neg_indices]\n",
    "    \n",
    "    # Combine\n",
    "    X_sampled = np.vstack([X_pos, X_neg])\n",
    "    y_sampled = np.hstack([y_pos, y_neg])\n",
    "    \n",
    "    # Shuffle\n",
    "    shuffle_idx = np.random.permutation(len(y_sampled))\n",
    "    return X_sampled[shuffle_idx], y_sampled[shuffle_idx], negative_sample_rate\n",
    "\n",
    "def calibrate_predictions(predictions, sample_rate):\n",
    "    \"\"\"\n",
    "    Calibrate predictions trained on downsampled data.\n",
    "    \n",
    "    If we sampled negatives at rate r, the model sees\n",
    "    P'(pos) = P(pos) / (P(pos) + r * P(neg))\n",
    "    \n",
    "    To recover true probability:\n",
    "    P(pos) = r * P'(pos) / (r * P'(pos) + (1 - P'(pos)))\n",
    "    \"\"\"\n",
    "    r = sample_rate\n",
    "    p_prime = predictions\n",
    "    return (r * p_prime) / (r * p_prime + (1 - p_prime))\n",
    "\n",
    "# Downsample and train\n",
    "X_ds, y_ds, sample_rate = downsample_negatives(X, y, negative_sample_rate=0.1)\n",
    "print(f\"Downsampled data: {len(y_ds):,} samples, {y_ds.sum():,} positives ({y_ds.mean():.1%})\")\n",
    "\n",
    "model_ds = LogisticRegression(max_iter=1000)\n",
    "model_ds.fit(X_ds, y_ds)\n",
    "\n",
    "# Raw predictions (before calibration)\n",
    "y_prob_raw = model_ds.predict_proba(X)[:, 1]\n",
    "\n",
    "# Calibrated predictions\n",
    "y_prob_cal = calibrate_predictions(y_prob_raw, sample_rate)\n",
    "\n",
    "# Compare\n",
    "print(f\"\\nTrue positive rate: {y.mean():.4f}\")\n",
    "print(f\"Avg raw prediction: {y_prob_raw.mean():.4f} (biased high due to downsampling)\")\n",
    "print(f\"Avg calibrated:     {y_prob_cal.mean():.4f} (close to true rate)\")\n",
    "print(f\"\\nAUC-ROC is unchanged: {roc_auc_score(y, y_prob_raw):.4f} (ranking preserved)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ğŸ§  Meta Interview Signal**\n",
    ">\n",
    "> **Strong E5 Answer:** Explains why class imbalance matters and proposes downsampling or class weights.\n",
    ">\n",
    "> **E6 Answer Adds:** Discusses calibrationâ€”why raw predictions from downsampled training are biased and how to correct. Mentions that ranking metrics (AUC) are preserved but probability estimates need adjustment.\n",
    ">\n",
    "> **Common Pitfall:** Reporting accuracy as the main metric for imbalanced problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Label Delay and Attribution\n",
    "\n",
    "At Meta, actions happen at different times after impression:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                              LABEL DELAY SPECTRUM                            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Impression     Click      View      Like      Comment    Purchase    Return\n",
    "    â”‚            â”‚         â”‚          â”‚           â”‚          â”‚          â”‚\n",
    "    â–¼            â–¼         â–¼          â–¼           â–¼          â–¼          â–¼\n",
    "â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â–¶ Time\n",
    "   T=0        T+1s      T+10s      T+30s      T+5min     T+1day    T+30days\n",
    "    â”‚            â”‚         â”‚          â”‚           â”‚          â”‚          â”‚\n",
    "    â”‚     Immediate    Short-term    Medium      Delayed   Very delayed\n",
    "    â”‚       labels       labels      labels      labels      labels\n",
    "```\n",
    "\n",
    "### The Attribution Window Problem\n",
    "\n",
    "**Question:** If a user sees an ad at T=0 and purchases at T=7 days, did the ad cause the purchase?\n",
    "\n",
    "Options:\n",
    "1. **Last-click attribution:** Only credit if purchase within 1 hour\n",
    "2. **7-day window:** Credit if purchase within 7 days\n",
    "3. **View-through:** Credit even if user didn't click but saw the ad\n",
    "\n",
    "Each choice creates different labels and different model behavior!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: How attribution window affects labels\n",
    "\n",
    "np.random.seed(42)\n",
    "n_impressions = 10000\n",
    "\n",
    "# Simulate impressions with delayed conversions\n",
    "impressions = pd.DataFrame({\n",
    "    'impression_id': range(n_impressions),\n",
    "    'user_id': np.random.randint(0, 1000, n_impressions),\n",
    "    'ad_quality': np.random.uniform(0, 1, n_impressions),  # True quality of ad\n",
    "})\n",
    "\n",
    "# Conversion probability depends on ad quality, but timing is random\n",
    "conversion_prob = impressions['ad_quality'] * 0.05  # Up to 5% conversion rate\n",
    "will_convert = np.random.binomial(1, conversion_prob)\n",
    "\n",
    "# Time to conversion (if converted): mix of immediate and delayed\n",
    "# 30% convert within 1 hour, 50% within 1 day, 20% take up to 7 days\n",
    "conversion_time = np.where(\n",
    "    will_convert == 1,\n",
    "    np.random.choice(\n",
    "        [0.5, 12, 72, 168],  # hours: 30min, 12hr, 3day, 7day\n",
    "        n_impressions,\n",
    "        p=[0.3, 0.2, 0.3, 0.2]\n",
    "    ),\n",
    "    np.inf  # No conversion\n",
    ")\n",
    "\n",
    "impressions['will_convert'] = will_convert\n",
    "impressions['conversion_time_hours'] = conversion_time\n",
    "\n",
    "# Create labels with different attribution windows\n",
    "def apply_attribution_window(df, window_hours):\n",
    "    return ((df['will_convert'] == 1) & (df['conversion_time_hours'] <= window_hours)).astype(int)\n",
    "\n",
    "impressions['label_1h'] = apply_attribution_window(impressions, 1)\n",
    "impressions['label_24h'] = apply_attribution_window(impressions, 24)\n",
    "impressions['label_7d'] = apply_attribution_window(impressions, 168)\n",
    "\n",
    "print(\"Conversion rates by attribution window:\")\n",
    "print(f\"  1-hour window:  {impressions['label_1h'].mean():.2%}\")\n",
    "print(f\"  24-hour window: {impressions['label_24h'].mean():.2%}\")\n",
    "print(f\"  7-day window:   {impressions['label_7d'].mean():.2%}\")\n",
    "\n",
    "# Train models with different labels\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = impressions[['ad_quality']].values\n",
    "\n",
    "print(\"\\nModel correlation with TRUE ad quality:\")\n",
    "for label_col, window in [('label_1h', '1-hour'), ('label_24h', '24-hour'), ('label_7d', '7-day')]:\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X, impressions[label_col])\n",
    "    coef = model.coef_[0][0]\n",
    "    print(f\"  {window} model coefficient: {coef:.3f}\")\n",
    "    \n",
    "print(\"\\nâ†’ Wider window captures more true signal (higher coefficient on ad_quality)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Delayed Labels\n",
    "\n",
    "**Problem:** If labels take 7 days, training data is always 7+ days old.\n",
    "\n",
    "**Solutions:**\n",
    "1. **Multi-task learning:** Train on immediate signals (clicks), transfer to delayed signals (purchases)\n",
    "2. **Label propagation:** Use partial labels (label at T+1 day as proxy for T+7 day)\n",
    "3. **Survival modeling:** Model time-to-conversion, not just binary conversion\n",
    "4. **Fresh feature, stale label:** Use fresh features at serving, accept stale labels at training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Label Bias: Systematic Errors in Labels\n",
    "\n",
    "### Types of Label Bias\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                              LABEL BIAS TAXONOMY                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "SELECTION BIAS\n",
    "â”œâ”€â”€ You only observe labels for items you showed\n",
    "â”œâ”€â”€ Items never shown have no labels\n",
    "â””â”€â”€ Model can't learn about unexplored space\n",
    "\n",
    "POSITION BIAS\n",
    "â”œâ”€â”€ Higher positions get more clicks\n",
    "â”œâ”€â”€ Labels confound relevance with position\n",
    "â””â”€â”€ Model learns \"position 1 is good\" not \"relevant is good\"\n",
    "\n",
    "POPULARITY BIAS\n",
    "â”œâ”€â”€ Popular items get more engagement\n",
    "â”œâ”€â”€ Labels confound quality with popularity\n",
    "â””â”€â”€ Model reinforces popularity, new content starves\n",
    "\n",
    "PRESENTATION BIAS\n",
    "â”œâ”€â”€ Visual presentation affects engagement\n",
    "â”œâ”€â”€ A better thumbnail != better content\n",
    "â””â”€â”€ Model learns to predict thumbnail quality, not content quality\n",
    "\n",
    "SOCIAL BIAS\n",
    "â”œâ”€â”€ Actions influenced by social proof\n",
    "â”œâ”€â”€ \"100K likes\" â†’ more likely to like\n",
    "â””â”€â”€ Model learns to predict what's already popular\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: Popularity bias creating rich-get-richer dynamics\n",
    "\n",
    "def simulate_popularity_bias(n_items=100, n_rounds=50, bias_strength=0.5):\n",
    "    \"\"\"\n",
    "    Simulate how popularity-biased labels create rich-get-richer dynamics.\n",
    "    \n",
    "    Items have true quality, but observed engagement also depends on current popularity.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # True quality (fixed)\n",
    "    true_quality = np.random.uniform(0, 1, n_items)\n",
    "    \n",
    "    # Current popularity (changes over time)\n",
    "    popularity = np.ones(n_items) / n_items  # Start equal\n",
    "    \n",
    "    history = []\n",
    "    \n",
    "    for round_num in range(n_rounds):\n",
    "        # Observed engagement = quality + bias * popularity\n",
    "        observed_score = (1 - bias_strength) * true_quality + bias_strength * popularity\n",
    "        \n",
    "        # Show top items (greedy)\n",
    "        shown_items = np.argsort(observed_score)[-10:]  # Show top 10\n",
    "        \n",
    "        # Update popularity based on who was shown\n",
    "        new_popularity = np.zeros(n_items)\n",
    "        for item in shown_items:\n",
    "            new_popularity[item] = true_quality[item]  # Actual engagement\n",
    "        new_popularity = new_popularity / new_popularity.sum()\n",
    "        \n",
    "        # Popularity accumulates over time\n",
    "        popularity = 0.9 * popularity + 0.1 * new_popularity\n",
    "        popularity = popularity / popularity.sum()\n",
    "        \n",
    "        # Track diversity: how many unique items have been shown\n",
    "        history.append({\n",
    "            'round': round_num,\n",
    "            'top_item_quality': true_quality[shown_items].mean(),\n",
    "            'quality_correlation': np.corrcoef(true_quality, observed_score)[0, 1],\n",
    "            'gini_coefficient': gini(popularity)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(history), true_quality, popularity\n",
    "\n",
    "def gini(x):\n",
    "    \"\"\"Calculate Gini coefficient (0 = equal, 1 = one item has everything)\"\"\"\n",
    "    x = np.sort(x)\n",
    "    n = len(x)\n",
    "    index = np.arange(1, n + 1)\n",
    "    return (2 * np.sum(index * x) - (n + 1) * np.sum(x)) / (n * np.sum(x))\n",
    "\n",
    "# Compare low vs high bias\n",
    "history_low, q_low, p_low = simulate_popularity_bias(bias_strength=0.1)\n",
    "history_high, q_high, p_high = simulate_popularity_bias(bias_strength=0.7)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Gini coefficient over time\n",
    "axes[0].plot(history_low['round'], history_low['gini_coefficient'], label='Low bias (0.1)')\n",
    "axes[0].plot(history_high['round'], history_high['gini_coefficient'], label='High bias (0.7)')\n",
    "axes[0].set_xlabel('Round')\n",
    "axes[0].set_ylabel('Gini Coefficient (inequality)')\n",
    "axes[0].set_title('Popularity Concentration Over Time')\n",
    "axes[0].legend()\n",
    "\n",
    "# Final popularity vs true quality\n",
    "axes[1].scatter(q_high, p_high, alpha=0.5)\n",
    "axes[1].set_xlabel('True Quality')\n",
    "axes[1].set_ylabel('Final Popularity Share')\n",
    "axes[1].set_title('High Bias: Popularity Disconnected from Quality')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Correlation (quality, final popularity):\")\n",
    "print(f\"  Low bias:  {np.corrcoef(q_low, p_low)[0,1]:.3f}\")\n",
    "print(f\"  High bias: {np.corrcoef(q_high, p_high)[0,1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ğŸ§  Meta Interview Signal**\n",
    ">\n",
    "> **Strong E5 Answer:** Identifies popularity bias and proposes exploration (random traffic, Thompson sampling) to counteract it.\n",
    ">\n",
    "> **E6 Answer Adds:** Discusses systemic solutionsâ€”inverse propensity weighting for training, diversity constraints in ranking, cold start strategies for new content, and monitoring for concentration metrics.\n",
    ">\n",
    "> **Common Pitfall:** Training on biased labels without acknowledging the bias exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Collection Strategies\n",
    "\n",
    "### Logging for ML\n",
    "\n",
    "At Meta, logging is designed for ML from the start:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                          ML LOGGING REQUIREMENTS                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "For every impression, log:\n",
    "â”œâ”€â”€ Request context\n",
    "â”‚   â”œâ”€â”€ User ID (or anonymous ID)\n",
    "â”‚   â”œâ”€â”€ Session ID\n",
    "â”‚   â”œâ”€â”€ Timestamp\n",
    "â”‚   â”œâ”€â”€ Device type\n",
    "â”‚   â””â”€â”€ Geo location (bucketed for privacy)\n",
    "â”‚\n",
    "â”œâ”€â”€ Candidate information\n",
    "â”‚   â”œâ”€â”€ Item ID\n",
    "â”‚   â”œâ”€â”€ Item features (at request time)\n",
    "â”‚   â”œâ”€â”€ Position in ranking\n",
    "â”‚   â””â”€â”€ Model score\n",
    "â”‚\n",
    "â”œâ”€â”€ User features (at request time)\n",
    "â”‚   â”œâ”€â”€ Historical features\n",
    "â”‚   â”œâ”€â”€ Real-time features\n",
    "â”‚   â””â”€â”€ Session features\n",
    "â”‚\n",
    "â””â”€â”€ Outcomes (joined later)\n",
    "    â”œâ”€â”€ Click (immediate)\n",
    "    â”œâ”€â”€ Dwell time\n",
    "    â”œâ”€â”€ Engagement (like, comment, share)\n",
    "    â”œâ”€â”€ Negative actions (hide, report)\n",
    "    â””â”€â”€ Downstream conversions\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Log Features at Request Time?\n",
    "\n",
    "**Problem:** If you log only user ID and item ID, and join features later, you get **label-feature leakage**.\n",
    "\n",
    "Example:\n",
    "- User clicks ad at T=0\n",
    "- At T+1 day, user's \"click history\" feature includes this click\n",
    "- If you join features at T+7 days, the \"click history\" feature now includes the click you're trying to predict\n",
    "\n",
    "**Solution:** Log features at request time, so training uses the same features that were available at serving time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: Label leakage through feature joins\n",
    "\n",
    "np.random.seed(42)\n",
    "n_events = 1000\n",
    "\n",
    "# Simulate events over time\n",
    "events = pd.DataFrame({\n",
    "    'event_id': range(n_events),\n",
    "    'user_id': np.random.randint(0, 100, n_events),\n",
    "    'timestamp': pd.date_range('2024-01-01', periods=n_events, freq='1min'),\n",
    "    'true_intent': np.random.uniform(0, 1, n_events),  # User's true click propensity\n",
    "})\n",
    "\n",
    "# Labels (click or not)\n",
    "events['clicked'] = (np.random.random(n_events) < events['true_intent']).astype(int)\n",
    "\n",
    "# CORRECT: Feature logged at request time (doesn't include current event)\n",
    "def compute_historical_ctr_correct(df):\n",
    "    \"\"\"Compute CTR based on events BEFORE current event.\"\"\"\n",
    "    result = []\n",
    "    for idx, row in df.iterrows():\n",
    "        user_history = df[(df['user_id'] == row['user_id']) & (df['timestamp'] < row['timestamp'])]\n",
    "        if len(user_history) > 0:\n",
    "            result.append(user_history['clicked'].mean())\n",
    "        else:\n",
    "            result.append(0.5)  # Default for new users\n",
    "    return result\n",
    "\n",
    "# WRONG: Feature joined later (includes current event - LEAKAGE)\n",
    "def compute_historical_ctr_leaky(df):\n",
    "    \"\"\"Compute CTR INCLUDING current event - causes leakage.\"\"\"\n",
    "    result = []\n",
    "    for idx, row in df.iterrows():\n",
    "        user_history = df[df['user_id'] == row['user_id']]  # Includes current!\n",
    "        result.append(user_history['clicked'].mean())\n",
    "    return result\n",
    "\n",
    "events['hist_ctr_correct'] = compute_historical_ctr_correct(events)\n",
    "events['hist_ctr_leaky'] = compute_historical_ctr_leaky(events)\n",
    "\n",
    "# Train models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_correct = events[['hist_ctr_correct']].values\n",
    "X_leaky = events[['hist_ctr_leaky']].values\n",
    "y = events['clicked'].values\n",
    "\n",
    "# Evaluate\n",
    "print(\"Correlation of feature with label:\")\n",
    "print(f\"  Correct (no leakage): {np.corrcoef(X_correct.flatten(), y)[0,1]:.3f}\")\n",
    "print(f\"  Leaky:                {np.corrcoef(X_leaky.flatten(), y)[0,1]:.3f}  â† Artificially high!\")\n",
    "print(\"\\nâ†’ Leaky feature has artificially high correlation because it includes the label!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Negative Sampling for Multi-Class Problems\n",
    "\n",
    "For candidate generation models, you need to predict which of millions of items a user will engage with.\n",
    "\n",
    "**Problem:** You can't train on all negatives (too many).\n",
    "\n",
    "**Solution:** Sample negatives, but how you sample affects what the model learns.\n",
    "\n",
    "| Sampling Strategy | What Model Learns | Best For |\n",
    "|-------------------|-------------------|----------|\n",
    "| **Uniform random** | Distinguish target from random | General retrieval |\n",
    "| **Popularity-based** | Distinguish target from popular items | When popular â‰  relevant |\n",
    "| **In-batch** | Distinguish target from other users' targets | Efficient training |\n",
    "| **Hard negatives** | Distinguish target from similar items | Fine-grained ranking |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: Impact of negative sampling strategy\n",
    "\n",
    "np.random.seed(42)\n",
    "n_items = 1000\n",
    "n_samples = 5000\n",
    "\n",
    "# Item properties\n",
    "item_popularity = np.random.pareto(2, n_items)  # Power law popularity\n",
    "item_popularity = item_popularity / item_popularity.sum()\n",
    "item_quality = np.random.uniform(0, 1, n_items)\n",
    "\n",
    "# Generate positive interactions (user likes item)\n",
    "# Positive = high quality items (with some noise)\n",
    "positive_probs = item_quality / item_quality.sum()\n",
    "positives = np.random.choice(n_items, n_samples, p=positive_probs)\n",
    "\n",
    "# Different negative sampling strategies\n",
    "def sample_negatives_uniform(positives, n_items):\n",
    "    return np.random.randint(0, n_items, len(positives))\n",
    "\n",
    "def sample_negatives_popularity(positives, n_items, popularity):\n",
    "    return np.random.choice(n_items, len(positives), p=popularity)\n",
    "\n",
    "def sample_negatives_hard(positives, n_items, embeddings, k=5):\n",
    "    \"\"\"Sample from items similar to positives (hard negatives).\"\"\"\n",
    "    negatives = []\n",
    "    for pos in positives:\n",
    "        # Find k nearest neighbors, sample one\n",
    "        distances = np.abs(embeddings - embeddings[pos])\n",
    "        nearest = np.argsort(distances)[1:k+1]  # Exclude self\n",
    "        negatives.append(np.random.choice(nearest))\n",
    "    return np.array(negatives)\n",
    "\n",
    "# Simple 1D embeddings based on quality (for demonstration)\n",
    "embeddings = item_quality\n",
    "\n",
    "neg_uniform = sample_negatives_uniform(positives, n_items)\n",
    "neg_popular = sample_negatives_popularity(positives, n_items, item_popularity)\n",
    "neg_hard = sample_negatives_hard(positives, n_items, embeddings)\n",
    "\n",
    "# Analyze what each strategy surfaces\n",
    "print(\"Average item QUALITY in negative samples:\")\n",
    "print(f\"  Uniform:    {item_quality[neg_uniform].mean():.3f}\")\n",
    "print(f\"  Popularity: {item_quality[neg_popular].mean():.3f}\")\n",
    "print(f\"  Hard:       {item_quality[neg_hard].mean():.3f}\")\n",
    "\n",
    "print(\"\\nAverage item POPULARITY in negative samples:\")\n",
    "print(f\"  Uniform:    {item_popularity[neg_uniform].mean():.6f}\")\n",
    "print(f\"  Popularity: {item_popularity[neg_popular].mean():.6f}\")\n",
    "print(f\"  Hard:       {item_popularity[neg_hard].mean():.6f}\")\n",
    "\n",
    "print(\"\\nâ†’ Hard negatives have higher quality (closer to positives)\")\n",
    "print(\"  This forces the model to learn fine-grained distinctions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ğŸ§  Meta Interview Signal**\n",
    ">\n",
    "> **Strong E5 Answer:** Explains why negative sampling is necessary and describes uniform vs popularity-based sampling.\n",
    ">\n",
    "> **E6 Answer Adds:** Discusses hard negative mining, curriculum learning (start easy, get harder), and how sampling distribution affects what the model learns. Mentions potential issues with in-batch negatives (batch composition matters).\n",
    ">\n",
    "> **Common Pitfall:** Not mentioning that negative sampling strategy affects model behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Interview Drills\n",
    "\n",
    "### Drill 1: \"Where do your labels come from for [X system]?\"\n",
    "\n",
    "**Framework:**\n",
    "1. List explicit signals (if any)\n",
    "2. List implicit signals with their noise characteristics\n",
    "3. Describe positive vs negative labels\n",
    "4. Mention biases in labels\n",
    "5. Describe attribution window\n",
    "\n",
    "**Example for Ad Click Prediction:**\n",
    "> \"For ad click prediction, labels come from click events. This is an implicit signalâ€”a click means interest but not necessarily satisfaction. Positive labels are clicks, negative labels are non-clicks. Key biases: position bias (higher ads get more clicks), presentation bias (better creative gets more clicks regardless of relevance). For conversion-based models, we use 7-day attribution windows, which means training data is always at least 7 days stale.\"\n",
    "\n",
    "---\n",
    "\n",
    "### Drill 2: \"How do you handle a 0.01% positive rate?\"\n",
    "\n",
    "**Strong answer:**\n",
    "> \"With 0.01% positive rate, we face severe class imbalance. Solutions: (1) Negative downsamplingâ€”keep all positives, sample 1-10% of negatives, then calibrate predictions at serving. (2) Use ranking metrics like AUC that are robust to imbalance instead of accuracy. (3) Multi-task learningâ€”train on a related but more frequent signal (clicks) and transfer to the rare signal (purchases). (4) For evaluation, use stratified sampling to ensure enough positives in test set.\"\n",
    "\n",
    "---\n",
    "\n",
    "### Drill 3: \"How do you prevent label leakage?\"\n",
    "\n",
    "**Strong answer:**\n",
    "> \"Label leakage happens when features contain information about the label that wouldn't be available at serving time. Prevention: (1) Log features at request time, not join later. (2) Use point-in-time feature snapshots for historical features. (3) For user history features, ensure they don't include the current event. (4) Audit features by checking correlation with labelâ€”suspiciously high correlation suggests leakage. (5) Validate by checking that offline and online metrics agreeâ€”large discrepancy suggests training-serving skew or leakage.\"\n",
    "\n",
    "---\n",
    "\n",
    "### Drill 4: \"What breaks at Meta scale with this data approach?\"\n",
    "\n",
    "**Strong answer:**\n",
    "> \"At Meta scale: (1) Logging volumeâ€”billions of impressions per day, storage becomes expensive, need sampling or aggregation. (2) Feature joinsâ€”joining user features to impression logs at scale requires distributed systems, and stale joins introduce training-serving skew. (3) Label delaysâ€”waiting 7 days for labels means training data is always stale, model can't adapt to trends. (4) Negative samplingâ€”with millions of items, can't enumerate all negatives, sampling strategy significantly affects model behavior. (5) Feedback loopsâ€”model predictions determine what gets shown, which determines future labels, can cause drift.\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Data & Labels Checklist\n",
    "\n",
    "When designing an ML system, verify:\n",
    "\n",
    "- [ ] **Label source identified:** Explicit or implicit? What's the noise level?\n",
    "- [ ] **Class imbalance addressed:** What's the positive rate? Downsampling? Calibration?\n",
    "- [ ] **Label delay understood:** How long until labels are available? Impact on freshness?\n",
    "- [ ] **Attribution window defined:** For delayed outcomes, what window makes sense?\n",
    "- [ ] **Biases acknowledged:** Position? Popularity? Selection? Presentation?\n",
    "- [ ] **Logging at request time:** Are features logged when prediction is made?\n",
    "- [ ] **Negative sampling strategy:** Uniform? Popularity? Hard negatives?\n",
    "- [ ] **Label leakage prevented:** No future information in features?\n",
    "- [ ] **Label quality validated:** Spot checks? Human review? Correlation with ground truth?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

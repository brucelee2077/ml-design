{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Data Engineering: ETL and Storage Types\n",
    "\n",
    "---\n",
    "\n",
    "## What the Chapter Says\n",
    "\n",
    "The chapter defines **Data Preparation = Data Engineering + Feature Engineering**.\n",
    "\n",
    "This notebook covers **Data Engineering fundamentals**:\n",
    "\n",
    "1. **Data Sources**: who collected, clean, trusted, user-generated vs system-generated\n",
    "2. **Data Storage Overview**\n",
    "3. **Database Types**:\n",
    "   - SQL (MySQL, PostgreSQL)\n",
    "   - NoSQL: key/value (Redis, DynamoDB), column-based (Cassandra, HBase), graph (Neo4J), document (MongoDB, CouchDB)\n",
    "4. **ETL**: Extract, Transform (cleanse/map/format), Load (db/files/data warehouse)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta Interview Signal\n",
    "\n",
    "| Level | Expectations |\n",
    "|-------|-------------|\n",
    "| **E5** | Understands ETL pipeline. Can choose appropriate database type for use case. Knows structured vs unstructured data. |\n",
    "| **E6** | Discusses data quality at scale. Designs for data freshness SLAs. Considers data lineage and versioning. Proposes hybrid storage solutions. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Preparation Pipeline Diagram (Chapter Figure)\n",
    "\n",
    "The chapter shows this pipeline:\n",
    "\n",
    "```\n",
    "Data Sources → Data Engineering → Feature Engineering → Prepared Features\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "ax.axis('off')\n",
    "ax.set_title('Data Preparation Pipeline (Chapter Figure)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Pipeline boxes\n",
    "boxes = [\n",
    "    ('Data\\nSources', 1, '#BBDEFB'),\n",
    "    ('Data\\nEngineering', 4.5, '#FFF9C4'),\n",
    "    ('Feature\\nEngineering', 8, '#C8E6C9'),\n",
    "    ('Prepared\\nFeatures', 11.5, '#E1BEE7'),\n",
    "]\n",
    "\n",
    "for (label, x, color) in boxes:\n",
    "    rect = mpatches.FancyBboxPatch((x, 1), 2.5, 2, boxstyle='round,pad=0.1',\n",
    "                                    facecolor=color, edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x + 1.25, 2, label, ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Arrows\n",
    "for x in [3.5, 7, 10.5]:\n",
    "    ax.annotate('', xy=(x + 1, 2), xytext=(x, 2),\n",
    "               arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
    "\n",
    "# Labels\n",
    "ax.text(2.25, 0.5, 'Logs, DBs,\\nAPIs, Files', ha='center', fontsize=9, style='italic')\n",
    "ax.text(5.75, 0.5, 'ETL: Extract,\\nTransform, Load', ha='center', fontsize=9, style='italic')\n",
    "ax.text(9.25, 0.5, 'Scaling, Encoding,\\nImputation', ha='center', fontsize=9, style='italic')\n",
    "ax.text(12.75, 0.5, 'Model-ready\\nDataset', ha='center', fontsize=9, style='italic')\n",
    "\n",
    "ax.set_xlim(0, 15)\n",
    "ax.set_ylim(0, 4)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## E1) Data Sources (Chapter Content)\n",
    "\n",
    "The chapter asks these questions about data sources:\n",
    "- **Who collected** the data?\n",
    "- Is it **clean**?\n",
    "- Is it **trusted**?\n",
    "- Is it **user-generated** vs **system-generated**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data source analysis framework\n",
    "data_sources = pd.DataFrame({\n",
    "    'Source Type': [\n",
    "        'User-Generated',\n",
    "        'User-Generated',\n",
    "        'System-Generated',\n",
    "        'System-Generated',\n",
    "        'Third-Party'\n",
    "    ],\n",
    "    'Example': [\n",
    "        'Posts, comments, reviews',\n",
    "        'Profile information',\n",
    "        'Click logs, view events',\n",
    "        'Server logs, errors',\n",
    "        'External APIs, purchased data'\n",
    "    ],\n",
    "    'Clean?': [\n",
    "        'Often noisy, typos, spam',\n",
    "        'Moderate - users may lie',\n",
    "        'Usually clean, structured',\n",
    "        'Clean but verbose',\n",
    "        'Varies widely'\n",
    "    ],\n",
    "    'Trusted?': [\n",
    "        'Low - adversarial users',\n",
    "        'Moderate',\n",
    "        'High - we control collection',\n",
    "        'High',\n",
    "        'Depends on vendor'\n",
    "    ],\n",
    "    'Interview Consideration': [\n",
    "        'Need content moderation, spam filtering',\n",
    "        'Validate critical fields',\n",
    "        'Primary source for ML labels',\n",
    "        'Good for debugging, monitoring',\n",
    "        'Data contracts, freshness SLAs'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"DATA SOURCES ANALYSIS (Chapter Framework)\")\n",
    "print(\"=\"*100)\n",
    "print(data_sources.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate different data sources for a feed ranking system\n",
    "np.random.seed(42)\n",
    "\n",
    "# User-generated data (noisy)\n",
    "n_posts = 1000\n",
    "user_generated = pd.DataFrame({\n",
    "    'post_id': range(n_posts),\n",
    "    'text': ['Sample post text ' + str(i) for i in range(n_posts)],\n",
    "    'author_id': np.random.randint(1, 101, n_posts),\n",
    "    # User-generated often has issues:\n",
    "    'has_typos': np.random.choice([True, False], n_posts, p=[0.15, 0.85]),\n",
    "    'is_spam': np.random.choice([True, False], n_posts, p=[0.05, 0.95]),\n",
    "    'missing_fields': np.random.choice([True, False], n_posts, p=[0.1, 0.9]),\n",
    "})\n",
    "\n",
    "print(\"USER-GENERATED DATA (Posts)\")\n",
    "print(f\"Total posts: {len(user_generated)}\")\n",
    "print(f\"Posts with typos: {user_generated['has_typos'].sum()} ({user_generated['has_typos'].mean()*100:.1f}%)\")\n",
    "print(f\"Spam posts: {user_generated['is_spam'].sum()} ({user_generated['is_spam'].mean()*100:.1f}%)\")\n",
    "print(f\"Posts with missing fields: {user_generated['missing_fields'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System-generated data (clean)\n",
    "n_events = 10000\n",
    "system_generated = pd.DataFrame({\n",
    "    'event_id': range(n_events),\n",
    "    'user_id': np.random.randint(1, 1001, n_events),\n",
    "    'post_id': np.random.randint(0, n_posts, n_events),\n",
    "    'action': np.random.choice(['view', 'like', 'share', 'comment'], n_events, p=[0.7, 0.15, 0.05, 0.1]),\n",
    "    'timestamp': pd.date_range('2024-01-01', periods=n_events, freq='1min'),\n",
    "    'device': np.random.choice(['iOS', 'Android', 'Web'], n_events, p=[0.4, 0.4, 0.2]),\n",
    "})\n",
    "\n",
    "print(\"\\nSYSTEM-GENERATED DATA (Event Logs)\")\n",
    "print(f\"Total events: {len(system_generated)}\")\n",
    "print(f\"Missing values: {system_generated.isnull().sum().sum()}\")\n",
    "print(f\"\\nAction distribution:\")\n",
    "print(system_generated['action'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## E2) Database Types (Chapter Content)\n",
    "\n",
    "The chapter lists these database types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database types from chapter\n",
    "db_types = pd.DataFrame({\n",
    "    'Category': [\n",
    "        'SQL',\n",
    "        'NoSQL - Key/Value',\n",
    "        'NoSQL - Column-based',\n",
    "        'NoSQL - Graph',\n",
    "        'NoSQL - Document'\n",
    "    ],\n",
    "    'Examples': [\n",
    "        'MySQL, PostgreSQL',\n",
    "        'Redis, DynamoDB',\n",
    "        'Cassandra, HBase',\n",
    "        'Neo4J',\n",
    "        'MongoDB, CouchDB'\n",
    "    ],\n",
    "    'Best For': [\n",
    "        'Structured data with relationships, transactions',\n",
    "        'Caching, session storage, real-time features',\n",
    "        'Time-series, write-heavy workloads',\n",
    "        'Social networks, recommendations',\n",
    "        'Flexible schemas, JSON-like data'\n",
    "    ],\n",
    "    'ML Use Case': [\n",
    "        'User profiles, product catalog',\n",
    "        'Feature store (online), real-time predictions',\n",
    "        'Event logs, training data at scale',\n",
    "        'Friend recommendations, knowledge graphs',\n",
    "        'Unstructured content, embeddings'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"DATABASE TYPES (from Chapter)\")\n",
    "print(\"=\"*100)\n",
    "print(db_types.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual diagram of database types\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "ax.axis('off')\n",
    "ax.set_title('Database Types Overview (Chapter)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Main categories\n",
    "# SQL\n",
    "rect = mpatches.FancyBboxPatch((1, 5), 3, 1.5, boxstyle='round,pad=0.1',\n",
    "                                facecolor='#BBDEFB', edgecolor='black', linewidth=2)\n",
    "ax.add_patch(rect)\n",
    "ax.text(2.5, 5.75, 'SQL', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# SQL examples\n",
    "for i, db in enumerate(['MySQL', 'PostgreSQL']):\n",
    "    rect = mpatches.FancyBboxPatch((0.5 + i*2, 3), 1.8, 1, boxstyle='round,pad=0.1',\n",
    "                                    facecolor='#E3F2FD', edgecolor='black', linewidth=1)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(1.4 + i*2, 3.5, db, ha='center', va='center', fontsize=10)\n",
    "    ax.annotate('', xy=(1.4 + i*2, 4), xytext=(2.5, 5),\n",
    "               arrowprops=dict(arrowstyle='->', color='gray', lw=1))\n",
    "\n",
    "# NoSQL\n",
    "rect = mpatches.FancyBboxPatch((6, 5), 8, 1.5, boxstyle='round,pad=0.1',\n",
    "                                facecolor='#C8E6C9', edgecolor='black', linewidth=2)\n",
    "ax.add_patch(rect)\n",
    "ax.text(10, 5.75, 'NoSQL', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# NoSQL subcategories\n",
    "nosql_types = [\n",
    "    ('Key/Value', 6.5, ['Redis', 'DynamoDB']),\n",
    "    ('Column-based', 9, ['Cassandra', 'HBase']),\n",
    "    ('Graph', 11.5, ['Neo4J']),\n",
    "    ('Document', 13.5, ['MongoDB', 'CouchDB']),\n",
    "]\n",
    "\n",
    "for (subtype, x, examples) in nosql_types:\n",
    "    rect = mpatches.FancyBboxPatch((x-0.7, 3), 1.8, 1, boxstyle='round,pad=0.1',\n",
    "                                    facecolor='#E8F5E9', edgecolor='black', linewidth=1.5)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x+0.2, 3.5, subtype, ha='center', va='center', fontsize=9, fontweight='bold')\n",
    "    ax.annotate('', xy=(x+0.2, 4), xytext=(10, 5),\n",
    "               arrowprops=dict(arrowstyle='->', color='gray', lw=1))\n",
    "    \n",
    "    # Examples\n",
    "    for j, ex in enumerate(examples):\n",
    "        rect = mpatches.FancyBboxPatch((x-0.5+j*0.9, 1), 0.8, 0.8, boxstyle='round,pad=0.05',\n",
    "                                        facecolor='#F1F8E9', edgecolor='black', linewidth=1)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x-0.1+j*0.9, 1.4, ex, ha='center', va='center', fontsize=7)\n",
    "        ax.annotate('', xy=(x-0.1+j*0.9, 1.8), xytext=(x+0.2, 3),\n",
    "                   arrowprops=dict(arrowstyle='->', color='lightgray', lw=0.5))\n",
    "\n",
    "ax.set_xlim(0, 16)\n",
    "ax.set_ylim(0, 7.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## E3) Structured vs Unstructured Data (Chapter Table)\n",
    "\n",
    "The chapter provides this comparison table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chapter's structured vs unstructured table\n",
    "data_comparison = pd.DataFrame({\n",
    "    'Aspect': [\n",
    "        'Schema',\n",
    "        'Search',\n",
    "        'Storage',\n",
    "        'Examples',\n",
    "        'ML Models'\n",
    "    ],\n",
    "    'Structured': [\n",
    "        'Predefined schema',\n",
    "        'Easier to search',\n",
    "        'Relational DB, many NoSQL, data warehouses',\n",
    "        'Dates, phone numbers, credit cards, addresses, names',\n",
    "        'Traditional ML (logistic regression, trees, etc.)'\n",
    "    ],\n",
    "    'Unstructured': [\n",
    "        'No schema',\n",
    "        'Harder to search',\n",
    "        'NoSQL, data lakes',\n",
    "        'Text, audio, images, videos',\n",
    "        'Deep learning spans both'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(\"STRUCTURED vs UNSTRUCTURED DATA (Chapter Table)\")\n",
    "print(\"=\"*90)\n",
    "print(data_comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types breakdown from chapter\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA TYPES HIERARCHY (Chapter)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "hierarchy = \"\"\"\n",
    "DATA TYPES\n",
    "├── STRUCTURED\n",
    "│   ├── Numerical\n",
    "│   │   ├── Discrete (count of items, number of clicks)\n",
    "│   │   └── Continuous (price, temperature, watch time)\n",
    "│   └── Categorical\n",
    "│       ├── Ordinal (rating: low/medium/high, education level)\n",
    "│       └── Nominal (color, country, device type)\n",
    "│\n",
    "└── UNSTRUCTURED\n",
    "    ├── Text (posts, comments, reviews)\n",
    "    ├── Audio (voice messages, podcasts)\n",
    "    ├── Image (photos, screenshots)\n",
    "    └── Video (reels, stories)\n",
    "\"\"\"\n",
    "print(hierarchy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate structured data\n",
    "np.random.seed(42)\n",
    "n = 500\n",
    "\n",
    "structured_data = pd.DataFrame({\n",
    "    # Numerical - Discrete\n",
    "    'num_clicks': np.random.poisson(5, n),\n",
    "    'item_count': np.random.randint(1, 20, n),\n",
    "    \n",
    "    # Numerical - Continuous\n",
    "    'watch_time_sec': np.random.exponential(120, n),\n",
    "    'price_usd': np.random.uniform(10, 500, n),\n",
    "    \n",
    "    # Categorical - Ordinal\n",
    "    'rating': np.random.choice(['low', 'medium', 'high'], n, p=[0.2, 0.5, 0.3]),\n",
    "    \n",
    "    # Categorical - Nominal\n",
    "    'device': np.random.choice(['iOS', 'Android', 'Web'], n),\n",
    "    'country': np.random.choice(['US', 'UK', 'IN', 'BR', 'DE'], n),\n",
    "})\n",
    "\n",
    "print(\"STRUCTURED DATA EXAMPLE\")\n",
    "print(\"=\"*60)\n",
    "print(structured_data.head(10))\n",
    "print(f\"\\nData types:\")\n",
    "print(structured_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## E4) ETL: Extract, Transform, Load (Chapter Content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL stages from chapter\n",
    "etl_stages = pd.DataFrame({\n",
    "    'Stage': ['Extract', 'Transform', 'Load'],\n",
    "    'Description': [\n",
    "        'Pull data from various sources',\n",
    "        'Cleanse, map, and format data',\n",
    "        'Store in target destination'\n",
    "    ],\n",
    "    'Operations': [\n",
    "        'API calls, database queries, file reads',\n",
    "        'Data cleansing, schema mapping, format conversion',\n",
    "        'Database insert, file write, data warehouse load'\n",
    "    ],\n",
    "    'Targets': [\n",
    "        'Source systems (DBs, APIs, files)',\n",
    "        'In-memory processing',\n",
    "        'Database, files, data warehouse'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ETL PIPELINE (Chapter Content)\")\n",
    "print(\"=\"*80)\n",
    "print(etl_stages.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual ETL diagram\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "ax.axis('off')\n",
    "ax.set_title('ETL Pipeline (Chapter)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Sources\n",
    "sources = ['APIs', 'Databases', 'Files', 'Streams']\n",
    "for i, src in enumerate(sources):\n",
    "    rect = mpatches.FancyBboxPatch((0.5, 3.5 - i*1), 1.5, 0.8, boxstyle='round,pad=0.1',\n",
    "                                    facecolor='#BBDEFB', edgecolor='black', linewidth=1)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(1.25, 3.9 - i*1, src, ha='center', va='center', fontsize=9)\n",
    "\n",
    "# Extract box\n",
    "rect = mpatches.FancyBboxPatch((3, 1.5), 2, 2.5, boxstyle='round,pad=0.1',\n",
    "                                facecolor='#FFF9C4', edgecolor='black', linewidth=2)\n",
    "ax.add_patch(rect)\n",
    "ax.text(4, 2.75, 'EXTRACT', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Arrows to Extract\n",
    "for i in range(4):\n",
    "    ax.annotate('', xy=(3, 2.75), xytext=(2, 3.9 - i*1),\n",
    "               arrowprops=dict(arrowstyle='->', color='gray', lw=1))\n",
    "\n",
    "# Transform box\n",
    "rect = mpatches.FancyBboxPatch((6, 1.5), 2.5, 2.5, boxstyle='round,pad=0.1',\n",
    "                                facecolor='#FFCCBC', edgecolor='black', linewidth=2)\n",
    "ax.add_patch(rect)\n",
    "ax.text(7.25, 3.2, 'TRANSFORM', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "ax.text(7.25, 2.6, '• Cleanse', ha='center', va='center', fontsize=9)\n",
    "ax.text(7.25, 2.2, '• Map', ha='center', va='center', fontsize=9)\n",
    "ax.text(7.25, 1.8, '• Format', ha='center', va='center', fontsize=9)\n",
    "\n",
    "ax.annotate('', xy=(6, 2.75), xytext=(5, 2.75),\n",
    "           arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
    "\n",
    "# Load box\n",
    "rect = mpatches.FancyBboxPatch((9.5, 1.5), 2, 2.5, boxstyle='round,pad=0.1',\n",
    "                                facecolor='#C8E6C9', edgecolor='black', linewidth=2)\n",
    "ax.add_patch(rect)\n",
    "ax.text(10.5, 2.75, 'LOAD', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.annotate('', xy=(9.5, 2.75), xytext=(8.5, 2.75),\n",
    "           arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
    "\n",
    "# Destinations\n",
    "destinations = ['Database', 'Files', 'Data Warehouse']\n",
    "for i, dst in enumerate(destinations):\n",
    "    rect = mpatches.FancyBboxPatch((12.5, 3 - i*1), 2, 0.8, boxstyle='round,pad=0.1',\n",
    "                                    facecolor='#E1BEE7', edgecolor='black', linewidth=1)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(13.5, 3.4 - i*1, dst, ha='center', va='center', fontsize=9)\n",
    "    ax.annotate('', xy=(12.5, 3.4 - i*1), xytext=(11.5, 2.75),\n",
    "               arrowprops=dict(arrowstyle='->', color='gray', lw=1))\n",
    "\n",
    "ax.set_xlim(0, 15.5)\n",
    "ax.set_ylim(0, 5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hands-On: Simulated ETL Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a realistic ETL pipeline for feed ranking\n",
    "np.random.seed(42)\n",
    "\n",
    "class ETLPipeline:\n",
    "    \"\"\"Simulates ETL for ML data preparation\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.raw_data = None\n",
    "        self.transformed_data = None\n",
    "        self.stats = {'extracted': 0, 'transformed': 0, 'loaded': 0}\n",
    "    \n",
    "    def extract(self, sources):\n",
    "        \"\"\"EXTRACT: Pull data from multiple sources\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"EXTRACT STAGE\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        all_data = []\n",
    "        for source_name, data in sources.items():\n",
    "            print(f\"  Extracting from {source_name}: {len(data)} records\")\n",
    "            data['source'] = source_name\n",
    "            all_data.append(data)\n",
    "        \n",
    "        self.raw_data = pd.concat(all_data, ignore_index=True)\n",
    "        self.stats['extracted'] = len(self.raw_data)\n",
    "        print(f\"\\n  Total extracted: {self.stats['extracted']} records\")\n",
    "        return self\n",
    "    \n",
    "    def transform(self):\n",
    "        \"\"\"TRANSFORM: Cleanse, map, format\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TRANSFORM STAGE\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        df = self.raw_data.copy()\n",
    "        initial_count = len(df)\n",
    "        \n",
    "        # 1. Cleanse: Remove nulls and duplicates\n",
    "        null_count = df.isnull().any(axis=1).sum()\n",
    "        df = df.dropna()\n",
    "        print(f\"  [Cleanse] Removed {null_count} rows with nulls\")\n",
    "        \n",
    "        dup_count = df.duplicated().sum()\n",
    "        df = df.drop_duplicates()\n",
    "        print(f\"  [Cleanse] Removed {dup_count} duplicate rows\")\n",
    "        \n",
    "        # 2. Map: Standardize column names and values\n",
    "        if 'timestamp' in df.columns:\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            print(\"  [Map] Converted timestamp to datetime\")\n",
    "        \n",
    "        if 'action' in df.columns:\n",
    "            df['action'] = df['action'].str.lower()\n",
    "            print(\"  [Map] Standardized action values to lowercase\")\n",
    "        \n",
    "        # 3. Format: Create derived fields\n",
    "        if 'timestamp' in df.columns:\n",
    "            df['hour_of_day'] = df['timestamp'].dt.hour\n",
    "            df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "            print(\"  [Format] Created hour_of_day and day_of_week\")\n",
    "        \n",
    "        self.transformed_data = df\n",
    "        self.stats['transformed'] = len(df)\n",
    "        \n",
    "        print(f\"\\n  Transformed: {initial_count} → {self.stats['transformed']} records\")\n",
    "        print(f\"  Dropped: {initial_count - self.stats['transformed']} records\")\n",
    "        return self\n",
    "    \n",
    "    def load(self, destination='data_warehouse'):\n",
    "        \"\"\"LOAD: Store to destination\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"LOAD STAGE\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        self.stats['loaded'] = len(self.transformed_data)\n",
    "        print(f\"  Loaded {self.stats['loaded']} records to {destination}\")\n",
    "        print(f\"\\n  Final schema:\")\n",
    "        print(f\"  {list(self.transformed_data.columns)}\")\n",
    "        return self.transformed_data\n",
    "    \n",
    "    def summary(self):\n",
    "        \"\"\"Print pipeline summary\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ETL PIPELINE SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"  Extracted: {self.stats['extracted']}\")\n",
    "        print(f\"  Transformed: {self.stats['transformed']}\")\n",
    "        print(f\"  Loaded: {self.stats['loaded']}\")\n",
    "        print(f\"  Data loss: {self.stats['extracted'] - self.stats['loaded']} records \"\n",
    "              f\"({(1 - self.stats['loaded']/self.stats['extracted'])*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample source data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Source 1: Click logs\n",
    "click_logs = pd.DataFrame({\n",
    "    'user_id': np.random.randint(1, 1001, 5000),\n",
    "    'post_id': np.random.randint(1, 2001, 5000),\n",
    "    'action': np.random.choice(['view', 'like', 'share', None], 5000, p=[0.65, 0.2, 0.1, 0.05]),\n",
    "    'timestamp': pd.date_range('2024-01-01', periods=5000, freq='2min'),\n",
    "})\n",
    "\n",
    "# Source 2: Mobile app events\n",
    "mobile_events = pd.DataFrame({\n",
    "    'user_id': np.random.randint(1, 1001, 3000),\n",
    "    'post_id': np.random.randint(1, 2001, 3000),\n",
    "    'action': np.random.choice(['VIEW', 'LIKE', 'COMMENT'], 3000, p=[0.6, 0.25, 0.15]),\n",
    "    'timestamp': pd.date_range('2024-01-01', periods=3000, freq='3min'),\n",
    "})\n",
    "\n",
    "# Run ETL\n",
    "pipeline = ETLPipeline()\n",
    "result = (pipeline\n",
    "    .extract({'click_logs': click_logs, 'mobile_events': mobile_events})\n",
    "    .transform()\n",
    "    .load('feature_store'))\n",
    "\n",
    "pipeline.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show transformed data sample\n",
    "print(\"\\nTransformed Data Sample:\")\n",
    "print(result.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Tradeoffs (Chapter-Aligned)\n",
    "\n",
    "| Tradeoff | Discussion | Interview Signal |\n",
    "|----------|------------|------------------|\n",
    "| **SQL vs NoSQL** | ACID compliance vs horizontal scaling | E5: Knows when each applies. E6: Proposes hybrid (SQL for users, NoSQL for events) |\n",
    "| **Batch vs Streaming ETL** | Throughput vs freshness | E5: Understands difference. E6: Designs for different freshness SLAs |\n",
    "| **Data Warehouse vs Data Lake** | Schema-on-write vs schema-on-read | E5: Can explain both. E6: Discusses cost/flexibility tradeoffs at scale |\n",
    "| **Denormalization** | Query speed vs storage/consistency | E5: Knows tradeoff. E6: Discusses specific denormalization for ML features |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Meta Interview Signal (Detailed)\n",
    "\n",
    "### E5 Answer Expectations\n",
    "\n",
    "- Understands ETL stages and what happens in each\n",
    "- Can choose appropriate database type for a use case\n",
    "- Knows difference between structured and unstructured data\n",
    "- Can explain data sources: user-generated vs system-generated\n",
    "\n",
    "### E6 Additions\n",
    "\n",
    "- **Data quality at scale**: \"At Meta scale, we need data validation pipelines that catch schema drift before it breaks models\"\n",
    "- **Freshness SLAs**: \"Real-time features need sub-second freshness, while daily aggregates can have 24h delay\"\n",
    "- **Data lineage**: \"We track data lineage so when a feature breaks, we can trace back to the source\"\n",
    "- **Hybrid storage**: \"User profiles in SQL for ACID, event logs in Cassandra for write throughput, features in Redis for low-latency serving\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Interview Drills\n",
    "\n",
    "### Drill 1: Database Selection\n",
    "For each use case, choose the appropriate database type and justify:\n",
    "- User profiles with relationships (friends)\n",
    "- Real-time feature lookup during inference\n",
    "- Storing 1B daily events for training\n",
    "- Flexible content storage (posts with varying fields)\n",
    "\n",
    "### Drill 2: ETL Design\n",
    "Design an ETL pipeline for a video recommendation system:\n",
    "- What sources would you extract from?\n",
    "- What transformations are needed?\n",
    "- Where would you load the results?\n",
    "\n",
    "### Drill 3: Data Source Analysis\n",
    "For a harmful content detection system:\n",
    "- List 3 user-generated data sources\n",
    "- List 3 system-generated data sources\n",
    "- Which is more trustworthy and why?\n",
    "\n",
    "### Drill 4: Structured vs Unstructured\n",
    "Classify these data types and explain how you'd process each:\n",
    "- User age\n",
    "- Post text\n",
    "- Profile photo\n",
    "- Number of followers\n",
    "- Account creation date\n",
    "\n",
    "### Drill 5: Scale Discussion (E6)\n",
    "Discuss how ETL changes at Meta scale:\n",
    "- 1B daily active users\n",
    "- 100B events per day\n",
    "- Sub-second freshness requirements for some features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

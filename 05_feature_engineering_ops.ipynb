{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Feature Engineering Operations\n",
    "\n",
    "---\n",
    "\n",
    "## What the Chapter Says\n",
    "\n",
    "The chapter covers **Feature Engineering** for structured data with these specific operations:\n",
    "\n",
    "1. **Missing Values**: deletion (row/column) vs imputation (defaults, mean/median/mode)\n",
    "2. **Feature Scaling**: normalization (min-max), standardization (z-score), log scaling\n",
    "3. **Discretization/Bucketing**: continuous → categorical (with age bucketing example)\n",
    "4. **Encoding Categorical Features**: integer encoding, one-hot encoding, embedding learning\n",
    "\n",
    "Each operation includes pros and cons as specified in the chapter.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta Interview Signal\n",
    "\n",
    "| Level | Expectations |\n",
    "|-------|-------------|\n",
    "| **E5** | Knows all operations and when to use each. Can implement them. Understands pros/cons. |\n",
    "| **E6** | Discusses feature engineering at scale (batch vs online features). Proposes feature store design. Considers feature drift and freshness. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup: Create Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder, OneHotEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create synthetic user engagement data (matching chapter examples)\n",
    "n = 1000\n",
    "\n",
    "raw_data = pd.DataFrame({\n",
    "    'user_id': range(n),\n",
    "    \n",
    "    # Numerical features with various issues\n",
    "    'age': np.random.choice(\n",
    "        list(range(13, 80)) + [None], n, \n",
    "        p=[0.01]*67 + [0.33]  # 33% missing\n",
    "    ),\n",
    "    'watch_time_sec': np.random.exponential(300, n),  # Highly skewed\n",
    "    'num_sessions': np.random.poisson(10, n),\n",
    "    'follower_count': np.random.pareto(2, n) * 100,  # Power law, needs log\n",
    "    \n",
    "    # Categorical features\n",
    "    'device': np.random.choice(['iOS', 'Android', 'Web', None], n, p=[0.35, 0.35, 0.25, 0.05]),\n",
    "    'country': np.random.choice(['US', 'UK', 'IN', 'BR', 'DE', 'JP', 'FR', 'CA', 'AU', 'MX'], n),\n",
    "    'content_category': np.random.choice(['sports', 'music', 'news', 'comedy', 'education', 'gaming'], n),\n",
    "    \n",
    "    # Target: did user engage? (like/share)\n",
    "    'engaged': np.random.choice([0, 1], n, p=[0.7, 0.3])\n",
    "})\n",
    "\n",
    "# Convert age to numeric (with NaN)\n",
    "raw_data['age'] = pd.to_numeric(raw_data['age'], errors='coerce')\n",
    "\n",
    "print(\"RAW DATA OVERVIEW\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Shape: {raw_data.shape}\")\n",
    "print(f\"\\nMissing values:\")\n",
    "print(raw_data.isnull().sum())\n",
    "print(f\"\\nSample:\")\n",
    "print(raw_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1) Missing Values (Chapter Content)\n",
    "\n",
    "The chapter specifies two approaches:\n",
    "\n",
    "| Approach | Methods | Drawback |\n",
    "|----------|---------|----------|\n",
    "| **Deletion** | Row deletion, Column deletion | Reduces data quantity |\n",
    "| **Imputation** | Defaults, Mean/Median/Mode | Introduces noise |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate missing value handling\n",
    "print(\"=\"*60)\n",
    "print(\"MISSING VALUES HANDLING (Chapter Methods)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df = raw_data.copy()\n",
    "print(f\"\\nOriginal data: {len(df)} rows\")\n",
    "print(f\"Missing values: age={df['age'].isnull().sum()}, device={df['device'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Row Deletion\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"METHOD 1: Row Deletion\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "df_row_deleted = df.dropna(subset=['age', 'device'])\n",
    "rows_lost = len(df) - len(df_row_deleted)\n",
    "\n",
    "print(f\"After row deletion: {len(df_row_deleted)} rows\")\n",
    "print(f\"Rows lost: {rows_lost} ({rows_lost/len(df)*100:.1f}%)\")\n",
    "print(f\"\\n[Drawback]: Reduces data quantity - lost {rows_lost/len(df)*100:.1f}% of data!\")\n",
    "print(\"[When to use]: When missing data is random and you have plenty of data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Column Deletion\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"METHOD 2: Column Deletion\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# If a column has >50% missing, consider dropping\n",
    "missing_pct = df.isnull().sum() / len(df) * 100\n",
    "print(\"Missing percentage per column:\")\n",
    "print(missing_pct[missing_pct > 0])\n",
    "\n",
    "# In this case, age has 33% missing - borderline\n",
    "print(f\"\\n[Drawback]: Lose the entire feature's predictive power\")\n",
    "print(\"[When to use]: When column has very high missing rate (>50-70%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Imputation - Default Value\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"METHOD 3: Imputation - Default Value\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "df_imputed_default = df.copy()\n",
    "df_imputed_default['device'] = df_imputed_default['device'].fillna('Unknown')\n",
    "df_imputed_default['age'] = df_imputed_default['age'].fillna(-1)  # Sentinel value\n",
    "\n",
    "print(f\"Device value counts after imputation:\")\n",
    "print(df_imputed_default['device'].value_counts())\n",
    "print(f\"\\n[Drawback]: Introduces artificial category; sentinel values can confuse models\")\n",
    "print(\"[When to use]: For categorical features where 'unknown' is meaningful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 4: Imputation - Mean/Median/Mode\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"METHOD 4: Imputation - Mean/Median/Mode\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "df_imputed_stats = df.copy()\n",
    "\n",
    "# Numerical: Use median (robust to outliers)\n",
    "age_median = df_imputed_stats['age'].median()\n",
    "df_imputed_stats['age'] = df_imputed_stats['age'].fillna(age_median)\n",
    "\n",
    "# Categorical: Use mode (most frequent)\n",
    "device_mode = df_imputed_stats['device'].mode()[0]\n",
    "df_imputed_stats['device'] = df_imputed_stats['device'].fillna(device_mode)\n",
    "\n",
    "print(f\"Age imputed with median: {age_median}\")\n",
    "print(f\"Device imputed with mode: {device_mode}\")\n",
    "print(f\"\\n[Drawback]: Introduces noise - all missing ages now look the same\")\n",
    "print(\"[When to use]: When missing is random and feature is important\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of imputation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Original age distribution (without NaN)\n",
    "axes[0].hist(df['age'].dropna(), bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[0].axvline(age_median, color='red', linestyle='--', linewidth=2, label=f'Median={age_median}')\n",
    "axes[0].set_title('Original Age Distribution')\n",
    "axes[0].set_xlabel('Age')\n",
    "axes[0].legend()\n",
    "\n",
    "# After imputation\n",
    "axes[1].hist(df_imputed_stats['age'], bins=30, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[1].axvline(age_median, color='red', linestyle='--', linewidth=2, label=f'Spike at median')\n",
    "axes[1].set_title('After Median Imputation')\n",
    "axes[1].set_xlabel('Age')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n[Notice]: The spike at the median is the 'noise' introduced by imputation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2) Feature Scaling (Chapter Content)\n",
    "\n",
    "The chapter specifies three methods:\n",
    "\n",
    "| Method | Idea | Use Case |\n",
    "|--------|------|----------|\n",
    "| **Normalization** | Min-max scaling to [0, 1] | Bounded range needed |\n",
    "| **Standardization** | Z-score (mean=0, std=1) | Gaussian-like data |\n",
    "| **Log Scaling** | Reduce skewness | Power-law distributions |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"FEATURE SCALING (Chapter Methods)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use imputed data\n",
    "df_scaling = df_imputed_stats.copy()\n",
    "\n",
    "# Show original distributions\n",
    "print(\"\\nOriginal feature statistics:\")\n",
    "print(df_scaling[['age', 'watch_time_sec', 'follower_count']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Normalization (Min-Max)\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"METHOD 1: Normalization (Min-Max Scaling)\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "scaler_minmax = MinMaxScaler()\n",
    "age_normalized = scaler_minmax.fit_transform(df_scaling[['age']])\n",
    "\n",
    "print(f\"Formula: x_scaled = (x - min) / (max - min)\")\n",
    "print(f\"\\nOriginal age: min={df_scaling['age'].min()}, max={df_scaling['age'].max()}\")\n",
    "print(f\"Normalized age: min={age_normalized.min():.3f}, max={age_normalized.max():.3f}\")\n",
    "print(f\"\\n[Use case]: When you need bounded range [0, 1]\")\n",
    "print(\"[Drawback]: Sensitive to outliers (outlier becomes 1.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Standardization (Z-score)\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"METHOD 2: Standardization (Z-score)\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "scaler_std = StandardScaler()\n",
    "age_standardized = scaler_std.fit_transform(df_scaling[['age']])\n",
    "\n",
    "print(f\"Formula: z = (x - mean) / std\")\n",
    "print(f\"\\nOriginal age: mean={df_scaling['age'].mean():.2f}, std={df_scaling['age'].std():.2f}\")\n",
    "print(f\"Standardized age: mean={age_standardized.mean():.6f}, std={age_standardized.std():.3f}\")\n",
    "print(f\"\\n[Use case]: When data is roughly Gaussian, algorithms assume zero mean\")\n",
    "print(\"[Benefit]: Not bounded, preserves outlier information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Log Scaling (for skewed data)\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"METHOD 3: Log Scaling\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Follower count is heavily skewed (power law)\n",
    "print(f\"follower_count statistics before log:\")\n",
    "print(f\"  mean: {df_scaling['follower_count'].mean():.2f}\")\n",
    "print(f\"  median: {df_scaling['follower_count'].median():.2f}\")\n",
    "print(f\"  skewness: {df_scaling['follower_count'].skew():.2f}\")\n",
    "\n",
    "# Apply log transform (add 1 to handle zeros)\n",
    "follower_log = np.log1p(df_scaling['follower_count'])\n",
    "\n",
    "print(f\"\\nAfter log transform:\")\n",
    "print(f\"  mean: {follower_log.mean():.2f}\")\n",
    "print(f\"  median: {follower_log.median():.2f}\")\n",
    "print(f\"  skewness: {follower_log.skew():.2f}\")\n",
    "print(f\"\\n[Use case]: Power-law distributions (followers, likes, views)\")\n",
    "print(\"[Benefit]: Reduces skewness, speeds up convergence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize scaling effects\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "\n",
    "# Row 1: Age scaling\n",
    "axes[0, 0].hist(df_scaling['age'], bins=30, color='blue', alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_title('Age: Original')\n",
    "\n",
    "axes[0, 1].hist(age_normalized, bins=30, color='green', alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].set_title('Age: Normalized [0,1]')\n",
    "\n",
    "axes[0, 2].hist(age_standardized, bins=30, color='orange', alpha=0.7, edgecolor='black')\n",
    "axes[0, 2].set_title('Age: Standardized (z-score)')\n",
    "\n",
    "# Row 2: Follower count (skewed)\n",
    "axes[1, 0].hist(df_scaling['follower_count'], bins=50, color='blue', alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_title(f'Followers: Original (skew={df_scaling[\"follower_count\"].skew():.2f})')\n",
    "\n",
    "axes[1, 1].hist(follower_log, bins=30, color='purple', alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].set_title(f'Followers: Log Transformed (skew={follower_log.skew():.2f})')\n",
    "\n",
    "axes[1, 2].axis('off')\n",
    "axes[1, 2].text(0.1, 0.7, 'Scaling Summary:', fontsize=12, fontweight='bold')\n",
    "axes[1, 2].text(0.1, 0.5, '• Normalization: [0,1] range', fontsize=10)\n",
    "axes[1, 2].text(0.1, 0.35, '• Standardization: mean=0, std=1', fontsize=10)\n",
    "axes[1, 2].text(0.1, 0.2, '• Log: reduces skewness', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3) Discretization / Bucketing (Chapter Content)\n",
    "\n",
    "The chapter specifies:\n",
    "- Turn continuous features into categorical buckets\n",
    "- **Age bucketing example**: 0-9, 10-19, 20-39, 40-59, 60+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DISCRETIZATION / BUCKETING (Chapter Content)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df_bucket = df_imputed_stats.copy()\n",
    "\n",
    "# Chapter's exact age bucketing example\n",
    "age_bins = [0, 9, 19, 39, 59, 100]\n",
    "age_labels = ['0-9', '10-19', '20-39', '40-59', '60+']\n",
    "\n",
    "df_bucket['age_bucket'] = pd.cut(df_bucket['age'], bins=age_bins, labels=age_labels)\n",
    "\n",
    "print(\"\\nAge Bucketing (Chapter Example)\")\n",
    "print(f\"Bins: {age_bins}\")\n",
    "print(f\"Labels: {age_labels}\")\n",
    "print(f\"\\nBucket distribution:\")\n",
    "print(df_bucket['age_bucket'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucket watch time (quintiles)\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"Watch Time Bucketing (Quantile-based)\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "df_bucket['watch_time_bucket'] = pd.qcut(\n",
    "    df_bucket['watch_time_sec'], \n",
    "    q=5, \n",
    "    labels=['very_low', 'low', 'medium', 'high', 'very_high']\n",
    ")\n",
    "\n",
    "print(f\"\\nBucket distribution:\")\n",
    "print(df_bucket['watch_time_bucket'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize bucketing\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Age buckets\n",
    "df_bucket['age_bucket'].value_counts().sort_index().plot(kind='bar', ax=axes[0], color='#4CAF50', edgecolor='black')\n",
    "axes[0].set_title('Age Buckets (Chapter Example)')\n",
    "axes[0].set_xlabel('Age Bucket')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Watch time buckets\n",
    "order = ['very_low', 'low', 'medium', 'high', 'very_high']\n",
    "df_bucket['watch_time_bucket'].value_counts()[order].plot(kind='bar', ax=axes[1], color='#2196F3', edgecolor='black')\n",
    "axes[1].set_title('Watch Time Buckets (Quantile-based)')\n",
    "axes[1].set_xlabel('Watch Time Bucket')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n[When to use]: Non-linear relationships, tree-based models, interpretability\")\n",
    "print(\"[Drawback]: Loss of granularity, boundary effects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4) Encoding Categorical Features (Chapter Content)\n",
    "\n",
    "The chapter specifies:\n",
    "\n",
    "| Method | When to Use | Issue |\n",
    "|--------|-------------|-------|\n",
    "| **Integer Encoding** | Only when ordinal relationship exists | Implies order when there isn't |\n",
    "| **One-Hot Encoding** | Nominal categories | High-cardinality explosion |\n",
    "| **Embedding Learning** | High-cardinality categories | Requires training |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ENCODING CATEGORICAL FEATURES (Chapter Content)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df_encode = df_imputed_stats.copy()\n",
    "print(f\"\\nCategorical features to encode:\")\n",
    "print(f\"  device: {df_encode['device'].nunique()} unique values\")\n",
    "print(f\"  country: {df_encode['country'].nunique()} unique values\")\n",
    "print(f\"  content_category: {df_encode['content_category'].nunique()} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Integer Encoding (for ordinal data ONLY)\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"METHOD 1: Integer Encoding\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Create an ordinal feature (e.g., engagement level)\n",
    "df_encode['engagement_level'] = pd.cut(\n",
    "    df_encode['watch_time_sec'],\n",
    "    bins=[0, 60, 180, 600, float('inf')],\n",
    "    labels=['none', 'low', 'medium', 'high']\n",
    ")\n",
    "\n",
    "# Integer encoding makes sense here (there IS an order)\n",
    "engagement_mapping = {'none': 0, 'low': 1, 'medium': 2, 'high': 3}\n",
    "df_encode['engagement_encoded'] = df_encode['engagement_level'].map(engagement_mapping)\n",
    "\n",
    "print(f\"Ordinal encoding for engagement_level:\")\n",
    "print(f\"  {engagement_mapping}\")\n",
    "print(f\"\\n[When to use]: ONLY when ordinal relationship exists (low < medium < high)\")\n",
    "print(\"[Drawback]: If used on nominal data, implies false ordering (iOS < Android < Web?)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: One-Hot Encoding\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"METHOD 2: One-Hot Encoding\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# One-hot encode device (nominal, low cardinality)\n",
    "device_onehot = pd.get_dummies(df_encode['device'], prefix='device')\n",
    "\n",
    "print(f\"Original 'device' column: 1 column with {df_encode['device'].nunique()} categories\")\n",
    "print(f\"After one-hot: {device_onehot.shape[1]} columns\")\n",
    "print(f\"\\nOne-hot columns: {list(device_onehot.columns)}\")\n",
    "print(f\"\\nSample:\")\n",
    "print(pd.concat([df_encode['device'].head(), device_onehot.head()], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding failure case: high cardinality\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"ONE-HOT ENCODING: High Cardinality Problem\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Simulate high-cardinality feature (e.g., user_id, product_id)\n",
    "n_unique_items = 100000  # 100K products\n",
    "\n",
    "print(f\"\\nScenario: 100,000 unique product IDs\")\n",
    "print(f\"One-hot encoding would create: 100,000 binary columns!\")\n",
    "print(f\"Memory for 1M rows: ~100GB (assuming 1 byte per value)\")\n",
    "print(f\"\\n[Problem]: Sparse, high-dimensional, slow training\")\n",
    "print(\"[Solution]: Use embedding learning instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Embedding Learning (conceptual)\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"METHOD 3: Embedding Learning\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "print(\"\"\"\n",
    "For HIGH-CARDINALITY categorical features:\n",
    "\n",
    "Instead of:  user_id → [0, 0, 0, 1, 0, 0, ..., 0]  (100K dimensions)\n",
    "\n",
    "Learn:       user_id → [0.23, -0.15, 0.87, 0.42]  (embedding, e.g., 64 dimensions)\n",
    "\n",
    "Benefits:\n",
    "• Dense, low-dimensional representation\n",
    "• Captures semantic similarity (similar users have similar embeddings)\n",
    "• Learned during model training\n",
    "\n",
    "Used in:\n",
    "• Recommendation systems (user/item embeddings)\n",
    "• NLP (word embeddings like Word2Vec)\n",
    "• Ads CTR prediction (ad/user embeddings)\n",
    "\"\"\")\n",
    "\n",
    "# Simulate learned embeddings\n",
    "np.random.seed(42)\n",
    "embedding_dim = 8\n",
    "n_categories = 1000\n",
    "\n",
    "# Pretend these are learned embeddings\n",
    "fake_embeddings = np.random.randn(n_categories, embedding_dim)\n",
    "\n",
    "print(f\"\\nSimulated embedding table shape: {fake_embeddings.shape}\")\n",
    "print(f\"  {n_categories} categories × {embedding_dim} dimensions\")\n",
    "print(f\"\\nSample embedding for category 0:\")\n",
    "print(f\"  {fake_embeddings[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## E4) Data Prep Talking Points Checklist (Chapter Content)\n",
    "\n",
    "The chapter requires discussing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "talking_points = pd.DataFrame({\n",
    "    'Topic': [\n",
    "        'Availability / Collection / Size / Freshness',\n",
    "        'Storage',\n",
    "        'Feature Engineering',\n",
    "        'Privacy',\n",
    "        'Bias'\n",
    "    ],\n",
    "    'Questions to Address': [\n",
    "        'What data exists? How is it collected? How much? How fresh?',\n",
    "        'Cloud vs device? Formats? Multimodal data?',\n",
    "        'Missing data handling? Normalization? Constructed features? Combining text+numbers+images?',\n",
    "        'Sensitive data? Anonymization? On-device constraints?',\n",
    "        'What kinds of bias? How to mitigate?'\n",
    "    ],\n",
    "    'Example Answer': [\n",
    "        'We have 1 year of click logs, updated hourly, 10TB total',\n",
    "        'Event logs in cloud data warehouse, features served from Redis',\n",
    "        'Median imputation for age, log transform for followers, embeddings for user IDs',\n",
    "        'Remove PII, train on aggregated stats, differential privacy for sensitive features',\n",
    "        'Selection bias in training data, mitigate with stratified sampling'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"DATA PREP TALKING POINTS CHECKLIST (Chapter Content)\")\n",
    "print(\"=\"*100)\n",
    "for _, row in talking_points.iterrows():\n",
    "    print(f\"\\n{row['Topic']}\")\n",
    "    print(f\"  Q: {row['Questions to Address']}\")\n",
    "    print(f\"  Example: {row['Example Answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Complete Feature Engineering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete pipeline combining all operations\n",
    "def feature_engineering_pipeline(df):\n",
    "    \"\"\"Complete feature engineering pipeline (Chapter operations)\"\"\"\n",
    "    result = df.copy()\n",
    "    \n",
    "    print(\"FEATURE ENGINEERING PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Input shape: {result.shape}\")\n",
    "    \n",
    "    # 1. Handle missing values\n",
    "    print(\"\\n1. Handling missing values...\")\n",
    "    result['age'] = result['age'].fillna(result['age'].median())\n",
    "    result['device'] = result['device'].fillna('Unknown')\n",
    "    print(f\"   Imputed age with median, device with 'Unknown'\")\n",
    "    \n",
    "    # 2. Feature scaling\n",
    "    print(\"\\n2. Scaling features...\")\n",
    "    result['age_scaled'] = StandardScaler().fit_transform(result[['age']])\n",
    "    result['watch_time_log'] = np.log1p(result['watch_time_sec'])\n",
    "    result['follower_log'] = np.log1p(result['follower_count'])\n",
    "    print(f\"   Standardized age, log-transformed watch_time and followers\")\n",
    "    \n",
    "    # 3. Discretization\n",
    "    print(\"\\n3. Bucketing continuous features...\")\n",
    "    age_bins = [0, 9, 19, 39, 59, 100]\n",
    "    age_labels = ['0-9', '10-19', '20-39', '40-59', '60+']\n",
    "    result['age_bucket'] = pd.cut(result['age'], bins=age_bins, labels=age_labels)\n",
    "    print(f\"   Created age buckets: {age_labels}\")\n",
    "    \n",
    "    # 4. Encode categoricals\n",
    "    print(\"\\n4. Encoding categorical features...\")\n",
    "    device_dummies = pd.get_dummies(result['device'], prefix='device')\n",
    "    result = pd.concat([result, device_dummies], axis=1)\n",
    "    print(f\"   One-hot encoded device: {list(device_dummies.columns)}\")\n",
    "    \n",
    "    # For high-cardinality, we'd use embeddings (simulated here)\n",
    "    result['country_encoded'] = LabelEncoder().fit_transform(result['country'])\n",
    "    print(f\"   Integer encoded country (would use embeddings in production)\")\n",
    "    \n",
    "    print(f\"\\nOutput shape: {result.shape}\")\n",
    "    return result\n",
    "\n",
    "# Run pipeline\n",
    "processed_data = feature_engineering_pipeline(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show processed data\n",
    "print(\"\\nProcessed Data Sample:\")\n",
    "print(processed_data.head())\n",
    "\n",
    "print(f\"\\nNew columns created:\")\n",
    "new_cols = [c for c in processed_data.columns if c not in raw_data.columns]\n",
    "print(new_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Tradeoffs (Chapter-Aligned)\n",
    "\n",
    "| Tradeoff | Discussion | Interview Signal |\n",
    "|----------|------------|------------------|\n",
    "| **Deletion vs Imputation** | Lose data vs introduce noise | E5: Knows both. E6: Discusses impact on model bias |\n",
    "| **Normalization vs Standardization** | Bounded range vs unbounded | E5: Knows when to use each. E6: Discusses outlier handling |\n",
    "| **One-Hot vs Embeddings** | Simple vs learnable | E5: Understands cardinality issue. E6: Proposes embedding strategies |\n",
    "| **Bucketing granularity** | Fine vs coarse buckets | E5: Can implement. E6: Discusses business-relevant buckets |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Meta Interview Signal (Detailed)\n",
    "\n",
    "### E5 Answer Expectations\n",
    "\n",
    "- Knows all four feature engineering operations (missing values, scaling, bucketing, encoding)\n",
    "- Can implement each with sklearn/pandas\n",
    "- Understands pros and cons of each method\n",
    "- Can explain when to use each approach\n",
    "\n",
    "### E6 Additions\n",
    "\n",
    "- **Feature store design**: \"We compute batch features daily and real-time features on-the-fly, served from Redis\"\n",
    "- **Feature drift**: \"We monitor feature distributions and alert when they drift from training\"\n",
    "- **Embedding strategies**: \"For cold-start users, we use average embeddings or fallback to demographic features\"\n",
    "- **Scale considerations**: \"At Meta scale, feature engineering is a distributed Spark job running on 1000s of nodes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Interview Drills\n",
    "\n",
    "### Drill 1: Missing Value Strategy\n",
    "For each scenario, choose deletion or imputation and justify:\n",
    "- 5% of user ages are missing\n",
    "- 60% of a new feature \"last_purchase_date\" is missing\n",
    "- 2% of target labels are missing\n",
    "\n",
    "### Drill 2: Scaling Selection\n",
    "Choose the appropriate scaling method:\n",
    "- User age for a neural network\n",
    "- Number of followers (power law distribution)\n",
    "- Temperature in Celsius for a linear regression\n",
    "\n",
    "### Drill 3: Age Bucketing\n",
    "Reproduce the chapter's age bucketing example from memory: 0-9, 10-19, 20-39, 40-59, 60+\n",
    "\n",
    "### Drill 4: Encoding Decision\n",
    "For each feature, choose integer, one-hot, or embedding encoding:\n",
    "- Device type (iOS, Android, Web)\n",
    "- User ID (100M users)\n",
    "- Product category (100 categories)\n",
    "- Rating (1-5 stars)\n",
    "\n",
    "### Drill 5: Complete Pipeline\n",
    "Design a feature engineering pipeline for a video recommendation system. Include:\n",
    "- What features would you extract?\n",
    "- How would you handle missing values?\n",
    "- What scaling would you apply?\n",
    "- How would you encode categorical features?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

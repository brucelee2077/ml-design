{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 - Evaluation: Offline and Online Metrics\n",
    "\n",
    "---\n",
    "\n",
    "## What the Chapter Says\n",
    "\n",
    "The chapter covers **Evaluation = Offline + Online**:\n",
    "\n",
    "### Offline Metrics (Chapter Table)\n",
    "| Task | Metrics |\n",
    "|------|--------|\n",
    "| Classification | Precision, Recall, F1, Accuracy, ROC-AUC, PR-AUC, Confusion Matrix |\n",
    "| Regression | MSE, MAE, RMSE |\n",
    "| Ranking | Precision@k, Recall@k, MRR, mAP, nDCG |\n",
    "| Image Generation | FID, Inception Score |\n",
    "| NLP | BLEU, METEOR, ROUGE, CIDEr, SPICE |\n",
    "\n",
    "### Online Metrics (Chapter Table)\n",
    "| System | Metrics |\n",
    "|--------|--------|\n",
    "| Ads | CTR, Revenue lift |\n",
    "| Harmful detection | Prevalence, Valid appeals |\n",
    "| Video recommendation | CTR, Watch time, Completed videos |\n",
    "| Friend recommendation | Requests sent per day, Requests accepted per day |\n",
    "\n",
    "The chapter also mentions **fairness/bias questions** and **business sense** emphasis.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta Interview Signal\n",
    "\n",
    "| Level | Expectations |\n",
    "|-------|-------------|\n",
    "| **E5** | Knows offline metrics for each task type. Can compute and interpret them. Understands online metrics relevance. |\n",
    "| **E6** | Ties metrics to business impact. Discusses metric tradeoffs (precision vs recall). Proposes fairness metrics. Designs A/B test metrics. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score, confusion_matrix,\n",
    "    mean_squared_error, mean_absolute_error,\n",
    "    precision_recall_curve, roc_curve\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Offline Evaluation\n",
    "\n",
    "**Definition**: Evaluation on held-out test data before deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics (Chapter Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification metrics from chapter\n",
    "classification_metrics = pd.DataFrame({\n",
    "    'Metric': [\n",
    "        'Precision',\n",
    "        'Recall',\n",
    "        'F1 Score',\n",
    "        'Accuracy',\n",
    "        'ROC-AUC',\n",
    "        'PR-AUC',\n",
    "        'Confusion Matrix'\n",
    "    ],\n",
    "    'Formula/Definition': [\n",
    "        'TP / (TP + FP)',\n",
    "        'TP / (TP + FN)',\n",
    "        '2 * (P * R) / (P + R)',\n",
    "        '(TP + TN) / Total',\n",
    "        'Area under ROC curve',\n",
    "        'Area under Precision-Recall curve',\n",
    "        'Matrix of TP, TN, FP, FN'\n",
    "    ],\n",
    "    'Use When': [\n",
    "        'Cost of false positives is high (spam detection)',\n",
    "        'Cost of false negatives is high (fraud, disease)',\n",
    "        'Need balance of precision and recall',\n",
    "        'Balanced classes (not recommended for imbalanced)',\n",
    "        'Comparing models, threshold-independent',\n",
    "        'Imbalanced datasets (better than ROC-AUC)',\n",
    "        'Understanding error types'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(\"CLASSIFICATION METRICS (Chapter Table)\")\n",
    "print(\"=\"*90)\n",
    "print(classification_metrics.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic classification data (CTR prediction)\n",
    "n = 2000\n",
    "y_true = np.random.choice([0, 1], n, p=[0.9, 0.1])  # 10% CTR (imbalanced)\n",
    "\n",
    "# Simulate model predictions (not perfect)\n",
    "y_scores = np.where(\n",
    "    y_true == 1,\n",
    "    np.random.beta(5, 2, n),  # Higher scores for positives\n",
    "    np.random.beta(2, 5, n)   # Lower scores for negatives\n",
    ")\n",
    "y_pred = (y_scores > 0.5).astype(int)\n",
    "\n",
    "print(\"SYNTHETIC CTR DATA\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Total samples: {n}\")\n",
    "print(f\"Positive rate: {y_true.mean()*100:.1f}%\")\n",
    "print(f\"Predicted positive rate: {y_pred.mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute all classification metrics\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CLASSIFICATION METRICS COMPUTATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Basic metrics\n",
    "print(f\"\\nPrecision: {precision_score(y_true, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_true, y_pred):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_true, y_pred):.4f}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_true, y_scores):.4f}\")\n",
    "print(f\"PR-AUC: {average_precision_score(y_true, y_scores):.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"            Predicted\")\n",
    "print(f\"             0     1\")\n",
    "print(f\"Actual 0  [{cm[0,0]:4d}  {cm[0,1]:4d}]\")\n",
    "print(f\"       1  [{cm[1,0]:4d}  {cm[1,1]:4d}]\")\n",
    "print(f\"\\nTP={cm[1,1]}, TN={cm[0,0]}, FP={cm[0,1]}, FN={cm[1,0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ROC and PR curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_true, y_scores)\n",
    "roc_auc = roc_auc_score(y_true, y_scores)\n",
    "\n",
    "axes[0].plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "axes[0].plot([0, 1], [0, 1], color='gray', linestyle='--', label='Random')\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('ROC Curve')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# PR Curve\n",
    "precision, recall, thresholds_pr = precision_recall_curve(y_true, y_scores)\n",
    "pr_auc = average_precision_score(y_true, y_scores)\n",
    "\n",
    "axes[1].plot(recall, precision, color='green', lw=2, label=f'PR curve (AUC = {pr_auc:.3f})')\n",
    "axes[1].axhline(y=y_true.mean(), color='gray', linestyle='--', label=f'Baseline (prevalence={y_true.mean():.2f})')\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Precision-Recall Curve')\n",
    "axes[1].legend(loc='upper right')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n[Key Insight]: PR-AUC is more informative for imbalanced datasets\")\n",
    "print(\"               because it doesn't get inflated by true negatives.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Metrics (Chapter Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression metrics from chapter\n",
    "regression_metrics = pd.DataFrame({\n",
    "    'Metric': ['MSE', 'MAE', 'RMSE'],\n",
    "    'Formula': [\n",
    "        '(1/n) * Σ(y - ŷ)²',\n",
    "        '(1/n) * Σ|y - ŷ|',\n",
    "        '√MSE'\n",
    "    ],\n",
    "    'Characteristics': [\n",
    "        'Penalizes large errors more (squared)',\n",
    "        'Robust to outliers (absolute)',\n",
    "        'Same unit as target variable'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"REGRESSION METRICS (Chapter Table)\")\n",
    "print(\"=\"*70)\n",
    "print(regression_metrics.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic regression data (watch time prediction)\n",
    "n = 1000\n",
    "y_true_reg = np.random.exponential(300, n)  # Watch time in seconds\n",
    "y_pred_reg = y_true_reg + np.random.normal(0, 50, n)  # Predictions with noise\n",
    "y_pred_reg = np.clip(y_pred_reg, 0, None)  # No negative watch time\n",
    "\n",
    "print(\"REGRESSION METRICS COMPUTATION (Watch Time)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nMSE: {mean_squared_error(y_true_reg, y_pred_reg):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_true_reg, y_pred_reg):.2f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_true_reg, y_pred_reg)):.2f} seconds\")\n",
    "\n",
    "print(f\"\\n[Interpretation]: On average, predictions are off by {mean_absolute_error(y_true_reg, y_pred_reg):.0f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking Metrics (Chapter Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking metrics from chapter\n",
    "ranking_metrics = pd.DataFrame({\n",
    "    'Metric': ['Precision@k', 'Recall@k', 'MRR', 'mAP', 'nDCG'],\n",
    "    'Definition': [\n",
    "        'Relevant items in top-k / k',\n",
    "        'Relevant items in top-k / total relevant',\n",
    "        'Mean Reciprocal Rank - 1/rank of first relevant',\n",
    "        'Mean Average Precision across queries',\n",
    "        'Normalized Discounted Cumulative Gain'\n",
    "    ],\n",
    "    'Use Case': [\n",
    "        'Are top-k results relevant?',\n",
    "        'How many relevant items are in top-k?',\n",
    "        'How quickly do we find first relevant item?',\n",
    "        'Overall ranking quality',\n",
    "        'Ranking quality with graded relevance'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RANKING METRICS (Chapter Table)\")\n",
    "print(\"=\"*80)\n",
    "print(ranking_metrics.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement ranking metrics\n",
    "def precision_at_k(relevant, retrieved, k):\n",
    "    \"\"\"Precision@k: fraction of top-k that are relevant\"\"\"\n",
    "    relevant_set = set(relevant)\n",
    "    retrieved_at_k = retrieved[:k]\n",
    "    return len([item for item in retrieved_at_k if item in relevant_set]) / k\n",
    "\n",
    "def recall_at_k(relevant, retrieved, k):\n",
    "    \"\"\"Recall@k: fraction of relevant items in top-k\"\"\"\n",
    "    relevant_set = set(relevant)\n",
    "    retrieved_at_k = retrieved[:k]\n",
    "    return len([item for item in retrieved_at_k if item in relevant_set]) / len(relevant)\n",
    "\n",
    "def mrr(relevant, retrieved):\n",
    "    \"\"\"Mean Reciprocal Rank\"\"\"\n",
    "    relevant_set = set(relevant)\n",
    "    for rank, item in enumerate(retrieved, 1):\n",
    "        if item in relevant_set:\n",
    "            return 1.0 / rank\n",
    "    return 0.0\n",
    "\n",
    "def ndcg_at_k(relevance_scores, k):\n",
    "    \"\"\"Normalized DCG@k\"\"\"\n",
    "    dcg = sum((2**rel - 1) / np.log2(i + 2) for i, rel in enumerate(relevance_scores[:k]))\n",
    "    ideal = sorted(relevance_scores, reverse=True)[:k]\n",
    "    idcg = sum((2**rel - 1) / np.log2(i + 2) for i, rel in enumerate(ideal))\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "# Example: Video recommendation\n",
    "print(\"RANKING METRICS EXAMPLE: Video Recommendations\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Ground truth: videos the user actually watched/liked\n",
    "relevant_videos = ['V1', 'V5', 'V8', 'V12']  # 4 relevant videos\n",
    "\n",
    "# Model's ranked output\n",
    "recommended = ['V3', 'V1', 'V7', 'V5', 'V2', 'V8', 'V4', 'V12', 'V9', 'V10']\n",
    "\n",
    "# Relevance scores for nDCG (graded: 3=highly relevant, 2=relevant, 1=somewhat, 0=not)\n",
    "relevance_scores = [0, 3, 0, 2, 0, 2, 0, 1, 0, 0]\n",
    "\n",
    "print(f\"\\nRelevant videos: {relevant_videos}\")\n",
    "print(f\"Recommended order: {recommended}\")\n",
    "print(f\"Relevance scores: {relevance_scores}\")\n",
    "\n",
    "for k in [3, 5, 10]:\n",
    "    print(f\"\\n--- k={k} ---\")\n",
    "    print(f\"Precision@{k}: {precision_at_k(relevant_videos, recommended, k):.3f}\")\n",
    "    print(f\"Recall@{k}: {recall_at_k(relevant_videos, recommended, k):.3f}\")\n",
    "\n",
    "print(f\"\\nMRR: {mrr(relevant_videos, recommended):.3f}\")\n",
    "print(f\"nDCG@10: {ndcg_at_k(relevance_scores, 10):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Metrics (Chapter Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP and Image Generation metrics from chapter\n",
    "other_metrics = pd.DataFrame({\n",
    "    'Domain': ['Image Generation', 'Image Generation', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP'],\n",
    "    'Metric': ['FID', 'Inception Score', 'BLEU', 'METEOR', 'ROUGE', 'CIDEr', 'SPICE'],\n",
    "    'Description': [\n",
    "        'Fréchet Inception Distance - distance between real/generated distributions',\n",
    "        'Quality and diversity of generated images',\n",
    "        'N-gram overlap with reference translations',\n",
    "        'Alignment, synonymy, and recall',\n",
    "        'Recall-oriented summary evaluation',\n",
    "        'Consensus-based image description evaluation',\n",
    "        'Semantic propositional evaluation'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(\"NLP & IMAGE GENERATION METRICS (Chapter Table)\")\n",
    "print(\"=\"*90)\n",
    "print(other_metrics.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Online Evaluation\n",
    "\n",
    "**Definition**: Evaluation in production with real users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online metrics from chapter table\n",
    "online_metrics = pd.DataFrame({\n",
    "    'System': [\n",
    "        'Ads',\n",
    "        'Ads',\n",
    "        'Harmful content detection',\n",
    "        'Harmful content detection',\n",
    "        'Video recommendation',\n",
    "        'Video recommendation',\n",
    "        'Video recommendation',\n",
    "        'Friend recommendation',\n",
    "        'Friend recommendation'\n",
    "    ],\n",
    "    'Metric': [\n",
    "        'CTR (Click-Through Rate)',\n",
    "        'Revenue lift',\n",
    "        'Prevalence',\n",
    "        'Valid appeals',\n",
    "        'CTR',\n",
    "        'Watch time',\n",
    "        'Completed videos',\n",
    "        'Requests sent per day',\n",
    "        'Requests accepted per day'\n",
    "    ],\n",
    "    'Definition': [\n",
    "        'Clicks / Impressions',\n",
    "        'Incremental revenue from model',\n",
    "        'Fraction of harmful content on platform',\n",
    "        'Appealed decisions overturned',\n",
    "        'Video clicks / Impressions',\n",
    "        'Total time users spend watching',\n",
    "        'Videos watched to completion',\n",
    "        'Friend requests users send',\n",
    "        'Friend requests accepted'\n",
    "    ],\n",
    "    'Higher/Lower is Better': [\n",
    "        'Higher',\n",
    "        'Higher',\n",
    "        'Lower',\n",
    "        'Lower',\n",
    "        'Higher',\n",
    "        'Higher',\n",
    "        'Higher',\n",
    "        'Higher',\n",
    "        'Higher'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(\"ONLINE METRICS (Chapter Table)\")\n",
    "print(\"=\"*90)\n",
    "print(online_metrics.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate online metrics for A/B test\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate 14 days of A/B test data\n",
    "n_days = 14\n",
    "users_per_day = 10000\n",
    "\n",
    "# Control group metrics\n",
    "control_ctr = np.random.normal(0.025, 0.002, n_days)  # ~2.5% CTR\n",
    "control_watch_time = np.random.normal(180, 10, n_days)  # ~180 sec average\n",
    "\n",
    "# Treatment group metrics (model improvement)\n",
    "treatment_ctr = np.random.normal(0.028, 0.002, n_days)  # ~2.8% CTR (+12%)\n",
    "treatment_watch_time = np.random.normal(195, 10, n_days)  # ~195 sec (+8%)\n",
    "\n",
    "ab_data = pd.DataFrame({\n",
    "    'Day': range(1, n_days + 1),\n",
    "    'Control CTR': control_ctr,\n",
    "    'Treatment CTR': treatment_ctr,\n",
    "    'Control Watch Time': control_watch_time,\n",
    "    'Treatment Watch Time': treatment_watch_time,\n",
    "})\n",
    "\n",
    "print(\"A/B TEST SIMULATION: Video Recommendation\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nControl (Old Model):\")\n",
    "print(f\"  Average CTR: {control_ctr.mean()*100:.2f}%\")\n",
    "print(f\"  Average Watch Time: {control_watch_time.mean():.1f} sec\")\n",
    "\n",
    "print(f\"\\nTreatment (New Model):\")\n",
    "print(f\"  Average CTR: {treatment_ctr.mean()*100:.2f}%\")\n",
    "print(f\"  Average Watch Time: {treatment_watch_time.mean():.1f} sec\")\n",
    "\n",
    "print(f\"\\nRelative Lift:\")\n",
    "print(f\"  CTR: +{(treatment_ctr.mean()/control_ctr.mean()-1)*100:.1f}%\")\n",
    "print(f\"  Watch Time: +{(treatment_watch_time.mean()/control_watch_time.mean()-1)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize A/B test results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# CTR over time\n",
    "axes[0].plot(ab_data['Day'], ab_data['Control CTR']*100, 'b-o', label='Control', markersize=6)\n",
    "axes[0].plot(ab_data['Day'], ab_data['Treatment CTR']*100, 'g-o', label='Treatment', markersize=6)\n",
    "axes[0].axhline(y=control_ctr.mean()*100, color='blue', linestyle='--', alpha=0.5)\n",
    "axes[0].axhline(y=treatment_ctr.mean()*100, color='green', linestyle='--', alpha=0.5)\n",
    "axes[0].set_xlabel('Day')\n",
    "axes[0].set_ylabel('CTR (%)')\n",
    "axes[0].set_title('CTR Over A/B Test Period')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Watch Time over time\n",
    "axes[1].plot(ab_data['Day'], ab_data['Control Watch Time'], 'b-o', label='Control', markersize=6)\n",
    "axes[1].plot(ab_data['Day'], ab_data['Treatment Watch Time'], 'g-o', label='Treatment', markersize=6)\n",
    "axes[1].axhline(y=control_watch_time.mean(), color='blue', linestyle='--', alpha=0.5)\n",
    "axes[1].axhline(y=treatment_watch_time.mean(), color='green', linestyle='--', alpha=0.5)\n",
    "axes[1].set_xlabel('Day')\n",
    "axes[1].set_ylabel('Watch Time (sec)')\n",
    "axes[1].set_title('Watch Time Over A/B Test Period')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical significance check\n",
    "from scipy import stats\n",
    "\n",
    "print(\"\\nSTATISTICAL SIGNIFICANCE (t-test)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# CTR significance\n",
    "t_stat_ctr, p_val_ctr = stats.ttest_ind(treatment_ctr, control_ctr)\n",
    "print(f\"\\nCTR:\")\n",
    "print(f\"  t-statistic: {t_stat_ctr:.3f}\")\n",
    "print(f\"  p-value: {p_val_ctr:.4f}\")\n",
    "print(f\"  Significant at α=0.05: {p_val_ctr < 0.05}\")\n",
    "\n",
    "# Watch Time significance\n",
    "t_stat_wt, p_val_wt = stats.ttest_ind(treatment_watch_time, control_watch_time)\n",
    "print(f\"\\nWatch Time:\")\n",
    "print(f\"  t-statistic: {t_stat_wt:.3f}\")\n",
    "print(f\"  p-value: {p_val_wt:.4f}\")\n",
    "print(f\"  Significant at α=0.05: {p_val_wt < 0.05}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Fairness and Bias (Chapter Mention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"FAIRNESS AND BIAS EVALUATION (Chapter Mention)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "The chapter emphasizes including fairness/bias questions:\n",
    "\n",
    "1. DEMOGRAPHIC PARITY:\n",
    "   - Does the model have similar positive rates across groups?\n",
    "   - Example: Are job ads shown equally to all genders?\n",
    "\n",
    "2. EQUALIZED ODDS:\n",
    "   - Same true positive rate and false positive rate across groups?\n",
    "   - Example: Harmful content detection equally accurate for all communities?\n",
    "\n",
    "3. CALIBRATION:\n",
    "   - When model says 70% probability, is it actually 70%?\n",
    "   - Important for decision-making based on scores\n",
    "\n",
    "4. REPRESENTATION:\n",
    "   - Are all groups adequately represented in training data?\n",
    "   - Underrepresented groups may have worse performance\n",
    "\n",
    "INTERVIEW SIGNAL:\n",
    "- E5: Knows fairness metrics exist, can name a few\n",
    "- E6: Proposes specific fairness constraints for the use case,\n",
    "      discusses tradeoffs (accuracy vs fairness)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple fairness metric example\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate model predictions for two groups\n",
    "n_each = 500\n",
    "\n",
    "# Group A\n",
    "group_a_true = np.random.choice([0, 1], n_each, p=[0.85, 0.15])\n",
    "group_a_pred = np.where(\n",
    "    group_a_true == 1,\n",
    "    np.random.choice([0, 1], n_each, p=[0.2, 0.8]),  # 80% recall\n",
    "    np.random.choice([0, 1], n_each, p=[0.95, 0.05])  # 5% FPR\n",
    ")\n",
    "\n",
    "# Group B (model is less accurate)\n",
    "group_b_true = np.random.choice([0, 1], n_each, p=[0.85, 0.15])\n",
    "group_b_pred = np.where(\n",
    "    group_b_true == 1,\n",
    "    np.random.choice([0, 1], n_each, p=[0.4, 0.6]),  # 60% recall (worse!)\n",
    "    np.random.choice([0, 1], n_each, p=[0.90, 0.10])  # 10% FPR (worse!)\n",
    ")\n",
    "\n",
    "print(\"FAIRNESS ANALYSIS: Model Performance Across Groups\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nGroup A:\")\n",
    "print(f\"  Precision: {precision_score(group_a_true, group_a_pred):.3f}\")\n",
    "print(f\"  Recall (TPR): {recall_score(group_a_true, group_a_pred):.3f}\")\n",
    "print(f\"  FPR: {(group_a_pred[group_a_true == 0] == 1).mean():.3f}\")\n",
    "\n",
    "print(f\"\\nGroup B:\")\n",
    "print(f\"  Precision: {precision_score(group_b_true, group_b_pred):.3f}\")\n",
    "print(f\"  Recall (TPR): {recall_score(group_b_true, group_b_pred):.3f}\")\n",
    "print(f\"  FPR: {(group_b_pred[group_b_true == 0] == 1).mean():.3f}\")\n",
    "\n",
    "print(f\"\\n[Fairness Issue]: Group B has lower recall - model underperforms for this group\")\n",
    "print(f\"[Action]: Investigate data representation, consider group-specific tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Business Sense Emphasis (Chapter Mention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"BUSINESS SENSE IN METRIC SELECTION (Chapter Emphasis)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "business_sense = pd.DataFrame({\n",
    "    'System': ['Harmful Content', 'Ad CTR', 'Friend Recs', 'Video Recs'],\n",
    "    'Wrong Metric Choice': [\n",
    "        'Accuracy (ignores class imbalance)',\n",
    "        'Clicks only (ignores conversions)',\n",
    "        'Requests sent (ignores quality)',\n",
    "        'CTR only (clickbait problem)'\n",
    "    ],\n",
    "    'Better Metric': [\n",
    "        'Precision + Recall (balance FP/FN cost)',\n",
    "        'Revenue lift + CTR (business outcome)',\n",
    "        'Accepted requests (meaningful connections)',\n",
    "        'Watch time + completion (genuine engagement)'\n",
    "    ],\n",
    "    'Business Rationale': [\n",
    "        'FP = censorship complaints, FN = safety risk',\n",
    "        'Ad that gets clicks but no purchase = bad',\n",
    "        'Spam friend requests = bad experience',\n",
    "        'User who clicks but leaves = not engaged'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(business_sense.to_string(index=False))\n",
    "\n",
    "print(\"\\n[Interview Signal]: Always tie metrics back to business impact!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Tradeoffs (Chapter-Aligned)\n",
    "\n",
    "| Tradeoff | Discussion | Interview Signal |\n",
    "|----------|------------|------------------|\n",
    "| **Precision vs Recall** | FP cost vs FN cost | E5: Knows formulas. E6: Maps to business impact |\n",
    "| **Offline vs Online** | Test set vs real users | E5: Knows both. E6: Discusses offline-online correlation |\n",
    "| **Short-term vs Long-term** | CTR vs retention | E5: Aware of difference. E6: Proposes guardrail metrics |\n",
    "| **Accuracy vs Fairness** | Overall performance vs group equity | E5: Knows fairness metrics. E6: Proposes constraints |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Meta Interview Signal (Detailed)\n",
    "\n",
    "### E5 Answer Expectations\n",
    "\n",
    "- Knows the metrics table for each task type (classification, regression, ranking)\n",
    "- Can compute metrics given predictions\n",
    "- Understands precision/recall tradeoff\n",
    "- Can name relevant online metrics for a system\n",
    "\n",
    "### E6 Additions\n",
    "\n",
    "- **Business impact**: \"Increasing recall from 0.7 to 0.8 catches 10% more harmful content = X fewer bad experiences\"\n",
    "- **Metric tradeoffs**: \"Higher CTR might hurt watch time if we optimize for clickbait - need both as guardrails\"\n",
    "- **Fairness**: \"We should slice metrics by demographic to ensure model works equally well for all users\"\n",
    "- **Offline-online gap**: \"Offline AUC improved 2% but online CTR was flat - investigating feature freshness\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Interview Drills\n",
    "\n",
    "### Drill 1: Metrics Table Recall\n",
    "From memory, list:\n",
    "- 5 classification metrics\n",
    "- 3 regression metrics\n",
    "- 4 ranking metrics\n",
    "\n",
    "### Drill 2: Metric Selection\n",
    "For each scenario, choose the best offline metric and justify:\n",
    "- Spam email detection (most important: not miss spam? or not flag good email?)\n",
    "- Search ranking (top 10 results matter most)\n",
    "- Watch time prediction\n",
    "\n",
    "### Drill 3: Online Metrics\n",
    "For a news feed ranking system, propose:\n",
    "- 2 primary online metrics\n",
    "- 2 guardrail metrics\n",
    "\n",
    "### Drill 4: Precision-Recall Tradeoff\n",
    "For harmful content detection:\n",
    "- What's the cost of high precision, low recall?\n",
    "- What's the cost of high recall, low precision?\n",
    "- How would you balance them?\n",
    "\n",
    "### Drill 5: Fairness Analysis\n",
    "For a job recommendation system:\n",
    "- What demographic slices would you analyze?\n",
    "- What fairness metric would you use?\n",
    "- How would you address disparities?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

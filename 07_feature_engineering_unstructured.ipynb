{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 07: Feature Engineering for Unstructured Data\n",
    "\n",
    "## Module 3: Data Preparation\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "\n",
    "1. **Extract features from text data** (tokenization, TF-IDF, word embeddings)\n",
    "2. **Process image data for ML models** (preprocessing, feature extraction)\n",
    "3. **Apply transfer learning** for feature extraction\n",
    "4. **Handle multimodal data** (combining text and image features)\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction to Unstructured Data](#1-introduction)\n",
    "2. [Text Feature Engineering](#2-text-features)\n",
    "3. [Image Feature Engineering](#3-image-features)\n",
    "4. [Transfer Learning for Features](#4-transfer-learning)\n",
    "5. [Multimodal Feature Fusion](#5-multimodal)\n",
    "6. [Hands-on Exercise](#6-exercise)\n",
    "7. [Summary and Key Takeaways](#7-summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Introduction to Unstructured Data <a id='1-introduction'></a>\n",
    "\n",
    "Unstructured data represents 80-90% of enterprise data but requires special processing.\n",
    "\n",
    "### Types of Unstructured Data\n",
    "\n",
    "| Type | Examples | ML Applications |\n",
    "|------|----------|----------------|\n",
    "| **Text** | Reviews, documents, emails | Sentiment, classification, NER |\n",
    "| **Images** | Photos, medical scans | Object detection, classification |\n",
    "| **Audio** | Speech, music | Recognition, transcription |\n",
    "| **Video** | Streams, recordings | Action recognition, tracking |\n",
    "\n",
    "### Key Challenge\n",
    "\n",
    "Converting variable-length, high-dimensional data into fixed-length numerical vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "import re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "\n",
    "# Text processing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Text Feature Engineering <a id='2-text-features'></a>\n",
    "\n",
    "Text data requires conversion to numerical representations.\n",
    "\n",
    "### 2.1 Text Preprocessing Pipeline\n",
    "\n",
    "1. **Lowercasing** - Normalize case\n",
    "2. **Tokenization** - Split into words/tokens\n",
    "3. **Stopword removal** - Remove common words\n",
    "4. **Stemming/Lemmatization** - Reduce to root form\n",
    "5. **Vectorization** - Convert to numbers\n",
    "\n",
    "### 2.2 Feature Extraction Methods\n",
    "\n",
    "| Method | Description | Captures |\n",
    "|--------|-------------|----------|\n",
    "| **Bag of Words** | Word frequency | Word presence |\n",
    "| **TF-IDF** | Term importance | Word importance |\n",
    "| **Word Embeddings** | Dense vectors | Semantic meaning |\n",
    "| **Sentence Embeddings** | Full sentence | Context |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample text dataset\n",
    "def create_text_dataset() -> pd.DataFrame:\n",
    "    \"\"\"Create sample product review dataset.\"\"\"\n",
    "    reviews = [\n",
    "        # Positive reviews\n",
    "        (\"Absolutely love this product! Great quality and fast shipping.\", 1),\n",
    "        (\"Best purchase I've made. Highly recommend to everyone.\", 1),\n",
    "        (\"Exceeded my expectations. Will definitely buy again.\", 1),\n",
    "        (\"Amazing value for money. Works perfectly.\", 1),\n",
    "        (\"Five stars! Perfect fit and excellent material.\", 1),\n",
    "        (\"Great product, exactly as described. Very happy!\", 1),\n",
    "        (\"Wonderful experience. Customer service was fantastic.\", 1),\n",
    "        (\"Super fast delivery and great packaging.\", 1),\n",
    "        (\"Love it! Already ordered two more for friends.\", 1),\n",
    "        (\"Perfect! Exactly what I was looking for.\", 1),\n",
    "        (\"Outstanding quality. Worth every penny.\", 1),\n",
    "        (\"Impressed with the build quality. Excellent!\", 1),\n",
    "        \n",
    "        # Negative reviews\n",
    "        (\"Terrible quality. Broke after one day.\", 0),\n",
    "        (\"Waste of money. Do not buy this product.\", 0),\n",
    "        (\"Very disappointed. Nothing like the pictures.\", 0),\n",
    "        (\"Poor quality and took forever to arrive.\", 0),\n",
    "        (\"Worst purchase ever. Complete garbage.\", 0),\n",
    "        (\"Does not work at all. Want my money back.\", 0),\n",
    "        (\"Cheap and flimsy. Falls apart easily.\", 0),\n",
    "        (\"Horrible customer service. Never again.\", 0),\n",
    "        (\"Damaged on arrival. Very frustrating.\", 0),\n",
    "        (\"Not worth the price. Very low quality.\", 0),\n",
    "        (\"Doesn't match description. Very misleading.\", 0),\n",
    "        (\"Terrible experience. Would not recommend.\", 0),\n",
    "    ]\n",
    "    \n",
    "    # Duplicate and add variations\n",
    "    expanded = []\n",
    "    for text, label in reviews:\n",
    "        expanded.append((text, label))\n",
    "        # Add slight variations\n",
    "        expanded.append((text.lower(), label))\n",
    "        expanded.append((text + \" \" + (\"Great!\" if label == 1 else \"Terrible!\"), label))\n",
    "    \n",
    "    df = pd.DataFrame(expanded, columns=['text', 'sentiment'])\n",
    "    return df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "text_df = create_text_dataset()\n",
    "print(\"Text Dataset:\")\n",
    "print(text_df.head(10))\n",
    "print(f\"\\nShape: {text_df.shape}\")\n",
    "print(f\"Sentiment distribution:\\n{text_df['sentiment'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing Pipeline\n",
    "class TextPreprocessor:\n",
    "    \"\"\"Text preprocessing pipeline.\"\"\"\n",
    "    \n",
    "    # Common English stopwords\n",
    "    STOPWORDS = set(['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \n",
    "                     'your', 'yours', 'yourself', 'he', 'him', 'his', 'himself', 'she',\n",
    "                     'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them',\n",
    "                     'their', 'theirs', 'what', 'which', 'who', 'whom', 'this', 'that',\n",
    "                     'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been',\n",
    "                     'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did',\n",
    "                     'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because',\n",
    "                     'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about',\n",
    "                     'against', 'between', 'into', 'through', 'during', 'before',\n",
    "                     'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out',\n",
    "                     'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once'])\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.word_counts = Counter()\n",
    "    \n",
    "    def lowercase(self, text: str) -> str:\n",
    "        return text.lower()\n",
    "    \n",
    "    def remove_punctuation(self, text: str) -> str:\n",
    "        return re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    def remove_numbers(self, text: str) -> str:\n",
    "        return re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    def remove_extra_whitespace(self, text: str) -> str:\n",
    "        return ' '.join(text.split())\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        return text.split()\n",
    "    \n",
    "    def remove_stopwords(self, tokens: List[str]) -> List[str]:\n",
    "        return [t for t in tokens if t not in self.STOPWORDS]\n",
    "    \n",
    "    def simple_stem(self, word: str) -> str:\n",
    "        \"\"\"Simple rule-based stemming.\"\"\"\n",
    "        suffixes = ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']\n",
    "        for suffix in suffixes:\n",
    "            if word.endswith(suffix) and len(word) > len(suffix) + 2:\n",
    "                return word[:-len(suffix)]\n",
    "        return word\n",
    "    \n",
    "    def preprocess(self, text: str, stem: bool = False) -> str:\n",
    "        \"\"\"Full preprocessing pipeline.\"\"\"\n",
    "        text = self.lowercase(text)\n",
    "        text = self.remove_punctuation(text)\n",
    "        text = self.remove_numbers(text)\n",
    "        text = self.remove_extra_whitespace(text)\n",
    "        \n",
    "        tokens = self.tokenize(text)\n",
    "        tokens = self.remove_stopwords(tokens)\n",
    "        \n",
    "        if stem:\n",
    "            tokens = [self.simple_stem(t) for t in tokens]\n",
    "        \n",
    "        self.word_counts.update(tokens)\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def preprocess_batch(self, texts: List[str], stem: bool = False) -> List[str]:\n",
    "        return [self.preprocess(t, stem) for t in texts]\n",
    "\n",
    "print(\"TextPreprocessor defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing demo\n",
    "print(\"=\" * 60)\n",
    "print(\"TEXT PREPROCESSING DEMO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "sample_text = \"This is an AMAZING product! I've never seen anything like it. 5 stars!!!\"\n",
    "print(f\"\\nOriginal: {sample_text}\")\n",
    "print(f\"Preprocessed: {preprocessor.preprocess(sample_text)}\")\n",
    "print(f\"With stemming: {preprocessor.preprocess(sample_text, stem=True)}\")\n",
    "\n",
    "# Preprocess entire dataset\n",
    "text_df['text_clean'] = preprocessor.preprocess_batch(text_df['text'].tolist())\n",
    "print(\"\\nPreprocessed Dataset:\")\n",
    "print(text_df[['text', 'text_clean']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words and TF-IDF\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BAG OF WORDS vs TF-IDF\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Bag of Words\n",
    "bow_vectorizer = CountVectorizer(max_features=100)\n",
    "X_bow = bow_vectorizer.fit_transform(text_df['text_clean'])\n",
    "\n",
    "print(f\"\\nBag of Words shape: {X_bow.shape}\")\n",
    "print(f\"Vocabulary size: {len(bow_vectorizer.vocabulary_)}\")\n",
    "print(f\"Top words: {list(bow_vectorizer.vocabulary_.keys())[:10]}\")\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(text_df['text_clean'])\n",
    "\n",
    "print(f\"\\nTF-IDF shape: {X_tfidf.shape}\")\n",
    "\n",
    "# Compare feature values\n",
    "sample_idx = 0\n",
    "print(f\"\\nSample text: '{text_df['text_clean'].iloc[sample_idx]}'\")\n",
    "print(f\"BoW vector (non-zero): {X_bow[sample_idx].toarray()[0][X_bow[sample_idx].toarray()[0] > 0][:5]}\")\n",
    "print(f\"TF-IDF vector (non-zero): {X_tfidf[sample_idx].toarray()[0][X_tfidf[sample_idx].toarray()[0] > 0][:5].round(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-grams\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"N-GRAMS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Unigrams\n",
    "unigram_vec = CountVectorizer(ngram_range=(1, 1), max_features=50)\n",
    "X_uni = unigram_vec.fit_transform(text_df['text_clean'])\n",
    "print(f\"\\nUnigrams: {X_uni.shape[1]} features\")\n",
    "print(f\"Examples: {list(unigram_vec.vocabulary_.keys())[:10]}\")\n",
    "\n",
    "# Bigrams\n",
    "bigram_vec = CountVectorizer(ngram_range=(2, 2), max_features=50)\n",
    "X_bi = bigram_vec.fit_transform(text_df['text_clean'])\n",
    "print(f\"\\nBigrams: {X_bi.shape[1]} features\")\n",
    "print(f\"Examples: {list(bigram_vec.vocabulary_.keys())[:10]}\")\n",
    "\n",
    "# Uni + Bigrams\n",
    "combined_vec = CountVectorizer(ngram_range=(1, 2), max_features=100)\n",
    "X_combined = combined_vec.fit_transform(text_df['text_clean'])\n",
    "print(f\"\\nUni+Bigrams: {X_combined.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Word Embeddings (simulation)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"WORD EMBEDDINGS (SIMULATED)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class SimpleWordEmbeddings:\n",
    "    \"\"\"Simulate word embeddings for demonstration.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim: int = 50, seed: int = 42):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.word_vectors = {}\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    def build_vocab(self, texts: List[str]) -> None:\n",
    "        \"\"\"Build vocabulary and assign random embeddings.\"\"\"\n",
    "        all_words = set()\n",
    "        for text in texts:\n",
    "            all_words.update(text.split())\n",
    "        \n",
    "        # Create embeddings with semantic clustering\n",
    "        positive_words = ['great', 'good', 'excellent', 'amazing', 'love', 'best', 'perfect', 'happy', 'recommend']\n",
    "        negative_words = ['terrible', 'bad', 'worst', 'poor', 'disappointed', 'waste', 'garbage', 'horrible']\n",
    "        \n",
    "        for word in all_words:\n",
    "            if word in positive_words:\n",
    "                # Positive cluster centered around [1, 1, ...]\n",
    "                self.word_vectors[word] = np.random.randn(self.embedding_dim) * 0.3 + 0.5\n",
    "            elif word in negative_words:\n",
    "                # Negative cluster centered around [-1, -1, ...]\n",
    "                self.word_vectors[word] = np.random.randn(self.embedding_dim) * 0.3 - 0.5\n",
    "            else:\n",
    "                # Random\n",
    "                self.word_vectors[word] = np.random.randn(self.embedding_dim) * 0.5\n",
    "    \n",
    "    def get_word_vector(self, word: str) -> np.ndarray:\n",
    "        return self.word_vectors.get(word, np.zeros(self.embedding_dim))\n",
    "    \n",
    "    def get_sentence_vector(self, text: str, method: str = 'mean') -> np.ndarray:\n",
    "        \"\"\"Get sentence vector by averaging word vectors.\"\"\"\n",
    "        words = text.split()\n",
    "        vectors = [self.get_word_vector(w) for w in words if w in self.word_vectors]\n",
    "        \n",
    "        if not vectors:\n",
    "            return np.zeros(self.embedding_dim)\n",
    "        \n",
    "        if method == 'mean':\n",
    "            return np.mean(vectors, axis=0)\n",
    "        elif method == 'sum':\n",
    "            return np.sum(vectors, axis=0)\n",
    "        elif method == 'max':\n",
    "            return np.max(vectors, axis=0)\n",
    "        return np.mean(vectors, axis=0)\n",
    "\n",
    "# Create embeddings\n",
    "embeddings = SimpleWordEmbeddings(embedding_dim=50)\n",
    "embeddings.build_vocab(text_df['text_clean'].tolist())\n",
    "\n",
    "print(f\"Vocabulary size: {len(embeddings.word_vectors)}\")\n",
    "print(f\"Embedding dimension: {embeddings.embedding_dim}\")\n",
    "\n",
    "# Get sentence embeddings\n",
    "X_embed = np.array([embeddings.get_sentence_vector(t) for t in text_df['text_clean']])\n",
    "print(f\"\\nSentence embeddings shape: {X_embed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare text feature methods\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TEXT FEATURE COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "y = text_df['sentiment']\n",
    "\n",
    "results = []\n",
    "\n",
    "# Bag of Words\n",
    "scores = cross_val_score(LogisticRegression(max_iter=1000), X_bow.toarray(), y, cv=5)\n",
    "results.append({'Method': 'Bag of Words', 'Mean Accuracy': f\"{scores.mean():.4f}\", 'Std': f\"{scores.std():.4f}\"})\n",
    "\n",
    "# TF-IDF\n",
    "scores = cross_val_score(LogisticRegression(max_iter=1000), X_tfidf.toarray(), y, cv=5)\n",
    "results.append({'Method': 'TF-IDF', 'Mean Accuracy': f\"{scores.mean():.4f}\", 'Std': f\"{scores.std():.4f}\"})\n",
    "\n",
    "# N-grams\n",
    "scores = cross_val_score(LogisticRegression(max_iter=1000), X_combined.toarray(), y, cv=5)\n",
    "results.append({'Method': 'Uni+Bigrams', 'Mean Accuracy': f\"{scores.mean():.4f}\", 'Std': f\"{scores.std():.4f}\"})\n",
    "\n",
    "# Word Embeddings\n",
    "scores = cross_val_score(LogisticRegression(max_iter=1000), X_embed, y, cv=5)\n",
    "results.append({'Method': 'Word Embeddings', 'Mean Accuracy': f\"{scores.mean():.4f}\", 'Std': f\"{scores.std():.4f}\"})\n",
    "\n",
    "print(pd.DataFrame(results).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize text embeddings\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Use TF-IDF with dimensionality reduction\n",
    "svd = TruncatedSVD(n_components=2, random_state=42)\n",
    "X_svd = svd.fit_transform(X_tfidf)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# TF-IDF with SVD\n",
    "colors = ['red' if s == 0 else 'green' for s in y]\n",
    "axes[0].scatter(X_svd[:, 0], X_svd[:, 1], c=colors, alpha=0.6)\n",
    "axes[0].set_title('TF-IDF + SVD Visualization')\n",
    "axes[0].set_xlabel('Component 1')\n",
    "axes[0].set_ylabel('Component 2')\n",
    "\n",
    "# Word embeddings with PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_embed)\n",
    "axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=colors, alpha=0.6)\n",
    "axes[1].set_title('Word Embeddings + PCA Visualization')\n",
    "axes[1].set_xlabel('PC 1')\n",
    "axes[1].set_ylabel('PC 2')\n",
    "\n",
    "# Add legend\n",
    "for ax in axes:\n",
    "    ax.scatter([], [], c='green', label='Positive')\n",
    "    ax.scatter([], [], c='red', label='Negative')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Image Feature Engineering <a id='3-image-features'></a>\n",
    "\n",
    "### 3.1 Image Preprocessing\n",
    "\n",
    "| Operation | Description | Purpose |\n",
    "|-----------|-------------|----------|\n",
    "| **Resizing** | Uniform dimensions | Model input |\n",
    "| **Normalization** | Scale pixels [0,1] | Training stability |\n",
    "| **Augmentation** | Random transforms | Increase diversity |\n",
    "\n",
    "### 3.2 Feature Extraction\n",
    "\n",
    "| Method | Description | Complexity |\n",
    "|--------|-------------|------------|\n",
    "| **Histograms** | Color/intensity distribution | Low |\n",
    "| **HOG** | Edge orientations | Medium |\n",
    "| **CNN Features** | Deep learning | High |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Processing (simulated without actual images)\n",
    "class ImageFeatureExtractor:\n",
    "    \"\"\"Simulated image feature extraction.\"\"\"\n",
    "    \n",
    "    def __init__(self, image_size: Tuple[int, int] = (224, 224)):\n",
    "        self.image_size = image_size\n",
    "    \n",
    "    def generate_synthetic_image(self, image_class: str) -> np.ndarray:\n",
    "        \"\"\"Generate synthetic image array for demonstration.\"\"\"\n",
    "        np.random.seed(hash(image_class) % 2**31)\n",
    "        \n",
    "        # Create base pattern based on class\n",
    "        if image_class == 'cat':\n",
    "            base = np.random.normal(0.4, 0.1, (32, 32, 3))\n",
    "        elif image_class == 'dog':\n",
    "            base = np.random.normal(0.6, 0.1, (32, 32, 3))\n",
    "        elif image_class == 'car':\n",
    "            base = np.random.normal(0.3, 0.15, (32, 32, 3))\n",
    "        else:\n",
    "            base = np.random.random((32, 32, 3))\n",
    "        \n",
    "        return np.clip(base, 0, 1)\n",
    "    \n",
    "    def extract_color_histogram(self, image: np.ndarray, bins: int = 16) -> np.ndarray:\n",
    "        \"\"\"Extract color histogram features.\"\"\"\n",
    "        features = []\n",
    "        for channel in range(3):\n",
    "            hist, _ = np.histogram(image[:, :, channel], bins=bins, range=(0, 1))\n",
    "            features.extend(hist / hist.sum())  # Normalize\n",
    "        return np.array(features)\n",
    "    \n",
    "    def extract_statistics(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Extract statistical features.\"\"\"\n",
    "        features = []\n",
    "        for channel in range(3):\n",
    "            features.extend([\n",
    "                np.mean(image[:, :, channel]),\n",
    "                np.std(image[:, :, channel]),\n",
    "                np.min(image[:, :, channel]),\n",
    "                np.max(image[:, :, channel]),\n",
    "                np.median(image[:, :, channel])\n",
    "            ])\n",
    "        return np.array(features)\n",
    "    \n",
    "    def extract_spatial_features(self, image: np.ndarray, grid_size: int = 4) -> np.ndarray:\n",
    "        \"\"\"Extract spatial grid features.\"\"\"\n",
    "        h, w = image.shape[:2]\n",
    "        cell_h, cell_w = h // grid_size, w // grid_size\n",
    "        \n",
    "        features = []\n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                cell = image[i*cell_h:(i+1)*cell_h, j*cell_w:(j+1)*cell_w]\n",
    "                features.append(np.mean(cell))\n",
    "        return np.array(features)\n",
    "    \n",
    "    def extract_all_features(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Combine all feature extraction methods.\"\"\"\n",
    "        hist = self.extract_color_histogram(image)\n",
    "        stats = self.extract_statistics(image)\n",
    "        spatial = self.extract_spatial_features(image)\n",
    "        return np.concatenate([hist, stats, spatial])\n",
    "\n",
    "print(\"ImageFeatureExtractor defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo image feature extraction\n",
    "print(\"=\" * 60)\n",
    "print(\"IMAGE FEATURE EXTRACTION DEMO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "extractor = ImageFeatureExtractor()\n",
    "\n",
    "# Create synthetic image dataset\n",
    "classes = ['cat', 'dog', 'car']\n",
    "n_per_class = 50\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for cls in classes:\n",
    "    for i in range(n_per_class):\n",
    "        img = extractor.generate_synthetic_image(cls)\n",
    "        # Add noise for variation\n",
    "        img += np.random.randn(*img.shape) * 0.05\n",
    "        img = np.clip(img, 0, 1)\n",
    "        images.append(img)\n",
    "        labels.append(cls)\n",
    "\n",
    "print(f\"Generated {len(images)} synthetic images\")\n",
    "print(f\"Image shape: {images[0].shape}\")\n",
    "\n",
    "# Extract features\n",
    "X_img = np.array([extractor.extract_all_features(img) for img in images])\n",
    "print(f\"Feature matrix shape: {X_img.shape}\")\n",
    "\n",
    "# Feature breakdown\n",
    "sample_img = images[0]\n",
    "print(f\"\\nFeature breakdown:\")\n",
    "print(f\"  Color histogram: {len(extractor.extract_color_histogram(sample_img))} features\")\n",
    "print(f\"  Statistics: {len(extractor.extract_statistics(sample_img))} features\")\n",
    "print(f\"  Spatial: {len(extractor.extract_spatial_features(sample_img))} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize synthetic images\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "\n",
    "for idx, cls in enumerate(classes):\n",
    "    # Show sample image\n",
    "    img = images[idx * n_per_class]\n",
    "    axes[0, idx].imshow(img)\n",
    "    axes[0, idx].set_title(f'Sample: {cls}')\n",
    "    axes[0, idx].axis('off')\n",
    "    \n",
    "    # Show histogram\n",
    "    axes[1, idx].hist(img[:, :, 0].flatten(), bins=20, alpha=0.5, color='red', label='R')\n",
    "    axes[1, idx].hist(img[:, :, 1].flatten(), bins=20, alpha=0.5, color='green', label='G')\n",
    "    axes[1, idx].hist(img[:, :, 2].flatten(), bins=20, alpha=0.5, color='blue', label='B')\n",
    "    axes[1, idx].set_title(f'Color Histogram: {cls}')\n",
    "    axes[1, idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image classification with extracted features\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"IMAGE CLASSIFICATION WITH FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_img = le.fit_transform(labels)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_img, y_img, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train classifier\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Transfer Learning for Features <a id='4-transfer-learning'></a>\n",
    "\n",
    "Pre-trained models provide powerful feature extractors.\n",
    "\n",
    "### Benefits\n",
    "\n",
    "- Leverage knowledge from large datasets\n",
    "- Better features with less data\n",
    "- Faster training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated Transfer Learning Features\n",
    "class TransferLearningSimulator:\n",
    "    \"\"\"Simulates transfer learning feature extraction.\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim: int = 512):\n",
    "        self.feature_dim = feature_dim\n",
    "        # Simulated pre-trained weights\n",
    "        np.random.seed(42)\n",
    "        self.projection = np.random.randn(79, feature_dim)  # 79 = handcrafted features\n",
    "    \n",
    "    def extract_features(self, handcrafted_features: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Simulate CNN feature extraction.\"\"\"\n",
    "        # Apply non-linear transformation\n",
    "        projected = np.tanh(handcrafted_features @ self.projection)\n",
    "        return projected\n",
    "\n",
    "# Demo\n",
    "print(\"=\" * 60)\n",
    "print(\"TRANSFER LEARNING FEATURES (SIMULATED)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "transfer_model = TransferLearningSimulator(feature_dim=128)\n",
    "X_transfer = transfer_model.extract_features(X_img)\n",
    "\n",
    "print(f\"Original features: {X_img.shape}\")\n",
    "print(f\"Transfer features: {X_transfer.shape}\")\n",
    "\n",
    "# Compare performance\n",
    "X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(X_transfer, y_img, test_size=0.2, random_state=42)\n",
    "\n",
    "clf_transfer = LogisticRegression(max_iter=1000)\n",
    "clf_transfer.fit(X_train_t, y_train_t)\n",
    "\n",
    "print(f\"\\nAccuracy with handcrafted features: {accuracy_score(y_test, clf.predict(X_test_scaled)):.4f}\")\n",
    "print(f\"Accuracy with transfer features: {accuracy_score(y_test_t, clf_transfer.predict(X_test_t)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize transfer learning features\n",
    "pca = PCA(n_components=2)\n",
    "X_transfer_pca = pca.fit_transform(X_transfer)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Handcrafted features\n",
    "X_img_pca = PCA(n_components=2).fit_transform(X_img)\n",
    "colors = [['red', 'green', 'blue'][c] for c in y_img]\n",
    "axes[0].scatter(X_img_pca[:, 0], X_img_pca[:, 1], c=colors, alpha=0.6)\n",
    "axes[0].set_title('Handcrafted Features (PCA)')\n",
    "\n",
    "# Transfer features\n",
    "axes[1].scatter(X_transfer_pca[:, 0], X_transfer_pca[:, 1], c=colors, alpha=0.6)\n",
    "axes[1].set_title('Transfer Learning Features (PCA)')\n",
    "\n",
    "for ax in axes:\n",
    "    for i, cls in enumerate(classes):\n",
    "        ax.scatter([], [], c=['red', 'green', 'blue'][i], label=cls)\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Multimodal Feature Fusion <a id='5-multimodal'></a>\n",
    "\n",
    "Combining features from different modalities.\n",
    "\n",
    "### Fusion Strategies\n",
    "\n",
    "| Strategy | Description | When to Use |\n",
    "|----------|-------------|-------------|\n",
    "| **Early Fusion** | Concatenate features | Similar modality scales |\n",
    "| **Late Fusion** | Combine predictions | Independent modalities |\n",
    "| **Attention** | Weighted combination | Complex interactions |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multimodal Dataset\n",
    "print(\"=\" * 60)\n",
    "print(\"MULTIMODAL FEATURE FUSION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create aligned text-image dataset\n",
    "n_samples = 100\n",
    "\n",
    "# Text features (from reviews)\n",
    "positive_texts = [\n",
    "    \"great quality amazing product highly recommend\",\n",
    "    \"love perfect excellent five stars\",\n",
    "    \"best purchase fantastic wonderful\"\n",
    "]\n",
    "negative_texts = [\n",
    "    \"terrible quality waste money disappointed\",\n",
    "    \"worst purchase horrible poor bad\",\n",
    "    \"broken damaged useless garbage\"\n",
    "]\n",
    "\n",
    "texts = []\n",
    "labels_mm = []\n",
    "for i in range(n_samples // 2):\n",
    "    texts.append(np.random.choice(positive_texts) + \" \" + str(np.random.randint(100)))\n",
    "    labels_mm.append(1)\n",
    "for i in range(n_samples // 2):\n",
    "    texts.append(np.random.choice(negative_texts) + \" \" + str(np.random.randint(100)))\n",
    "    labels_mm.append(0)\n",
    "\n",
    "# Text features\n",
    "tfidf = TfidfVectorizer(max_features=50)\n",
    "X_text_mm = tfidf.fit_transform(texts).toarray()\n",
    "\n",
    "# Image features (simulated product images)\n",
    "X_image_mm = np.zeros((n_samples, 20))\n",
    "for i in range(n_samples):\n",
    "    if labels_mm[i] == 1:  # Positive - bright images\n",
    "        X_image_mm[i] = np.random.normal(0.7, 0.1, 20)\n",
    "    else:  # Negative - dark images\n",
    "        X_image_mm[i] = np.random.normal(0.3, 0.1, 20)\n",
    "\n",
    "y_mm = np.array(labels_mm)\n",
    "\n",
    "print(f\"Text features: {X_text_mm.shape}\")\n",
    "print(f\"Image features: {X_image_mm.shape}\")\n",
    "print(f\"Labels: {y_mm.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Fusion: Concatenate features\n",
    "print(\"\\n--- EARLY FUSION ---\")\n",
    "\n",
    "# Normalize each modality\n",
    "text_scaled = StandardScaler().fit_transform(X_text_mm)\n",
    "image_scaled = StandardScaler().fit_transform(X_image_mm)\n",
    "\n",
    "# Concatenate\n",
    "X_early = np.concatenate([text_scaled, image_scaled], axis=1)\n",
    "print(f\"Fused features: {X_early.shape}\")\n",
    "\n",
    "# Evaluate\n",
    "scores = cross_val_score(LogisticRegression(max_iter=1000), X_early, y_mm, cv=5)\n",
    "print(f\"Early Fusion Accuracy: {scores.mean():.4f} (+/- {scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Late Fusion: Combine predictions\n",
    "print(\"\\n--- LATE FUSION ---\")\n",
    "\n",
    "# Train separate models\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "late_scores = []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X_text_mm):\n",
    "    # Text model\n",
    "    clf_text = LogisticRegression(max_iter=1000)\n",
    "    clf_text.fit(text_scaled[train_idx], y_mm[train_idx])\n",
    "    prob_text = clf_text.predict_proba(text_scaled[test_idx])[:, 1]\n",
    "    \n",
    "    # Image model\n",
    "    clf_img = LogisticRegression(max_iter=1000)\n",
    "    clf_img.fit(image_scaled[train_idx], y_mm[train_idx])\n",
    "    prob_img = clf_img.predict_proba(image_scaled[test_idx])[:, 1]\n",
    "    \n",
    "    # Average predictions\n",
    "    prob_fused = (prob_text + prob_img) / 2\n",
    "    pred_fused = (prob_fused > 0.5).astype(int)\n",
    "    \n",
    "    late_scores.append(accuracy_score(y_mm[test_idx], pred_fused))\n",
    "\n",
    "print(f\"Late Fusion Accuracy: {np.mean(late_scores):.4f} (+/- {np.std(late_scores):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all approaches\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MULTIMODAL COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = []\n",
    "\n",
    "# Text only\n",
    "scores = cross_val_score(LogisticRegression(max_iter=1000), text_scaled, y_mm, cv=5)\n",
    "results.append({'Method': 'Text Only', 'Accuracy': f\"{scores.mean():.4f}\"})\n",
    "\n",
    "# Image only\n",
    "scores = cross_val_score(LogisticRegression(max_iter=1000), image_scaled, y_mm, cv=5)\n",
    "results.append({'Method': 'Image Only', 'Accuracy': f\"{scores.mean():.4f}\"})\n",
    "\n",
    "# Early fusion\n",
    "scores = cross_val_score(LogisticRegression(max_iter=1000), X_early, y_mm, cv=5)\n",
    "results.append({'Method': 'Early Fusion', 'Accuracy': f\"{scores.mean():.4f}\"})\n",
    "\n",
    "# Late fusion\n",
    "results.append({'Method': 'Late Fusion', 'Accuracy': f\"{np.mean(late_scores):.4f}\"})\n",
    "\n",
    "print(pd.DataFrame(results).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Hands-on Exercise <a id='6-exercise'></a>\n",
    "\n",
    "Build a complete text classification pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Complete Text Classification Pipeline\n",
    "\n",
    "# News classification dataset\n",
    "news_data = [\n",
    "    (\"Stock market reaches all-time high as tech sector leads gains\", \"business\"),\n",
    "    (\"New smartphone features breakthrough battery technology\", \"tech\"),\n",
    "    (\"Local team wins championship in overtime thriller\", \"sports\"),\n",
    "    (\"Government announces new economic stimulus package\", \"business\"),\n",
    "    (\"Artificial intelligence transforms healthcare diagnostics\", \"tech\"),\n",
    "    (\"Olympic athlete breaks world record in swimming\", \"sports\"),\n",
    "    (\"Cryptocurrency values surge amid investor optimism\", \"business\"),\n",
    "    (\"Electric vehicles gain market share in automotive industry\", \"tech\"),\n",
    "    (\"Football season opens with exciting matchups\", \"sports\"),\n",
    "    (\"Bank merger creates largest financial institution\", \"business\"),\n",
    "    (\"Social media platform launches new privacy features\", \"tech\"),\n",
    "    (\"Tennis star announces retirement after legendary career\", \"sports\"),\n",
    "    (\"Oil prices impact global economic forecasts\", \"business\"),\n",
    "    (\"Quantum computing achieves major milestone\", \"tech\"),\n",
    "    (\"Basketball playoffs attract record viewership\", \"sports\"),\n",
    "]\n",
    "\n",
    "# Expand dataset\n",
    "expanded_news = []\n",
    "for text, label in news_data:\n",
    "    for _ in range(10):\n",
    "        noise = \" \" + \" \".join([str(np.random.randint(100)) for _ in range(3)])\n",
    "        expanded_news.append((text.lower() + noise, label))\n",
    "\n",
    "news_df = pd.DataFrame(expanded_news, columns=['text', 'category'])\n",
    "news_df = news_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"News Dataset:\")\n",
    "print(news_df.head())\n",
    "print(f\"\\nCategories: {news_df['category'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TASK: Build classification pipeline\n",
    "\n",
    "# 1. Preprocess text\n",
    "preprocessor = TextPreprocessor()\n",
    "news_df['text_clean'] = preprocessor.preprocess_batch(news_df['text'].tolist())\n",
    "\n",
    "# 2. Extract features\n",
    "tfidf = TfidfVectorizer(max_features=100, ngram_range=(1, 2))\n",
    "X = tfidf.fit_transform(news_df['text_clean'])\n",
    "\n",
    "# 3. Encode labels\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(news_df['category'])\n",
    "\n",
    "# 4. Train and evaluate\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"News Classification Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on new examples\n",
    "new_texts = [\n",
    "    \"Trading volumes surge as investors react to interest rate news\",\n",
    "    \"New app uses machine learning to recommend restaurants\",\n",
    "    \"Soccer team prepares for important league match\"\n",
    "]\n",
    "\n",
    "print(\"\\nPredictions on new texts:\")\n",
    "for text in new_texts:\n",
    "    clean = preprocessor.preprocess(text)\n",
    "    features = tfidf.transform([clean])\n",
    "    pred = clf.predict(features)\n",
    "    print(f\"  '{text[:50]}...' -> {le.inverse_transform(pred)[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Summary and Key Takeaways <a id='7-summary'></a>\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Text Features**\n",
    "   - Preprocessing: lowercase, tokenize, remove stopwords\n",
    "   - Vectorization: BoW, TF-IDF, word embeddings\n",
    "   - N-grams capture word combinations\n",
    "\n",
    "2. **Image Features**\n",
    "   - Histograms for color distribution\n",
    "   - Statistical and spatial features\n",
    "   - Transfer learning for rich representations\n",
    "\n",
    "3. **Multimodal Fusion**\n",
    "   - Early fusion: concatenate features\n",
    "   - Late fusion: combine predictions\n",
    "   - Often outperforms single modality\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- Clean and preprocess text consistently\n",
    "- Use TF-IDF for importance weighting\n",
    "- Leverage pre-trained models when possible\n",
    "- Normalize features before fusion\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Module 4: Model Development (Tutorial 08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TUTORIAL 07 COMPLETE: Feature Engineering for Unstructured Data\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nTopics covered:\")\n",
    "print(\"  1. Text Feature Engineering\")\n",
    "print(\"  2. Image Feature Engineering\")\n",
    "print(\"  3. Transfer Learning Features\")\n",
    "print(\"  4. Multimodal Feature Fusion\")\n",
    "print(\"\\nModule 3: Data Preparation - COMPLETE!\")\n",
    "print(\"\\nNext: Module 4 - Model Development (Tutorial 08)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
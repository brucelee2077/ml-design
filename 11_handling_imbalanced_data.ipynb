
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 11: Handling Imbalanced Data\n",
    "\n",
    "## Module 4: Model Development\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "\n",
    "1. **Identify and measure class imbalance** in datasets\n",
    "2. **Apply resampling techniques** including SMOTE and undersampling\n",
    "3. **Use class-weighted loss functions** to handle imbalance\n",
    "4. **Implement focal loss** for extreme imbalance\n",
    "5. **Choose appropriate metrics** for imbalanced datasets\n",
    "6. **Design evaluation strategies** that account for imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from typing import Tuple, List, Dict\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score,\n",
    "    precision_recall_curve, average_precision_score, roc_curve\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "np.random.seed(42)\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to import imbalanced-learn\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
    "    from imblearn.under_sampling import RandomUnderSampler, TomekLinks, NearMiss\n",
    "    from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "    from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "    IMBLEARN_AVAILABLE = True\n",
    "    print(\"imbalanced-learn available\")\n",
    "except ImportError:\n",
    "    IMBLEARN_AVAILABLE = False\n",
    "    print(\"imbalanced-learn not available - will use custom implementations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Class Imbalance\n",
    "\n",
    "Class imbalance occurs when one class significantly outnumbers the others.\n",
    "\n",
    "### Common Scenarios\n",
    "\n",
    "| Domain | Minority Class | Typical Ratio |\n",
    "|--------|---------------|---------------|\n",
    "| Fraud Detection | Fraud | 1:1000 |\n",
    "| Disease Diagnosis | Positive cases | 1:100 |\n",
    "| Spam Detection | Spam | 1:10 |\n",
    "| Churn Prediction | Churned | 1:20 |\n",
    "| Click-through Rate | Clicks | 1:100 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_imbalanced_dataset(n_samples=10000, imbalance_ratio=0.05, n_features=20, random_state=42):\n",
    "    \"\"\"\n",
    "    Create an imbalanced binary classification dataset.\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Total number of samples\n",
    "        imbalance_ratio: Fraction of minority class (e.g., 0.05 = 5%)\n",
    "        n_features: Number of features\n",
    "        random_state: Random seed\n",
    "    \"\"\"\n",
    "    X, y = make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=n_features,\n",
    "        n_informative=10,\n",
    "        n_redundant=5,\n",
    "        n_clusters_per_class=2,\n",
    "        weights=[1 - imbalance_ratio, imbalance_ratio],\n",
    "        flip_y=0.01,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    return X, y\n",
    "\n",
    "# Create datasets with different imbalance levels\n",
    "X_mild, y_mild = create_imbalanced_dataset(imbalance_ratio=0.3)     # Mild: 30%\n",
    "X_moderate, y_moderate = create_imbalanced_dataset(imbalance_ratio=0.1)  # Moderate: 10%\n",
    "X_severe, y_severe = create_imbalanced_dataset(imbalance_ratio=0.02)    # Severe: 2%\n",
    "\n",
    "print(\"Imbalanced Datasets Created:\")\n",
    "for name, y in [('Mild', y_mild), ('Moderate', y_moderate), ('Severe', y_severe)]:\n",
    "    counts = Counter(y)\n",
    "    ratio = counts[1] / counts[0]\n",
    "    print(f\"  {name}: {counts} (ratio 1:{1/ratio:.1f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, (name, y) in zip(axes, [('Mild (30%)', y_mild), ('Moderate (10%)', y_moderate), ('Severe (2%)', y_severe)]):\n",
    "    counts = Counter(y)\n",
    "    ax.bar(['Majority (0)', 'Minority (1)'], [counts[0], counts[1]], color=['steelblue', 'coral'])\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title(f'{name} Imbalance')\n",
    "    for i, (label, count) in enumerate([(0, counts[0]), (1, counts[1])]):\n",
    "        ax.text(i, count + 100, str(count), ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Problem with Standard Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate why accuracy is misleading\n",
    "class MajorityClassifier:\n",
    "    \"\"\"Always predicts the majority class.\"\"\"\n",
    "    def fit(self, X, y):\n",
    "        self.majority_class = Counter(y).most_common(1)[0][0]\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.full(len(X), self.majority_class)\n",
    "\n",
    "# Test on severe imbalance\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_severe, y_severe, test_size=0.2, random_state=42, stratify=y_severe\n",
    ")\n",
    "\n",
    "dummy = MajorityClassifier()\n",
    "dummy.fit(X_train, y_train)\n",
    "y_pred = dummy.predict(X_test)\n",
    "\n",
    "print(\"Majority Class Classifier on Severe Imbalance (2%):\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"  Precision: {precision_score(y_test, y_pred, zero_division=0):.4f}\")\n",
    "print(f\"  Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"  F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "print(\"\\n=> High accuracy but useless model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Better Metrics for Imbalanced Data\n",
    "\n",
    "| Metric | Description | When to Use |\n",
    "|--------|-------------|-------------|\n",
    "| **Precision** | TP / (TP + FP) | Cost of false positives is high |\n",
    "| **Recall** | TP / (TP + FN) | Cost of false negatives is high |\n",
    "| **F1 Score** | Harmonic mean | Balance precision and recall |\n",
    "| **AUC-ROC** | Area under ROC curve | Overall ranking ability |\n",
    "| **Average Precision** | Area under PR curve | Focus on minority class |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"Comprehensive evaluation for imbalanced data.\"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    results = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1': f1_score(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        results['AUC-ROC'] = roc_auc_score(y_test, y_proba)\n",
    "        results['Avg Precision'] = average_precision_score(y_test, y_proba)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Prepare data\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "# Evaluate standard Logistic Regression\n",
    "results = evaluate_model(\n",
    "    LogisticRegression(max_iter=1000, random_state=42),\n",
    "    X_train_s, X_test_s, y_train, y_test,\n",
    "    'Standard Logistic Regression'\n",
    ")\n",
    "\n",
    "print(\"Standard Logistic Regression on Imbalanced Data:\")\n",
    "for metric, value in results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Resampling Techniques\n",
    "\n",
    "### Overview\n",
    "\n",
    "| Technique | Description | Pros | Cons |\n",
    "|-----------|-------------|------|------|\n",
    "| **Random Oversampling** | Duplicate minority samples | Simple | Overfitting |\n",
    "| **SMOTE** | Synthetic minority samples | Better generalization | May create noise |\n",
    "| **Random Undersampling** | Remove majority samples | Reduces training time | Information loss |\n",
    "| **Tomek Links** | Remove borderline majority | Cleaner boundary | Limited reduction |\n",
    "| **Combination** | SMOTE + Undersampling | Balanced approach | More complex |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Random Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_oversample(X, y, target_ratio=1.0):\n",
    "    \"\"\"\n",
    "    Random oversampling of minority class.\n",
    "    \n",
    "    Args:\n",
    "        X: Features\n",
    "        y: Labels\n",
    "        target_ratio: Desired ratio of minority to majority (1.0 = balanced)\n",
    "    \"\"\"\n",
    "    counts = Counter(y)\n",
    "    minority_class = min(counts, key=counts.get)\n",
    "    majority_class = max(counts, key=counts.get)\n",
    "    \n",
    "    # Get indices for each class\n",
    "    minority_indices = np.where(y == minority_class)[0]\n",
    "    majority_indices = np.where(y == majority_class)[0]\n",
    "    \n",
    "    # Calculate how many samples to generate\n",
    "    n_target = int(len(majority_indices) * target_ratio)\n",
    "    n_to_generate = n_target - len(minority_indices)\n",
    "    \n",
    "    if n_to_generate <= 0:\n",
    "        return X, y\n",
    "    \n",
    "    # Randomly sample with replacement\n",
    "    oversample_indices = np.random.choice(minority_indices, n_to_generate, replace=True)\n",
    "    \n",
    "    X_new = np.vstack([X, X[oversample_indices]])\n",
    "    y_new = np.concatenate([y, y[oversample_indices]])\n",
    "    \n",
    "    return X_new, y_new\n",
    "\n",
    "# Apply random oversampling\n",
    "X_ros, y_ros = random_oversample(X_train, y_train)\n",
    "\n",
    "print(\"Random Oversampling:\")\n",
    "print(f\"  Before: {Counter(y_train)}\")\n",
    "print(f\"  After: {Counter(y_ros)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 SMOTE (Synthetic Minority Over-sampling Technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_smote(X, y, k=5, target_ratio=1.0):\n",
    "    \"\"\"\n",
    "    Simplified SMOTE implementation.\n",
    "    Creates synthetic samples by interpolating between minority samples and their neighbors.\n",
    "    \"\"\"\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    \n",
    "    counts = Counter(y)\n",
    "    minority_class = min(counts, key=counts.get)\n",
    "    majority_class = max(counts, key=counts.get)\n",
    "    \n",
    "    X_minority = X[y == minority_class]\n",
    "    X_majority = X[y == majority_class]\n",
    "    \n",
    "    # Calculate how many samples to generate\n",
    "    n_target = int(len(X_majority) * target_ratio)\n",
    "    n_to_generate = n_target - len(X_minority)\n",
    "    \n",
    "    if n_to_generate <= 0:\n",
    "        return X, y\n",
    "    \n",
    "    # Fit nearest neighbors on minority class\n",
    "    nn = NearestNeighbors(n_neighbors=k+1)\n",
    "    nn.fit(X_minority)\n",
    "    \n",
    "    # Generate synthetic samples\n",
    "    synthetic_samples = []\n",
    "    for _ in range(n_to_generate):\n",
    "        # Pick a random minority sample\n",
    "        idx = np.random.randint(0, len(X_minority))\n",
    "        sample = X_minority[idx]\n",
    "        \n",
    "        # Find its neighbors\n",
    "        _, neighbor_indices = nn.kneighbors([sample])\n",
    "        neighbor_idx = np.random.choice(neighbor_indices[0][1:])  # Exclude itself\n",
    "        neighbor = X_minority[neighbor_idx]\n",
    "        \n",
    "        # Interpolate\n",
    "        alpha = np.random.random()\n",
    "        synthetic = sample + alpha * (neighbor - sample)\n",
    "        synthetic_samples.append(synthetic)\n",
    "    \n",
    "    X_synthetic = np.array(synthetic_samples)\n",
    "    y_synthetic = np.full(len(synthetic_samples), minority_class)\n",
    "    \n",
    "    X_new = np.vstack([X, X_synthetic])\n",
    "    y_new = np.concatenate([y, y_synthetic])\n",
    "    \n",
    "    return X_new, y_new\n",
    "\n",
    "# Apply SMOTE\n",
    "X_smote, y_smote = simple_smote(X_train, y_train)\n",
    "\n",
    "print(\"SMOTE:\")\n",
    "print(f\"  Before: {Counter(y_train)}\")\n",
    "print(f\"  After: {Counter(y_smote)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize SMOTE effect in 2D\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce to 2D for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_train_2d = pca.fit_transform(X_train)\n",
    "X_smote_2d = pca.transform(X_smote)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Original\n",
    "axes[0].scatter(X_train_2d[y_train == 0, 0], X_train_2d[y_train == 0, 1], \n",
    "                alpha=0.5, label='Majority', c='steelblue', s=20)\n",
    "axes[0].scatter(X_train_2d[y_train == 1, 0], X_train_2d[y_train == 1, 1], \n",
    "                alpha=0.8, label='Minority', c='coral', s=50)\n",
    "axes[0].set_title(f'Original (n={len(y_train)})')\n",
    "axes[0].legend()\n",
    "\n",
    "# After SMOTE\n",
    "axes[1].scatter(X_smote_2d[y_smote == 0, 0], X_smote_2d[y_smote == 0, 1], \n",
    "                alpha=0.5, label='Majority', c='steelblue', s=20)\n",
    "axes[1].scatter(X_smote_2d[y_smote == 1, 0], X_smote_2d[y_smote == 1, 1], \n",
    "                alpha=0.8, label='Minority (incl. synthetic)', c='coral', s=50)\n",
    "axes[1].set_title(f'After SMOTE (n={len(y_smote)})')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_undersample(X, y, target_ratio=1.0):\n",
    "    \"\"\"\n",
    "    Random undersampling of majority class.\n",
    "    \"\"\"\n",
    "    counts = Counter(y)\n",
    "    minority_class = min(counts, key=counts.get)\n",
    "    majority_class = max(counts, key=counts.get)\n",
    "    \n",
    "    minority_indices = np.where(y == minority_class)[0]\n",
    "    majority_indices = np.where(y == majority_class)[0]\n",
    "    \n",
    "    # Calculate target majority size\n",
    "    n_target = int(len(minority_indices) / target_ratio)\n",
    "    n_target = min(n_target, len(majority_indices))\n",
    "    \n",
    "    # Randomly sample majority class\n",
    "    selected_majority = np.random.choice(majority_indices, n_target, replace=False)\n",
    "    \n",
    "    # Combine\n",
    "    selected_indices = np.concatenate([minority_indices, selected_majority])\n",
    "    \n",
    "    return X[selected_indices], y[selected_indices]\n",
    "\n",
    "# Apply undersampling\n",
    "X_under, y_under = random_undersample(X_train, y_train)\n",
    "\n",
    "print(\"Random Undersampling:\")\n",
    "print(f\"  Before: {Counter(y_train)}\")\n",
    "print(f\"  After: {Counter(y_under)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tomek_links_undersample(X, y):\n",
    "    \"\"\"\n",
    "    Remove Tomek links (pairs of different class nearest neighbors).\n",
    "    Cleans the boundary between classes.\n",
    "    \"\"\"\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    \n",
    "    nn = NearestNeighbors(n_neighbors=2)\n",
    "    nn.fit(X)\n",
    "    \n",
    "    _, indices = nn.kneighbors(X)\n",
    "    \n",
    "    # Find Tomek links\n",
    "    tomek_indices = set()\n",
    "    for i in range(len(X)):\n",
    "        neighbor = indices[i, 1]  # Nearest neighbor (excluding self)\n",
    "        \n",
    "        # Check if they form a Tomek link\n",
    "        if y[i] != y[neighbor]:  # Different classes\n",
    "            neighbor_of_neighbor = indices[neighbor, 1]\n",
    "            if neighbor_of_neighbor == i:  # Mutual nearest neighbors\n",
    "                # Remove the majority class sample\n",
    "                counts = Counter(y)\n",
    "                majority_class = max(counts, key=counts.get)\n",
    "                if y[i] == majority_class:\n",
    "                    tomek_indices.add(i)\n",
    "                else:\n",
    "                    tomek_indices.add(neighbor)\n",
    "    \n",
    "    # Keep non-Tomek samples\n",
    "    mask = np.ones(len(X), dtype=bool)\n",
    "    mask[list(tomek_indices)] = False\n",
    "    \n",
    "    return X[mask], y[mask], len(tomek_indices)\n",
    "\n",
    "# Apply Tomek links\n",
    "X_tomek, y_tomek, n_removed = tomek_links_undersample(X_train, y_train)\n",
    "\n",
    "print(f\"\\nTomek Links Undersampling:\")\n",
    "print(f\"  Removed {n_removed} Tomek link samples\")\n",
    "print(f\"  Before: {Counter(y_train)}\")\n",
    "print(f\"  After: {Counter(y_tomek)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Compare Resampling Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all resampling methods\n",
    "resampling_results = []\n",
    "\n",
    "# Prepare datasets\n",
    "datasets = {\n",
    "    'Original': (X_train_s, y_train),\n",
    "    'Random Oversampling': (scaler.fit_transform(X_ros), y_ros),\n",
    "    'SMOTE': (scaler.fit_transform(X_smote), y_smote),\n",
    "    'Random Undersampling': (scaler.fit_transform(X_under), y_under),\n",
    "    'Tomek Links': (scaler.fit_transform(X_tomek), y_tomek)\n",
    "}\n",
    "\n",
    "for name, (X_resampled, y_resampled) in datasets.items():\n",
    "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    model.fit(X_resampled, y_resampled)\n",
    "    y_pred = model.predict(X_test_s)\n",
    "    y_proba = model.predict_proba(X_test_s)[:, 1]\n",
    "    \n",
    "    resampling_results.append({\n",
    "        'Method': name,\n",
    "        'Train Size': len(y_resampled),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1': f1_score(y_test, y_pred),\n",
    "        'AUC-ROC': roc_auc_score(y_test, y_proba)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(resampling_results)\n",
    "print(\"Resampling Methods Comparison:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "metrics = ['Precision', 'Recall', 'F1', 'AUC-ROC']\n",
    "x = np.arange(len(results_df))\n",
    "width = 0.2\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax.bar(x + i*width, results_df[metric], width, label=metric)\n",
    "\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels(results_df['Method'], rotation=45, ha='right')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Resampling Methods Performance Comparison')\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Class-Weighted Loss Functions\n",
    "\n",
    "Instead of resampling, we can adjust the loss function to penalize misclassification of minority class more heavily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_balanced_weights(y):\n",
    "    \"\"\"Compute class weights inversely proportional to class frequencies.\"\"\"\n",
    "    classes = np.unique(y)\n",
    "    weights = compute_class_weight('balanced', classes=classes, y=y)\n",
    "    return dict(zip(classes, weights))\n",
    "\n",
    "# Calculate balanced weights\n",
    "class_weights = compute_balanced_weights(y_train)\n",
    "print(f\"Computed class weights: {class_weights}\")\n",
    "print(f\"Minority class is weighted {class_weights[1]/class_weights[0]:.1f}x more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare weighted vs unweighted models\n",
    "models = {\n",
    "    'Unweighted Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Weighted Logistic Regression': LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42),\n",
    "    'Unweighted Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Weighted Random Forest': RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
    "}\n",
    "\n",
    "weight_results = []\n",
    "for name, model in models.items():\n",
    "    result = evaluate_model(model, X_train_s, X_test_s, y_train, y_test, name)\n",
    "    weight_results.append(result)\n",
    "\n",
    "weight_df = pd.DataFrame(weight_results)\n",
    "print(\"Class Weighting Comparison:\")\n",
    "print(weight_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom class weights\n",
    "custom_weights = {0: 1, 1: 10}  # 10x weight for minority class\n",
    "\n",
    "lr_custom = LogisticRegression(class_weight=custom_weights, max_iter=1000, random_state=42)\n",
    "lr_custom.fit(X_train_s, y_train)\n",
    "y_pred_custom = lr_custom.predict(X_test_s)\n",
    "\n",
    "print(\"\\nCustom Weights (1:10):\")\n",
    "print(f\"  Precision: {precision_score(y_test, y_pred_custom):.4f}\")\n",
    "print(f\"  Recall: {recall_score(y_test, y_pred_custom):.4f}\")\n",
    "print(f\"  F1: {f1_score(y_test, y_pred_custom):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Focal Loss\n",
    "\n",
    "Focal loss is designed for extreme class imbalance. It down-weights easy examples and focuses on hard examples.\n",
    "\n",
    "**Formula**: FL(p_t) = -α_t * (1 - p_t)^γ * log(p_t)\n",
    "\n",
    "Where:\n",
    "- p_t is the predicted probability for the true class\n",
    "- γ (gamma) is the focusing parameter (typically 2)\n",
    "- α (alpha) is the class balancing weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(y_true, y_pred, gamma=2.0, alpha=0.25):\n",
    "    \"\"\"\n",
    "    Compute focal loss for binary classification.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels (0 or 1)\n",
    "        y_pred: Predicted probabilities\n",
    "        gamma: Focusing parameter (default: 2)\n",
    "        alpha: Class balancing weight for positive class\n",
    "    \"\"\"\n",
    "    epsilon = 1e-7\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    \n",
    "    # Cross entropy\n",
    "    ce = -y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred)\n",
    "    \n",
    "    # Focal weight\n",
    "    p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "    focal_weight = (1 - p_t) ** gamma\n",
    "    \n",
    "    # Alpha weight\n",
    "    alpha_weight = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "    \n",
    "    # Focal loss\n",
    "    fl = alpha_weight * focal_weight * ce\n",
    "    \n",
    "    return np.mean(fl)\n",
    "\n",
    "# Compare CE and Focal Loss\n",
    "y_true = np.array([1, 1, 0, 0])\n",
    "y_pred_easy = np.array([0.95, 0.90, 0.10, 0.05])  # Easy predictions\n",
    "y_pred_hard = np.array([0.60, 0.55, 0.40, 0.45])  # Hard predictions\n",
    "\n",
    "print(\"Loss Comparison:\")\n",
    "print(f\"\\nEasy predictions (confident):\")\n",
    "ce_easy = -np.mean(y_true * np.log(y_pred_easy + 1e-7) + (1 - y_true) * np.log(1 - y_pred_easy + 1e-7))\n",
    "print(f\"  Cross-Entropy: {ce_easy:.4f}\")\n",
    "print(f\"  Focal Loss (γ=2): {focal_loss(y_true, y_pred_easy, gamma=2):.4f}\")\n",
    "\n",
    "print(f\"\\nHard predictions (uncertain):\")\n",
    "ce_hard = -np.mean(y_true * np.log(y_pred_hard + 1e-7) + (1 - y_true) * np.log(1 - y_pred_hard + 1e-7))\n",
    "print(f\"  Cross-Entropy: {ce_hard:.4f}\")\n",
    "print(f\"  Focal Loss (γ=2): {focal_loss(y_true, y_pred_hard, gamma=2):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize focal loss vs cross-entropy\n",
    "p = np.linspace(0.01, 0.99, 100)\n",
    "\n",
    "# For true class = 1\n",
    "ce_loss = -np.log(p)\n",
    "fl_gamma1 = -(1 - p) * np.log(p)\n",
    "fl_gamma2 = -((1 - p) ** 2) * np.log(p)\n",
    "fl_gamma5 = -((1 - p) ** 5) * np.log(p)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(p, ce_loss, 'b-', label='Cross-Entropy', linewidth=2)\n",
    "ax.plot(p, fl_gamma1, 'r--', label='Focal (γ=1)', linewidth=2)\n",
    "ax.plot(p, fl_gamma2, 'g--', label='Focal (γ=2)', linewidth=2)\n",
    "ax.plot(p, fl_gamma5, 'm--', label='Focal (γ=5)', linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Predicted Probability for True Class')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Cross-Entropy vs Focal Loss (True Class = 1)')\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 5])\n",
    "ax.axvline(x=0.5, color='gray', linestyle=':', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLossClassifier:\n",
    "    \"\"\"\n",
    "    Simple logistic regression trained with focal loss using gradient descent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, gamma=2.0, alpha=0.25, lr=0.01, n_iter=1000):\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.lr = lr\n",
    "        self.n_iter = n_iter\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -250, 250)))\n",
    "    \n",
    "    def _focal_loss_gradient(self, y, p):\n",
    "        \"\"\"Compute gradient of focal loss.\"\"\"\n",
    "        epsilon = 1e-7\n",
    "        p = np.clip(p, epsilon, 1 - epsilon)\n",
    "        \n",
    "        # Focal loss gradient components\n",
    "        p_t = y * p + (1 - y) * (1 - p)\n",
    "        focal_weight = (1 - p_t) ** self.gamma\n",
    "        alpha_weight = y * self.alpha + (1 - y) * (1 - self.alpha)\n",
    "        \n",
    "        grad = alpha_weight * (self.gamma * (1 - p_t) ** (self.gamma - 1) * \n",
    "                               (y - p) * np.log(p_t + epsilon) + \n",
    "                               focal_weight * (p - y) / (p_t + epsilon))\n",
    "        \n",
    "        return grad\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        for _ in range(self.n_iter):\n",
    "            z = np.dot(X, self.weights) + self.bias\n",
    "            p = self._sigmoid(z)\n",
    "            \n",
    "            grad = self._focal_loss_gradient(y, p)\n",
    "            \n",
    "            self.weights -= self.lr * np.dot(X.T, grad) / n_samples\n",
    "            self.bias -= self.lr * np.mean(grad)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        p = self._sigmoid(z)\n",
    "        return np.column_stack([1 - p, p])\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return (self.predict_proba(X)[:, 1] >= threshold).astype(int)\n",
    "\n",
    "# Train focal loss classifier\n",
    "fl_clf = FocalLossClassifier(gamma=2.0, alpha=0.75, lr=0.1, n_iter=1000)\n",
    "fl_clf.fit(X_train_s, y_train)\n",
    "y_pred_fl = fl_clf.predict(X_test_s)\n",
    "\n",
    "print(\"Focal Loss Classifier:\")\n",
    "print(f\"  Precision: {precision_score(y_test, y_pred_fl):.4f}\")\n",
    "print(f\"  Recall: {recall_score(y_test, y_pred_fl):.4f}\")\n",
    "print(f\"  F1: {f1_score(y_test, y_pred_fl):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Threshold Adjustment\n",
    "\n",
    "Another approach is to adjust the classification threshold instead of the default 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train_s, y_train)\n",
    "y_proba = model.predict_proba(X_test_s)[:, 1]\n",
    "\n",
    "# Evaluate at different thresholds\n",
    "thresholds = np.arange(0.1, 0.9, 0.1)\n",
    "threshold_results = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    y_pred = (y_proba >= thresh).astype(int)\n",
    "    threshold_results.append({\n",
    "        'Threshold': thresh,\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1': f1_score(y_test, y_pred)\n",
    "    })\n",
    "\n",
    "thresh_df = pd.DataFrame(threshold_results)\n",
    "print(\"Effect of Classification Threshold:\")\n",
    "print(thresh_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize precision-recall trade-off\n",
    "precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_test, y_proba)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Precision-Recall curve\n",
    "axes[0].plot(recall_curve, precision_curve, 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('Recall')\n",
    "axes[0].set_ylabel('Precision')\n",
    "axes[0].set_title(f'Precision-Recall Curve (AP = {average_precision_score(y_test, y_proba):.3f})')\n",
    "\n",
    "# Threshold vs metrics\n",
    "axes[1].plot(thresh_df['Threshold'], thresh_df['Precision'], 'b-o', label='Precision', linewidth=2)\n",
    "axes[1].plot(thresh_df['Threshold'], thresh_df['Recall'], 'r-o', label='Recall', linewidth=2)\n",
    "axes[1].plot(thresh_df['Threshold'], thresh_df['F1'], 'g-o', label='F1', linewidth=2)\n",
    "axes[1].set_xlabel('Classification Threshold')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_title('Metrics vs Threshold')\n",
    "axes[1].legend()\n",
    "axes[1].axvline(x=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_threshold(y_true, y_proba, metric='f1'):\n",
    "    \"\"\"Find threshold that maximizes the specified metric.\"\"\"\n",
    "    thresholds = np.arange(0.05, 0.95, 0.01)\n",
    "    best_thresh, best_score = 0.5, 0\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        y_pred = (y_proba >= thresh).astype(int)\n",
    "        \n",
    "        if metric == 'f1':\n",
    "            score = f1_score(y_true, y_pred)\n",
    "        elif metric == 'precision':\n",
    "            score = precision_score(y_true, y_pred)\n",
    "        elif metric == 'recall':\n",
    "            score = recall_score(y_true, y_pred)\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_thresh = thresh\n",
    "    \n",
    "    return best_thresh, best_score\n",
    "\n",
    "optimal_thresh, optimal_f1 = find_optimal_threshold(y_test, y_proba, 'f1')\n",
    "print(f\"Optimal threshold for F1: {optimal_thresh:.2f} (F1 = {optimal_f1:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation Strategies for Imbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive evaluation\n",
    "def comprehensive_evaluation(y_true, y_pred, y_proba=None):\n",
    "    \"\"\"Generate comprehensive metrics for imbalanced classification.\"\"\"\n",
    "    print(\"Confusion Matrix:\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(cm)\n",
    "    \n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(f\"\\nTrue Negatives: {tn}\")\n",
    "    print(f\"False Positives: {fp}\")\n",
    "    print(f\"False Negatives: {fn}\")\n",
    "    print(f\"True Positives: {tp}\")\n",
    "    \n",
    "    print(f\"\\nMetrics:\")\n",
    "    print(f\"  Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"  Balanced Accuracy: {(tp/(tp+fn) + tn/(tn+fp))/2:.4f}\")\n",
    "    print(f\"  Precision: {precision_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"  Recall (Sensitivity): {recall_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"  Specificity: {tn/(tn+fp):.4f}\")\n",
    "    print(f\"  F1 Score: {f1_score(y_true, y_pred):.4f}\")\n",
    "    \n",
    "    if y_proba is not None:\n",
    "        print(f\"  AUC-ROC: {roc_auc_score(y_true, y_proba):.4f}\")\n",
    "        print(f\"  Average Precision: {average_precision_score(y_true, y_proba):.4f}\")\n",
    "\n",
    "# Best model evaluation\n",
    "best_model = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
    "best_model.fit(X_train_s, y_train)\n",
    "y_pred_best = best_model.predict(X_test_s)\n",
    "y_proba_best = best_model.predict_proba(X_test_s)[:, 1]\n",
    "\n",
    "comprehensive_evaluation(y_test, y_pred_best, y_proba_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Counts\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title('Confusion Matrix (Counts)')\n",
    "\n",
    "# Normalized\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.2%', cmap='Blues', ax=axes[1])\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_title('Confusion Matrix (Normalized)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC and PR curves comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Compare models\n",
    "models_to_compare = {\n",
    "    'Unweighted LR': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Weighted LR': LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42),\n",
    "    'SMOTE + LR': LogisticRegression(max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "# Prepare SMOTE data\n",
    "X_smote_s = scaler.fit_transform(X_smote)\n",
    "\n",
    "for name, model in models_to_compare.items():\n",
    "    if 'SMOTE' in name:\n",
    "        model.fit(X_smote_s, y_smote)\n",
    "    else:\n",
    "        model.fit(X_train_s, y_train)\n",
    "    \n",
    "    y_proba = model.predict_proba(X_test_s)[:, 1]\n",
    "    \n",
    "    # ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    axes[0].plot(fpr, tpr, label=f'{name} (AUC={auc:.3f})', linewidth=2)\n",
    "    \n",
    "    # PR curve\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "    ap = average_precision_score(y_test, y_proba)\n",
    "    axes[1].plot(rec, prec, label=f'{name} (AP={ap:.3f})', linewidth=2)\n",
    "\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('ROC Curves')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].axhline(y=Counter(y_test)[1]/len(y_test), color='k', linestyle='--', alpha=0.5, label='Baseline')\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Precision-Recall Curves')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hands-on Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Complete Imbalanced Classification Pipeline\n",
    "print(\"Exercise: Build a Pipeline for Highly Imbalanced Data\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a very imbalanced dataset (1% minority)\n",
    "X_exercise, y_exercise = create_imbalanced_dataset(n_samples=10000, imbalance_ratio=0.01)\n",
    "\n",
    "X_train_ex, X_test_ex, y_train_ex, y_test_ex = train_test_split(\n",
    "    X_exercise, y_exercise, test_size=0.2, random_state=42, stratify=y_exercise\n",
    ")\n",
    "\n",
    "scaler_ex = StandardScaler()\n",
    "X_train_ex_s = scaler_ex.fit_transform(X_train_ex)\n",
    "X_test_ex_s = scaler_ex.transform(X_test_ex)\n",
    "\n",
    "print(f\"Training set class distribution: {Counter(y_train_ex)}\")\n",
    "print(f\"Test set class distribution: {Counter(y_test_ex)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Baseline\n",
    "baseline = LogisticRegression(max_iter=1000, random_state=42)\n",
    "baseline.fit(X_train_ex_s, y_train_ex)\n",
    "y_pred_baseline = baseline.predict(X_test_ex_s)\n",
    "\n",
    "print(\"Step 1: Baseline Model\")\n",
    "print(f\"  F1 Score: {f1_score(y_test_ex, y_pred_baseline):.4f}\")\n",
    "print(f\"  Recall: {recall_score(y_test_ex, y_pred_baseline):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Apply SMOTE\n",
    "X_train_smote, y_train_smote = simple_smote(X_train_ex, y_train_ex)\n",
    "X_train_smote_s = scaler_ex.fit_transform(X_train_smote)\n",
    "\n",
    "smote_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "smote_model.fit(X_train_smote_s, y_train_smote)\n",
    "y_pred_smote = smote_model.predict(X_test_ex_s)\n",
    "\n",
    "print(\"Step 2: With SMOTE\")\n",
    "print(f\"  F1 Score: {f1_score(y_test_ex, y_pred_smote):.4f}\")\n",
    "print(f\"  Recall: {recall_score(y_test_ex, y_pred_smote):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Class weighting\n",
    "weighted_model = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
    "weighted_model.fit(X_train_ex_s, y_train_ex)\n",
    "y_pred_weighted = weighted_model.predict(X_test_ex_s)\n",
    "\n",
    "print(\"Step 3: With Class Weighting\")\n",
    "print(f\"  F1 Score: {f1_score(y_test_ex, y_pred_weighted):.4f}\")\n",
    "print(f\"  Recall: {recall_score(y_test_ex, y_pred_weighted):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Threshold optimization\n",
    "y_proba_ex = weighted_model.predict_proba(X_test_ex_s)[:, 1]\n",
    "opt_thresh, opt_f1 = find_optimal_threshold(y_test_ex, y_proba_ex, 'f1')\n",
    "y_pred_opt = (y_proba_ex >= opt_thresh).astype(int)\n",
    "\n",
    "print(\"Step 4: With Optimized Threshold\")\n",
    "print(f\"  Optimal Threshold: {opt_thresh:.2f}\")\n",
    "print(f\"  F1 Score: {f1_score(y_test_ex, y_pred_opt):.4f}\")\n",
    "print(f\"  Recall: {recall_score(y_test_ex, y_pred_opt):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all approaches\n",
    "print(\"\\nFinal Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "approaches = [\n",
    "    ('Baseline', y_pred_baseline),\n",
    "    ('SMOTE', y_pred_smote),\n",
    "    ('Class Weighting', y_pred_weighted),\n",
    "    ('Optimized Threshold', y_pred_opt)\n",
    "]\n",
    "\n",
    "for name, y_pred in approaches:\n",
    "    print(f\"{name}: F1={f1_score(y_test_ex, y_pred):.4f}, Recall={recall_score(y_test_ex, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Standard accuracy is misleading** for imbalanced data\n",
    "2. **Use appropriate metrics**: F1, Precision, Recall, AUC-PR\n",
    "3. **Resampling options**: SMOTE for oversampling, random/Tomek for undersampling\n",
    "4. **Class weighting** is often simpler and equally effective\n",
    "5. **Focal loss** works well for extreme imbalance\n",
    "6. **Threshold optimization** can significantly improve results\n",
    "\n",
    "### Decision Guide\n",
    "\n",
    "| Imbalance Level | Recommended Approach |\n",
    "|-----------------|---------------------|\n",
    "| Mild (20-40%) | Class weighting |\n",
    "| Moderate (5-20%) | SMOTE + Class weighting |\n",
    "| Severe (1-5%) | SMOTE + Focal loss + Threshold tuning |\n",
    "| Extreme (<1%) | Anomaly detection or specialized methods |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
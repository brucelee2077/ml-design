{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 12: Offline Evaluation Metrics\n",
    "\n",
    "## Module 5: Evaluation\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "\n",
    "1. **Master classification metrics** - precision, recall, F1-score, ROC-AUC, PR-AUC, confusion matrix\n",
    "2. **Apply regression metrics** - MSE, RMSE, MAE, R-squared, MAPE\n",
    "3. **Understand ranking metrics** - Precision@K, Recall@K, MRR, mAP, nDCG\n",
    "4. **Evaluate NLP models** - BLEU, ROUGE scores\n",
    "5. **Choose appropriate metrics** based on business objectives and model types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report,\n",
    "    roc_curve, roc_auc_score, precision_recall_curve, average_precision_score,\n",
    "    mean_squared_error, mean_absolute_error, r2_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Introduction to Evaluation Metrics\n",
    "\n",
    "### Why Offline Evaluation Matters\n",
    "\n",
    "Offline evaluation metrics are crucial for:\n",
    "- **Model selection**: Comparing different models before deployment\n",
    "- **Hyperparameter tuning**: Finding optimal model configurations\n",
    "- **Quality assurance**: Ensuring models meet minimum performance thresholds\n",
    "- **Debugging**: Identifying specific weaknesses in model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample datasets for demonstrations\n",
    "\n",
    "def create_classification_data(n_samples=1000, imbalance_ratio=0.3):\n",
    "    \"\"\"Create binary classification dataset with configurable class imbalance.\"\"\"\n",
    "    X, y = make_classification(\n",
    "        n_samples=n_samples, n_features=20, n_informative=10,\n",
    "        n_redundant=5, n_clusters_per_class=2,\n",
    "        weights=[1 - imbalance_ratio, imbalance_ratio], random_state=42\n",
    "    )\n",
    "    return train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "def create_regression_data(n_samples=1000, noise=10):\n",
    "    \"\"\"Create regression dataset.\"\"\"\n",
    "    X, y = make_regression(\n",
    "        n_samples=n_samples, n_features=20, n_informative=10,\n",
    "        noise=noise, random_state=42\n",
    "    )\n",
    "    return train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create datasets\n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = create_classification_data()\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = create_regression_data()\n",
    "\n",
    "print(f\"Classification: {len(y_train_clf)} train, {len(y_test_clf)} test\")\n",
    "print(f\"Class distribution (test): {np.bincount(y_test_clf)}\")\n",
    "print(f\"Regression: {len(y_train_reg)} train, {len(y_test_reg)} test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Classification Metrics\n",
    "\n",
    "### 2.1 The Confusion Matrix\n",
    "\n",
    "The confusion matrix is the foundation for understanding classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a classifier for demonstration\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "clf.fit(X_train_clf, y_train_clf)\n",
    "\n",
    "y_pred = clf.predict(X_test_clf)\n",
    "y_pred_proba = clf.predict_proba(X_test_clf)[:, 1]\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, labels=['Negative', 'Positive']):\n",
    "    \"\"\"Plot a detailed confusion matrix with annotations.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Raw counts\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=labels, yticklabels=labels, ax=axes[0])\n",
    "    axes[0].set_title('Confusion Matrix (Counts)')\n",
    "    axes[0].set_ylabel('Actual')\n",
    "    axes[0].set_xlabel('Predicted')\n",
    "    \n",
    "    # Normalized by row\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    sns.heatmap(cm_norm, annot=True, fmt='.2%', cmap='Blues',\n",
    "                xticklabels=labels, yticklabels=labels, ax=axes[1])\n",
    "    axes[1].set_title('Confusion Matrix (Normalized)')\n",
    "    axes[1].set_ylabel('Actual')\n",
    "    axes[1].set_xlabel('Predicted')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(f\"\\nConfusion Matrix Breakdown:\")\n",
    "    print(f\"  TN: {tn} | FP: {fp}\")\n",
    "    print(f\"  FN: {fn} | TP: {tp}\")\n",
    "\n",
    "plot_confusion_matrix(y_test_clf, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Precision, Recall, and F1-Score\n",
    "\n",
    "- **Precision**: TP / (TP + FP) - How reliable are positive predictions?\n",
    "- **Recall**: TP / (TP + FN) - How many positives did we find?\n",
    "- **F1-Score**: Harmonic mean of precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ClassificationMetrics:\n",
    "    accuracy: float\n",
    "    precision: float\n",
    "    recall: float\n",
    "    f1: float\n",
    "    specificity: float\n",
    "    \n",
    "    def display(self):\n",
    "        print(\"=\" * 50)\n",
    "        print(\"Classification Metrics Report\")\n",
    "        print(\"=\" * 50)\n",
    "        for name, value in [(\"Accuracy\", self.accuracy), (\"Precision\", self.precision),\n",
    "                            (\"Recall\", self.recall), (\"F1-Score\", self.f1),\n",
    "                            (\"Specificity\", self.specificity)]:\n",
    "            bar = \"#\" * int(value * 20) + \"-\" * (20 - int(value * 20))\n",
    "            print(f\"{name:12} [{bar}] {value:.4f}\")\n",
    "\n",
    "def compute_classification_metrics(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return ClassificationMetrics(\n",
    "        accuracy=accuracy_score(y_true, y_pred),\n",
    "        precision=precision_score(y_true, y_pred, zero_division=0),\n",
    "        recall=recall_score(y_true, y_pred, zero_division=0),\n",
    "        f1=f1_score(y_true, y_pred, zero_division=0),\n",
    "        specificity=tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    )\n",
    "\n",
    "metrics = compute_classification_metrics(y_test_clf, y_pred)\n",
    "metrics.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 The Accuracy Paradox\n",
    "\n",
    "Accuracy can be misleading with imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_accuracy_paradox():\n",
    "    \"\"\"Show why accuracy is misleading for imbalanced datasets.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    n_samples = 10000\n",
    "    y_imb = np.concatenate([np.zeros(9900), np.ones(100)]).astype(int)\n",
    "    \n",
    "    # Always predict negative\n",
    "    y_pred_neg = np.zeros(n_samples).astype(int)\n",
    "    # Random predictions\n",
    "    y_pred_random = np.random.randint(0, 2, n_samples)\n",
    "    \n",
    "    strategies = [(\"Always Negative\", y_pred_neg), (\"Random\", y_pred_random)]\n",
    "    \n",
    "    print(\"Accuracy Paradox Demonstration\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Dataset: 99% negative, 1% positive\\n\")\n",
    "    \n",
    "    for name, y_pred in strategies:\n",
    "        acc = accuracy_score(y_imb, y_pred)\n",
    "        rec = recall_score(y_imb, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_imb, y_pred, zero_division=0)\n",
    "        print(f\"{name:20} Acc: {acc:.2%} | Recall: {rec:.2%} | F1: {f1:.4f}\")\n",
    "    \n",
    "    print(\"\\nKey Insight: 'Always Negative' has 99% accuracy but 0% recall!\")\n",
    "\n",
    "demonstrate_accuracy_paradox()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 ROC Curve and AUC\n",
    "\n",
    "ROC-AUC measures the model's ability to rank positive examples higher than negative ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_true, y_scores, model_name=\"Model\"):\n",
    "    \"\"\"Plot ROC curve with annotations.\"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    roc_auc = roc_auc_score(y_true, y_scores)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # ROC Curve\n",
    "    axes[0].plot(fpr, tpr, 'b-', linewidth=2, label=f'{model_name} (AUC = {roc_auc:.4f})')\n",
    "    axes[0].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random (AUC = 0.5)')\n",
    "    axes[0].fill_between(fpr, tpr, alpha=0.3)\n",
    "    \n",
    "    optimal_idx = np.argmax(tpr - fpr)\n",
    "    axes[0].plot(fpr[optimal_idx], tpr[optimal_idx], 'ro', markersize=10,\n",
    "                 label=f'Optimal Threshold = {thresholds[optimal_idx]:.3f}')\n",
    "    \n",
    "    axes[0].set_xlabel('False Positive Rate')\n",
    "    axes[0].set_ylabel('True Positive Rate')\n",
    "    axes[0].set_title('ROC Curve')\n",
    "    axes[0].legend(loc='lower right')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Metrics vs Threshold\n",
    "    thresh_range = np.linspace(0.01, 0.99, 50)\n",
    "    precs, recs, f1s = [], [], []\n",
    "    for t in thresh_range:\n",
    "        y_pred_t = (y_scores >= t).astype(int)\n",
    "        precs.append(precision_score(y_true, y_pred_t, zero_division=0))\n",
    "        recs.append(recall_score(y_true, y_pred_t, zero_division=0))\n",
    "        f1s.append(f1_score(y_true, y_pred_t, zero_division=0))\n",
    "    \n",
    "    axes[1].plot(thresh_range, precs, 'b-', label='Precision')\n",
    "    axes[1].plot(thresh_range, recs, 'g-', label='Recall')\n",
    "    axes[1].plot(thresh_range, f1s, 'r-', label='F1-Score')\n",
    "    axes[1].set_xlabel('Threshold')\n",
    "    axes[1].set_ylabel('Score')\n",
    "    axes[1].set_title('Metrics vs Threshold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return roc_auc\n",
    "\n",
    "roc_auc = plot_roc_curve(y_test_clf, y_pred_proba, \"Logistic Regression\")\n",
    "print(f\"\\nROC-AUC Score: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Precision-Recall Curve and PR-AUC\n",
    "\n",
    "For imbalanced datasets, PR-AUC is often more informative than ROC-AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pr_curve(y_true, y_scores, model_name=\"Model\"):\n",
    "    \"\"\"Plot Precision-Recall curve.\"\"\"\n",
    "    precision_vals, recall_vals, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "    pr_auc = average_precision_score(y_true, y_scores)\n",
    "    baseline = y_true.sum() / len(y_true)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(recall_vals, precision_vals, 'b-', linewidth=2,\n",
    "             label=f'{model_name} (AP = {pr_auc:.4f})')\n",
    "    plt.axhline(y=baseline, color='r', linestyle='--', \n",
    "                label=f'Random Baseline = {baseline:.4f}')\n",
    "    plt.fill_between(recall_vals, precision_vals, alpha=0.3)\n",
    "    \n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return pr_auc\n",
    "\n",
    "pr_auc = plot_pr_curve(y_test_clf, y_pred_proba, \"Logistic Regression\")\n",
    "print(f\"\\nPR-AUC Score: {pr_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Multi-class Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_multiclass_metrics():\n",
    "    \"\"\"Demonstrate metrics for multi-class classification.\"\"\"\n",
    "    X, y = make_classification(\n",
    "        n_samples=1000, n_features=20, n_informative=15,\n",
    "        n_classes=4, n_clusters_per_class=2, random_state=42\n",
    "    )\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print(\"Multi-class Classification Metrics\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for avg in ['macro', 'micro', 'weighted']:\n",
    "        prec = precision_score(y_test, y_pred, average=avg)\n",
    "        rec = recall_score(y_test, y_pred, average=avg)\n",
    "        f1 = f1_score(y_test, y_pred, average=avg)\n",
    "        print(f\"{avg:10} | P: {prec:.4f} | R: {rec:.4f} | F1: {f1:.4f}\")\n",
    "    \n",
    "    print(\"\\nPer-Class Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Multi-class Confusion Matrix')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "demonstrate_multiclass_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Regression Metrics\n",
    "\n",
    "| Metric | Formula | Use Case |\n",
    "|--------|---------|----------|\n",
    "| MSE | mean((y - y_hat)^2) | Penalize large errors |\n",
    "| RMSE | sqrt(MSE) | Same units as target |\n",
    "| MAE | mean(abs(y - y_hat)) | Robust to outliers |\n",
    "| R-squared | 1 - SS_res/SS_tot | Variance explained |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train regression model\n",
    "reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "reg.fit(X_train_reg, y_train_reg)\n",
    "y_pred_reg = reg.predict(X_test_reg)\n",
    "\n",
    "@dataclass\n",
    "class RegressionMetrics:\n",
    "    mse: float\n",
    "    rmse: float\n",
    "    mae: float\n",
    "    r2: float\n",
    "    mape: float\n",
    "    \n",
    "    def display(self):\n",
    "        print(\"=\" * 50)\n",
    "        print(\"Regression Metrics Report\")\n",
    "        print(\"=\" * 50)\n",
    "        for name, value in [(\"MSE\", self.mse), (\"RMSE\", self.rmse),\n",
    "                            (\"MAE\", self.mae), (\"R-squared\", self.r2),\n",
    "                            (\"MAPE\", self.mape)]:\n",
    "            print(f\"{name:12}: {value:.4f}\")\n",
    "\n",
    "def compute_regression_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mask = y_true != 0\n",
    "    mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100 if mask.any() else np.inf\n",
    "    \n",
    "    return RegressionMetrics(\n",
    "        mse=mse,\n",
    "        rmse=np.sqrt(mse),\n",
    "        mae=mean_absolute_error(y_true, y_pred),\n",
    "        r2=r2_score(y_true, y_pred),\n",
    "        mape=mape\n",
    "    )\n",
    "\n",
    "reg_metrics = compute_regression_metrics(y_test_reg, y_pred_reg)\n",
    "reg_metrics.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_regression_performance(y_true, y_pred):\n",
    "    \"\"\"Create comprehensive regression visualization.\"\"\"\n",
    "    residuals = y_true - y_pred\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    \n",
    "    # Actual vs Predicted\n",
    "    axes[0, 0].scatter(y_true, y_pred, alpha=0.5)\n",
    "    min_val, max_val = min(y_true.min(), y_pred.min()), max(y_true.max(), y_pred.max())\n",
    "    axes[0, 0].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Actual')\n",
    "    axes[0, 0].set_ylabel('Predicted')\n",
    "    axes[0, 0].set_title('Actual vs Predicted')\n",
    "    \n",
    "    # Residual Distribution\n",
    "    axes[0, 1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[0, 1].axvline(x=0, color='r', linestyle='--')\n",
    "    axes[0, 1].set_xlabel('Residual')\n",
    "    axes[0, 1].set_title('Residual Distribution')\n",
    "    \n",
    "    # Residuals vs Predicted\n",
    "    axes[1, 0].scatter(y_pred, residuals, alpha=0.5)\n",
    "    axes[1, 0].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[1, 0].set_xlabel('Predicted')\n",
    "    axes[1, 0].set_ylabel('Residuals')\n",
    "    axes[1, 0].set_title('Residuals vs Predicted')\n",
    "    \n",
    "    # Error Percentiles\n",
    "    abs_errors = np.abs(residuals)\n",
    "    percentiles = np.arange(0, 101, 5)\n",
    "    error_pcts = np.percentile(abs_errors, percentiles)\n",
    "    axes[1, 1].plot(percentiles, error_pcts, 'b-o')\n",
    "    axes[1, 1].set_xlabel('Percentile')\n",
    "    axes[1, 1].set_ylabel('Absolute Error')\n",
    "    axes[1, 1].set_title('Error Distribution by Percentile')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_regression_performance(y_test_reg, y_pred_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Ranking Metrics\n",
    "\n",
    "Essential for search engines, recommendation systems, and information retrieval.\n",
    "\n",
    "### 4.1 Precision@K and Recall@K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(relevance, k):\n",
    "    \"\"\"Calculate Precision@K.\"\"\"\n",
    "    if k <= 0:\n",
    "        return 0.0\n",
    "    return np.sum(np.asarray(relevance)[:k]) / k\n",
    "\n",
    "def recall_at_k(relevance, k, total_relevant):\n",
    "    \"\"\"Calculate Recall@K.\"\"\"\n",
    "    if total_relevant == 0:\n",
    "        return 0.0\n",
    "    return np.sum(np.asarray(relevance)[:k]) / total_relevant\n",
    "\n",
    "# Example: Recommendation system\n",
    "recommended = np.array([1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0])\n",
    "total_relevant = 10\n",
    "\n",
    "print(\"Precision@K and Recall@K Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total relevant items: {total_relevant}\")\n",
    "print(f\"Recommendations: {recommended}\\n\")\n",
    "\n",
    "for k in [1, 3, 5, 10, 20]:\n",
    "    p = precision_at_k(recommended, k)\n",
    "    r = recall_at_k(recommended, k, total_relevant)\n",
    "    print(f\"K={k:2d} | P@K: {p:.4f} | R@K: {r:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Mean Reciprocal Rank (MRR)\n",
    "\n",
    "Measures how quickly relevant items appear in the ranked list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank(relevance):\n",
    "    \"\"\"Calculate reciprocal rank.\"\"\"\n",
    "    relevance = np.asarray(relevance)\n",
    "    positions = np.where(relevance == 1)[0]\n",
    "    if len(positions) == 0:\n",
    "        return 0.0\n",
    "    return 1.0 / (positions[0] + 1)\n",
    "\n",
    "def mean_reciprocal_rank(rankings):\n",
    "    \"\"\"Calculate MRR across multiple queries.\"\"\"\n",
    "    return np.mean([reciprocal_rank(r) for r in rankings])\n",
    "\n",
    "# Example: Search results for 5 queries\n",
    "query_results = [\n",
    "    np.array([1, 0, 0, 0, 0]),  # RR = 1.0\n",
    "    np.array([0, 0, 1, 0, 0]),  # RR = 0.33\n",
    "    np.array([0, 1, 0, 0, 0]),  # RR = 0.5\n",
    "    np.array([0, 0, 0, 0, 1]),  # RR = 0.2\n",
    "    np.array([0, 0, 0, 0, 0]),  # RR = 0\n",
    "]\n",
    "\n",
    "print(\"Mean Reciprocal Rank (MRR)\")\n",
    "print(\"=\" * 50)\n",
    "for i, results in enumerate(query_results):\n",
    "    rr = reciprocal_rank(results)\n",
    "    print(f\"Query {i+1}: {results} -> RR = {rr:.4f}\")\n",
    "\n",
    "mrr = mean_reciprocal_rank(query_results)\n",
    "print(f\"\\nMRR = {mrr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Mean Average Precision (mAP)\n",
    "\n",
    "Considers precision at each relevant item position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision(relevance):\n",
    "    \"\"\"Calculate Average Precision for a single query.\"\"\"\n",
    "    relevance = np.asarray(relevance)\n",
    "    n_relevant = np.sum(relevance)\n",
    "    if n_relevant == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    precisions = []\n",
    "    n_relevant_seen = 0\n",
    "    for i, rel in enumerate(relevance):\n",
    "        if rel == 1:\n",
    "            n_relevant_seen += 1\n",
    "            precisions.append(n_relevant_seen / (i + 1))\n",
    "    \n",
    "    return np.mean(precisions)\n",
    "\n",
    "def mean_average_precision(rankings):\n",
    "    \"\"\"Calculate mAP across multiple queries.\"\"\"\n",
    "    return np.mean([average_precision(r) for r in rankings])\n",
    "\n",
    "# Example\n",
    "rankings = [\n",
    "    np.array([1, 1, 0, 1, 0, 0, 0, 1, 0, 0]),  # AP = 0.69\n",
    "    np.array([1, 0, 0, 1, 0, 0, 0, 0, 0, 1]),  # AP = 0.57\n",
    "    np.array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0]),  # AP = 0.25\n",
    "]\n",
    "\n",
    "print(\"Mean Average Precision (mAP)\")\n",
    "print(\"=\" * 50)\n",
    "for i, ranking in enumerate(rankings):\n",
    "    ap = average_precision(ranking)\n",
    "    print(f\"Query {i+1}: AP = {ap:.4f}\")\n",
    "\n",
    "map_score = mean_average_precision(rankings)\n",
    "print(f\"\\nmAP = {map_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Normalized Discounted Cumulative Gain (nDCG)\n",
    "\n",
    "Considers graded relevance scores and position-based discounting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(relevance, k):\n",
    "    \"\"\"Calculate DCG@K.\"\"\"\n",
    "    relevance = np.asarray(relevance)[:k]\n",
    "    positions = np.arange(1, len(relevance) + 1)\n",
    "    discounts = np.log2(positions + 1)\n",
    "    return np.sum(relevance / discounts)\n",
    "\n",
    "def ndcg_at_k(relevance, k):\n",
    "    \"\"\"Calculate nDCG@K.\"\"\"\n",
    "    dcg = dcg_at_k(relevance, k)\n",
    "    ideal_relevance = np.sort(relevance)[::-1]\n",
    "    idcg = dcg_at_k(ideal_relevance, k)\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "# Example with graded relevance (0=not relevant, 1=somewhat, 2=relevant, 3=highly)\n",
    "graded_relevance = np.array([3, 2, 0, 1, 2, 0, 0, 1, 0, 0])\n",
    "\n",
    "print(\"nDCG@K Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Graded relevance: {graded_relevance}\\n\")\n",
    "\n",
    "for k in [1, 3, 5, 10]:\n",
    "    dcg = dcg_at_k(graded_relevance, k)\n",
    "    ndcg = ndcg_at_k(graded_relevance, k)\n",
    "    print(f\"K={k:2d} | DCG@K: {dcg:.4f} | nDCG@K: {ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. NLP Metrics\n",
    "\n",
    "### 5.1 BLEU Score\n",
    "\n",
    "Measures n-gram overlap between generated and reference text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def get_ngrams(tokens, n):\n",
    "    \"\"\"Extract n-grams from token list.\"\"\"\n",
    "    return [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
    "\n",
    "def compute_bleu(reference, candidate, max_n=4):\n",
    "    \"\"\"Compute BLEU score.\"\"\"\n",
    "    ref_tokens = reference.lower().split()\n",
    "    cand_tokens = candidate.lower().split()\n",
    "    \n",
    "    if len(cand_tokens) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Brevity penalty\n",
    "    bp = min(1.0, math.exp(1 - len(ref_tokens) / len(cand_tokens)))\n",
    "    \n",
    "    # N-gram precisions\n",
    "    precisions = []\n",
    "    for n in range(1, max_n + 1):\n",
    "        ref_ngrams = Counter(get_ngrams(ref_tokens, n))\n",
    "        cand_ngrams = Counter(get_ngrams(cand_tokens, n))\n",
    "        \n",
    "        matches = sum((cand_ngrams & ref_ngrams).values())\n",
    "        total = sum(cand_ngrams.values())\n",
    "        \n",
    "        if total > 0:\n",
    "            precisions.append(matches / total)\n",
    "        else:\n",
    "            precisions.append(0.0)\n",
    "    \n",
    "    # Geometric mean with smoothing\n",
    "    if all(p > 0 for p in precisions):\n",
    "        geo_mean = math.exp(sum(math.log(p) for p in precisions) / len(precisions))\n",
    "    else:\n",
    "        geo_mean = 0.0\n",
    "    \n",
    "    return bp * geo_mean\n",
    "\n",
    "# Example\n",
    "reference = \"The quick brown fox jumps over the lazy dog\"\n",
    "candidates = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",  # Perfect match\n",
    "    \"The fast brown fox jumps over the lazy dog\",   # One word different\n",
    "    \"A quick brown fox jumped over a lazy dog\",     # More differences\n",
    "    \"The dog is lazy\",                               # Very different\n",
    "]\n",
    "\n",
    "print(\"BLEU Score Examples\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Reference: {reference}\\n\")\n",
    "\n",
    "for cand in candidates:\n",
    "    bleu = compute_bleu(reference, cand)\n",
    "    print(f\"BLEU: {bleu:.4f} | {cand}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 ROUGE Score\n",
    "\n",
    "Recall-oriented metric for text summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rouge_n(reference, candidate, n=1):\n",
    "    \"\"\"Compute ROUGE-N score.\"\"\"\n",
    "    ref_tokens = reference.lower().split()\n",
    "    cand_tokens = candidate.lower().split()\n",
    "    \n",
    "    ref_ngrams = Counter(get_ngrams(ref_tokens, n))\n",
    "    cand_ngrams = Counter(get_ngrams(cand_tokens, n))\n",
    "    \n",
    "    matches = sum((ref_ngrams & cand_ngrams).values())\n",
    "    ref_total = sum(ref_ngrams.values())\n",
    "    cand_total = sum(cand_ngrams.values())\n",
    "    \n",
    "    recall = matches / ref_total if ref_total > 0 else 0.0\n",
    "    precision = matches / cand_total if cand_total > 0 else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return {'precision': precision, 'recall': recall, 'f1': f1}\n",
    "\n",
    "def compute_rouge_l(reference, candidate):\n",
    "    \"\"\"Compute ROUGE-L using LCS.\"\"\"\n",
    "    ref_tokens = reference.lower().split()\n",
    "    cand_tokens = candidate.lower().split()\n",
    "    \n",
    "    # Compute LCS length\n",
    "    m, n = len(ref_tokens), len(cand_tokens)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if ref_tokens[i-1] == cand_tokens[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1] + 1\n",
    "            else:\n",
    "                dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
    "    lcs_len = dp[m][n]\n",
    "    \n",
    "    recall = lcs_len / m if m > 0 else 0.0\n",
    "    precision = lcs_len / n if n > 0 else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return {'precision': precision, 'recall': recall, 'f1': f1}\n",
    "\n",
    "# Example\n",
    "reference = \"The cat sat on the mat\"\n",
    "candidate = \"The cat was sitting on the mat\"\n",
    "\n",
    "print(\"ROUGE Scores\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Reference: {reference}\")\n",
    "print(f\"Candidate: {candidate}\\n\")\n",
    "\n",
    "for n in [1, 2]:\n",
    "    scores = compute_rouge_n(reference, candidate, n)\n",
    "    print(f\"ROUGE-{n}: P={scores['precision']:.4f} R={scores['recall']:.4f} F1={scores['f1']:.4f}\")\n",
    "\n",
    "scores_l = compute_rouge_l(reference, candidate)\n",
    "print(f\"ROUGE-L: P={scores_l['precision']:.4f} R={scores_l['recall']:.4f} F1={scores_l['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Metric Selection Guidelines\n",
    "\n",
    "### Decision Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricSelector:\n",
    "    \"\"\"Guide for selecting appropriate evaluation metrics.\"\"\"\n",
    "    \n",
    "    RECOMMENDATIONS = {\n",
    "        'binary_classification': {\n",
    "            'balanced': ['ROC-AUC', 'F1-Score', 'Accuracy'],\n",
    "            'imbalanced': ['PR-AUC', 'F1-Score', 'Precision/Recall'],\n",
    "            'fp_costly': ['Precision', 'Specificity'],\n",
    "            'fn_costly': ['Recall', 'Sensitivity'],\n",
    "        },\n",
    "        'multiclass_classification': {\n",
    "            'balanced': ['Macro F1', 'Accuracy'],\n",
    "            'imbalanced': ['Weighted F1', 'Macro F1'],\n",
    "        },\n",
    "        'regression': {\n",
    "            'general': ['RMSE', 'MAE', 'R-squared'],\n",
    "            'outlier_sensitive': ['RMSE', 'MSE'],\n",
    "            'outlier_robust': ['MAE', 'Median AE'],\n",
    "            'scale_independent': ['MAPE', 'R-squared'],\n",
    "        },\n",
    "        'ranking': {\n",
    "            'top_k_matters': ['Precision@K', 'Recall@K', 'nDCG@K'],\n",
    "            'first_result_matters': ['MRR'],\n",
    "            'overall_ranking': ['mAP', 'nDCG'],\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def recommend(cls, task_type, scenario):\n",
    "        \"\"\"Get metric recommendations.\"\"\"\n",
    "        if task_type in cls.RECOMMENDATIONS:\n",
    "            task_recs = cls.RECOMMENDATIONS[task_type]\n",
    "            if scenario in task_recs:\n",
    "                return task_recs[scenario]\n",
    "        return ['Please specify valid task and scenario']\n",
    "\n",
    "# Display recommendations\n",
    "print(\"Metric Selection Guide\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "examples = [\n",
    "    ('binary_classification', 'imbalanced', 'Fraud Detection'),\n",
    "    ('binary_classification', 'fn_costly', 'Medical Diagnosis'),\n",
    "    ('regression', 'outlier_robust', 'House Price Prediction'),\n",
    "    ('ranking', 'top_k_matters', 'Search Engine'),\n",
    "]\n",
    "\n",
    "for task, scenario, use_case in examples:\n",
    "    metrics = MetricSelector.recommend(task, scenario)\n",
    "    print(f\"\\n{use_case}:\")\n",
    "    print(f\"  Task: {task}, Scenario: {scenario}\")\n",
    "    print(f\"  Recommended: {', '.join(metrics)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Hands-on Exercises\n",
    "\n",
    "### Exercise 1: Comprehensive Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_evaluation(y_true, y_pred, y_proba=None):\n",
    "    \"\"\"Perform comprehensive model evaluation.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Comprehensive Model Evaluation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Basic metrics\n",
    "    print(\"\\n1. Basic Classification Metrics:\")\n",
    "    print(f\"   Accuracy:  {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"   Precision: {precision_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"   Recall:    {recall_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"   F1-Score:  {f1_score(y_true, y_pred):.4f}\")\n",
    "    \n",
    "    if y_proba is not None:\n",
    "        print(\"\\n2. Probability-Based Metrics:\")\n",
    "        print(f\"   ROC-AUC:   {roc_auc_score(y_true, y_proba):.4f}\")\n",
    "        print(f\"   PR-AUC:    {average_precision_score(y_true, y_proba):.4f}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    print(\"\\n3. Confusion Matrix:\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(f\"   TN: {cm[0,0]:4d} | FP: {cm[0,1]:4d}\")\n",
    "    print(f\"   FN: {cm[1,0]:4d} | TP: {cm[1,1]:4d}\")\n",
    "\n",
    "# Run comprehensive evaluation\n",
    "comprehensive_evaluation(y_test_clf, y_pred, y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Compare Different Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_analysis(y_true, y_proba, thresholds=[0.3, 0.5, 0.7]):\n",
    "    \"\"\"Analyze model performance at different thresholds.\"\"\"\n",
    "    print(\"Threshold Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = []\n",
    "    for thresh in thresholds:\n",
    "        y_pred = (y_proba >= thresh).astype(int)\n",
    "        results.append({\n",
    "            'Threshold': thresh,\n",
    "            'Precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "            'Recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "            'F1': f1_score(y_true, y_pred, zero_division=0),\n",
    "            'Positives': y_pred.sum()\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    print(df.to_string(index=False))\n",
    "    return df\n",
    "\n",
    "threshold_analysis(y_test_clf, y_pred_proba, [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Classification Metrics**\n",
    "   - Use precision when FP is costly, recall when FN is costly\n",
    "   - F1-score balances precision and recall\n",
    "   - ROC-AUC for balanced data, PR-AUC for imbalanced data\n",
    "\n",
    "2. **Regression Metrics**\n",
    "   - RMSE penalizes large errors; MAE is robust to outliers\n",
    "   - R-squared shows variance explained\n",
    "   - MAPE provides scale-independent comparison\n",
    "\n",
    "3. **Ranking Metrics**\n",
    "   - Precision@K for top-K recommendation quality\n",
    "   - MRR for first-result importance\n",
    "   - nDCG for graded relevance with position discounting\n",
    "\n",
    "4. **Best Practices**\n",
    "   - Always choose metrics aligned with business objectives\n",
    "   - Use multiple metrics for comprehensive evaluation\n",
    "   - Consider class imbalance when selecting metrics\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next tutorial, we'll cover **Online Evaluation and A/B Testing** to learn how to validate models in production environments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 17: Monitoring ML Systems\n",
    "\n",
    "## Module 7: Monitoring and Infrastructure\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Understand why ML systems fail in production\n",
    "2. Implement operational metrics monitoring (latency, throughput, errors)\n",
    "3. Track ML-specific metrics (prediction distribution, feature statistics)\n",
    "4. Build logging and alerting systems\n",
    "5. Design monitoring dashboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from collections import deque\n",
    "import json\n",
    "import random\n",
    "from typing import Dict, List, Any, Optional\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "print('Setup complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why ML Systems Fail\n",
    "\n",
    "Common failure modes include:\n",
    "- **Data Issues**: Drift, missing data, schema changes\n",
    "- **Model Issues**: Staleness, concept drift, bias\n",
    "- **Infrastructure**: Latency, OOM errors, network issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FailureMode(Enum):\n",
    "    DATA_DRIFT = 'data_drift'\n",
    "    CONCEPT_DRIFT = 'concept_drift'\n",
    "    MODEL_STALENESS = 'model_staleness'\n",
    "    DATA_QUALITY = 'data_quality'\n",
    "    INFRASTRUCTURE = 'infrastructure'\n",
    "\n",
    "@dataclass\n",
    "class FailureModeInfo:\n",
    "    name: str\n",
    "    description: str\n",
    "    detection: str\n",
    "    mitigation: str\n",
    "    severity: str\n",
    "\n",
    "FAILURES = {\n",
    "    FailureMode.DATA_DRIFT: FailureModeInfo(\n",
    "        'Data Drift', 'Input distribution changes', 'Statistical tests', 'Retrain model', 'high'),\n",
    "    FailureMode.CONCEPT_DRIFT: FailureModeInfo(\n",
    "        'Concept Drift', 'Feature-target relationship changes', 'Monitor accuracy', 'Retrain', 'critical'),\n",
    "    FailureMode.MODEL_STALENESS: FailureModeInfo(\n",
    "        'Model Staleness', 'Performance degrades over time', 'Track model age', 'Schedule retraining', 'medium'),\n",
    "    FailureMode.DATA_QUALITY: FailureModeInfo(\n",
    "        'Data Quality', 'Missing/corrupted data', 'Schema validation', 'Data validation pipeline', 'high'),\n",
    "    FailureMode.INFRASTRUCTURE: FailureModeInfo(\n",
    "        'Infrastructure', 'Resource exhaustion', 'Health checks', 'Auto-scaling', 'critical')\n",
    "}\n",
    "\n",
    "print('ML SYSTEM FAILURE MODES')\n",
    "print('=' * 60)\n",
    "for mode, info in FAILURES.items():\n",
    "    print(f'\\n[{info.severity.upper()}] {info.name}: {info.description}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize failure impact\n",
    "scenarios = ['Data Drift', 'Infrastructure', 'Staleness', 'Data Quality']\n",
    "revenue_loss = [150000, 80000, 200000, 50000]\n",
    "recovery_hours = [48, 4, 24, 8]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].barh(scenarios, revenue_loss, color=plt.cm.Reds(np.linspace(0.4, 0.8, 4)))\n",
    "axes[0].set_xlabel('Revenue Loss ($)')\n",
    "axes[0].set_title('Estimated Revenue Impact')\n",
    "axes[1].barh(scenarios, recovery_hours, color=plt.cm.Blues(np.linspace(0.4, 0.8, 4)))\n",
    "axes[1].set_xlabel('Hours')\n",
    "axes[1].set_title('Time to Recovery')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f'Total potential loss: ${sum(revenue_loss):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Operational Metrics Monitoring\n",
    "\n",
    "Key metrics: Latency (P50, P95, P99), Throughput (QPS), Error Rate, Resource Utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RequestMetrics:\n",
    "    timestamp: datetime\n",
    "    latency_ms: float\n",
    "    success: bool\n",
    "    cpu_util: float\n",
    "    memory_util: float\n",
    "\n",
    "class OperationalMetricsCollector:\n",
    "    def __init__(self, window_size: int = 1000):\n",
    "        self.metrics: deque = deque(maxlen=window_size)\n",
    "    \n",
    "    def record(self, latency_ms, success, cpu_util, memory_util):\n",
    "        self.metrics.append(RequestMetrics(datetime.now(), latency_ms, success, cpu_util, memory_util))\n",
    "    \n",
    "    def compute_stats(self) -> Dict[str, float]:\n",
    "        if not self.metrics:\n",
    "            return {}\n",
    "        latencies = [m.latency_ms for m in self.metrics]\n",
    "        return {\n",
    "            'latency_mean': np.mean(latencies),\n",
    "            'latency_p50': np.percentile(latencies, 50),\n",
    "            'latency_p95': np.percentile(latencies, 95),\n",
    "            'latency_p99': np.percentile(latencies, 99),\n",
    "            'error_rate': 1 - np.mean([m.success for m in self.metrics]),\n",
    "            'cpu_mean': np.mean([m.cpu_util for m in self.metrics]),\n",
    "            'memory_mean': np.mean([m.memory_util for m in self.metrics]),\n",
    "            'count': len(self.metrics)\n",
    "        }\n",
    "\n",
    "collector = OperationalMetricsCollector()\n",
    "print('OperationalMetricsCollector created!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate production traffic\n",
    "for _ in range(1000):\n",
    "    latency = 50 * random.uniform(5, 20) if random.random() < 0.05 else np.random.lognormal(np.log(50), 0.5)\n",
    "    collector.record(latency, random.random() > 0.02, np.random.beta(2, 5) * 100, np.clip(40 + np.random.normal(0, 10), 0, 100))\n",
    "\n",
    "stats = collector.compute_stats()\n",
    "print('Operational Metrics Summary')\n",
    "print('=' * 40)\n",
    "print(f'Latency P50: {stats[\"latency_p50\"]:.1f}ms')\n",
    "print(f'Latency P99: {stats[\"latency_p99\"]:.1f}ms')\n",
    "print(f'Error Rate: {stats[\"error_rate\"]*100:.2f}%')\n",
    "print(f'CPU: {stats[\"cpu_mean\"]:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize operational metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "metrics_list = list(collector.metrics)\n",
    "latencies = [m.latency_ms for m in metrics_list]\n",
    "\n",
    "ax = axes[0, 0]\n",
    "ax.hist(latencies, bins=50, color='steelblue', alpha=0.7)\n",
    "for p, c, l in [(50, 'green', 'P50'), (95, 'orange', 'P95'), (99, 'red', 'P99')]:\n",
    "    ax.axvline(np.percentile(latencies, p), color=c, linestyle='--', label=f'{l}: {np.percentile(latencies, p):.0f}ms')\n",
    "ax.set_xlabel('Latency (ms)')\n",
    "ax.set_title('Latency Distribution')\n",
    "ax.legend()\n",
    "ax.set_xlim(0, np.percentile(latencies, 99.5))\n",
    "\n",
    "ax = axes[0, 1]\n",
    "cpu = pd.Series([m.cpu_util for m in metrics_list]).rolling(50).mean()\n",
    "mem = pd.Series([m.memory_util for m in metrics_list]).rolling(50).mean()\n",
    "ax.plot(cpu, label='CPU', color='blue')\n",
    "ax.plot(mem, label='Memory', color='purple')\n",
    "ax.axhline(80, color='red', linestyle='--', label='Warning')\n",
    "ax.set_title('Resource Utilization')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "ax = axes[1, 0]\n",
    "success_rate = pd.Series([m.success for m in metrics_list]).rolling(100).mean() * 100\n",
    "ax.plot(success_rate, color='green')\n",
    "ax.axhline(99, color='orange', linestyle='--', label='SLA (99%)')\n",
    "ax.set_title('Success Rate')\n",
    "ax.set_ylim(95, 100.5)\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[1, 1]\n",
    "lat_ma = pd.Series(latencies).rolling(50).mean()\n",
    "ax.plot(lat_ma, color='blue')\n",
    "ax.axhline(500, color='red', linestyle='--', label='SLA Limit')\n",
    "ax.set_title('Latency Trend')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Operational Dashboard', y=1.02, fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ML-Specific Metrics\n",
    "\n",
    "ML systems need specialized monitoring for prediction distribution, feature statistics, and model confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Prediction:\n",
    "    timestamp: datetime\n",
    "    prediction: float\n",
    "    confidence: float\n",
    "    features: Dict[str, float]\n",
    "\n",
    "class MLMetricsCollector:\n",
    "    def __init__(self, window_size: int = 5000):\n",
    "        self.predictions: deque = deque(maxlen=window_size)\n",
    "        self.reference_stats: Optional[Dict] = None\n",
    "    \n",
    "    def set_reference(self, stats: Dict):\n",
    "        self.reference_stats = stats\n",
    "    \n",
    "    def record(self, pred: Prediction):\n",
    "        self.predictions.append(pred)\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, float]:\n",
    "        if not self.predictions:\n",
    "            return {}\n",
    "        preds = [p.prediction for p in self.predictions]\n",
    "        confs = [p.confidence for p in self.predictions]\n",
    "        return {\n",
    "            'pred_mean': np.mean(preds),\n",
    "            'positive_rate': np.mean([p >= 0.5 for p in preds]),\n",
    "            'conf_mean': np.mean(confs),\n",
    "            'low_conf_rate': np.mean([c < 0.5 for c in confs])\n",
    "        }\n",
    "\n",
    "ml_collector = MLMetricsCollector()\n",
    "print('MLMetricsCollector created!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate predictions with drift at index 1500\n",
    "ref = {'age': {'mean': 35, 'std': 10}, 'income': {'mean': 50000, 'std': 20000}, 'amount': {'mean': 100, 'std': 50}}\n",
    "ml_collector.set_reference(ref)\n",
    "\n",
    "for i in range(2000):\n",
    "    is_drift = i >= 1500\n",
    "    features = {k: np.random.normal(v['mean'] + (v['mean']*0.2 if is_drift else 0), v['std']) for k, v in ref.items()}\n",
    "    prob = np.clip((0.15 if is_drift else 0.1) + (features['amount'] - 100) / 500, 0.01, 0.99)\n",
    "    prediction = 1.0 if random.random() < prob else 0.0\n",
    "    confidence = np.clip(prob + np.random.normal(0, 0.1), 0.1, 0.99)\n",
    "    ml_collector.record(Prediction(datetime.now(), prediction, confidence, features))\n",
    "\n",
    "ml_stats = ml_collector.get_stats()\n",
    "print('ML Metrics Summary')\n",
    "print('=' * 40)\n",
    "print(f'Positive Rate: {ml_stats[\"positive_rate\"]*100:.1f}%')\n",
    "print(f'Avg Confidence: {ml_stats[\"conf_mean\"]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ML metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "preds = list(ml_collector.predictions)\n",
    "predictions = [p.prediction for p in preds]\n",
    "confidences = [p.confidence for p in preds]\n",
    "\n",
    "ax = axes[0, 0]\n",
    "rolling = pd.Series(predictions).rolling(100).mean()\n",
    "ax.plot(rolling, color='blue')\n",
    "ax.axhline(0.1, color='green', linestyle='--', label='Expected (10%)')\n",
    "ax.axvline(1500, color='red', linestyle='--', label='Drift Start')\n",
    "ax.set_title('Prediction Rate Over Time')\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[0, 1]\n",
    "ax.hist(confidences, bins=50, color='purple', alpha=0.7)\n",
    "ax.axvline(0.5, color='red', linestyle='--', label='Threshold')\n",
    "ax.set_title('Confidence Distribution')\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[1, 0]\n",
    "before = [p.features['amount'] for p in preds[:1500]]\n",
    "after = [p.features['amount'] for p in preds[1500:]]\n",
    "ax.hist(before, bins=40, alpha=0.5, label='Before Drift', color='blue')\n",
    "ax.hist(after, bins=40, alpha=0.5, label='After Drift', color='red')\n",
    "ax.axvline(100, color='green', linestyle='--', label='Reference')\n",
    "ax.set_title('Feature: amount')\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[1, 1]\n",
    "conf_rolling = pd.Series(confidences).rolling(100).mean()\n",
    "ax.plot(conf_rolling, color='orange')\n",
    "ax.axvline(1500, color='red', linestyle='--', label='Drift Start')\n",
    "ax.set_title('Confidence Trend')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('ML Metrics Dashboard', y=1.02, fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Logging Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogLevel(Enum):\n",
    "    DEBUG = 'DEBUG'\n",
    "    INFO = 'INFO'\n",
    "    WARNING = 'WARNING'\n",
    "    ERROR = 'ERROR'\n",
    "    CRITICAL = 'CRITICAL'\n",
    "\n",
    "@dataclass\n",
    "class LogEntry:\n",
    "    timestamp: datetime\n",
    "    level: LogLevel\n",
    "    component: str\n",
    "    message: str\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "class MLLogger:\n",
    "    def __init__(self, component: str):\n",
    "        self.component = component\n",
    "        self.entries: List[LogEntry] = []\n",
    "        self._levels = {LogLevel.DEBUG: 0, LogLevel.INFO: 1, LogLevel.WARNING: 2, LogLevel.ERROR: 3, LogLevel.CRITICAL: 4}\n",
    "        self.min_level = LogLevel.INFO\n",
    "    \n",
    "    def _log(self, level, message, metadata=None):\n",
    "        if self._levels[level] >= self._levels[self.min_level]:\n",
    "            self.entries.append(LogEntry(datetime.now(), level, self.component, message, metadata or {}))\n",
    "    \n",
    "    def info(self, msg, **kw): self._log(LogLevel.INFO, msg, kw.get('metadata'))\n",
    "    def warning(self, msg, **kw): self._log(LogLevel.WARNING, msg, kw.get('metadata'))\n",
    "    def error(self, msg, **kw): self._log(LogLevel.ERROR, msg, kw.get('metadata'))\n",
    "\n",
    "logger = MLLogger('inference')\n",
    "logger.info('Model loaded', metadata={'size': 125})\n",
    "logger.warning('High latency', metadata={'latency': 450})\n",
    "logger.error('Feature missing', metadata={'feature': 'embedding'})\n",
    "\n",
    "print('Log Entries:')\n",
    "for e in logger.entries:\n",
    "    print(f'[{e.level.value}] {e.message} - {e.metadata}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Alerting Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlertSeverity(Enum):\n",
    "    INFO = 'info'\n",
    "    WARNING = 'warning'\n",
    "    CRITICAL = 'critical'\n",
    "\n",
    "@dataclass\n",
    "class AlertRule:\n",
    "    name: str\n",
    "    metric: str\n",
    "    condition: str\n",
    "    threshold: float\n",
    "    severity: AlertSeverity\n",
    "\n",
    "@dataclass\n",
    "class Alert:\n",
    "    timestamp: datetime\n",
    "    rule_name: str\n",
    "    value: float\n",
    "    severity: AlertSeverity\n",
    "\n",
    "class AlertingSystem:\n",
    "    def __init__(self):\n",
    "        self.rules: List[AlertRule] = []\n",
    "        self.alerts: List[Alert] = []\n",
    "    \n",
    "    def add_rule(self, rule: AlertRule):\n",
    "        self.rules.append(rule)\n",
    "    \n",
    "    def check(self, metrics: Dict[str, float]) -> List[Alert]:\n",
    "        new_alerts = []\n",
    "        for rule in self.rules:\n",
    "            if rule.metric not in metrics:\n",
    "                continue\n",
    "            value = metrics[rule.metric]\n",
    "            triggered = (rule.condition == 'gt' and value > rule.threshold) or (rule.condition == 'lt' and value < rule.threshold)\n",
    "            if triggered:\n",
    "                alert = Alert(datetime.now(), rule.name, value, rule.severity)\n",
    "                self.alerts.append(alert)\n",
    "                new_alerts.append(alert)\n",
    "        return new_alerts\n",
    "\n",
    "alerting = AlertingSystem()\n",
    "alerting.add_rule(AlertRule('High Latency', 'latency_p99', 'gt', 500, AlertSeverity.WARNING))\n",
    "alerting.add_rule(AlertRule('High Error Rate', 'error_rate', 'gt', 0.05, AlertSeverity.CRITICAL))\n",
    "alerting.add_rule(AlertRule('Prediction Drift', 'positive_rate', 'gt', 0.15, AlertSeverity.WARNING))\n",
    "print(f'Configured {len(alerting.rules)} alert rules')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test alerting\n",
    "for i in range(5):\n",
    "    is_anomaly = random.random() < 0.4\n",
    "    metrics = {\n",
    "        'latency_p99': 300 + (400 if is_anomaly else 50),\n",
    "        'error_rate': 0.02 + (0.08 if is_anomaly else 0.01),\n",
    "        'positive_rate': 0.1 + (0.1 if is_anomaly else 0.02)\n",
    "    }\n",
    "    alerts = alerting.check(metrics)\n",
    "    if alerts:\n",
    "        print(f'Iteration {i+1}: {len(alerts)} alerts')\n",
    "        for a in alerts:\n",
    "            print(f'  [{a.severity.value.upper()}] {a.rule_name}: {a.value:.2f}')\n",
    "\n",
    "print(f'\\nTotal alerts: {len(alerting.alerts)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Monitoring Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonitoringDashboard:\n",
    "    def __init__(self, ops_collector, ml_collector, alerting):\n",
    "        self.ops = ops_collector\n",
    "        self.ml = ml_collector\n",
    "        self.alerting = alerting\n",
    "    \n",
    "    def report(self) -> str:\n",
    "        ops_stats = self.ops.compute_stats()\n",
    "        ml_stats = self.ml.get_stats()\n",
    "        lines = ['=' * 60, 'ML SYSTEM MONITORING REPORT', '=' * 60]\n",
    "        lines.append(f'\\nOperational: P50={ops_stats.get(\"latency_p50\",0):.0f}ms, P99={ops_stats.get(\"latency_p99\",0):.0f}ms, Error={ops_stats.get(\"error_rate\",0)*100:.1f}%')\n",
    "        lines.append(f'ML: Positive={ml_stats.get(\"positive_rate\",0)*100:.1f}%, Confidence={ml_stats.get(\"conf_mean\",0):.2f}')\n",
    "        lines.append(f'Alerts: {len(self.alerting.alerts)} total')\n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "dashboard = MonitoringDashboard(collector, ml_collector, alerting)\n",
    "print(dashboard.report())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hands-on Exercise\n",
    "\n",
    "**Task**: Build a monitoring system for a recommendation engine.\n",
    "\n",
    "1. Create collectors for latency, error rate, and click-through rate\n",
    "2. Set up alerts for degraded CTR and high latency\n",
    "3. Simulate 1000 requests with occasional degradation\n",
    "4. Generate a monitoring report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Implement your recommendation engine monitoring here\n",
    "\n",
    "class RecommendationMonitor:\n",
    "    def __init__(self):\n",
    "        self.requests = []\n",
    "        self.ctr_history = []\n",
    "    \n",
    "    def record_request(self, latency_ms, shown, clicked):\n",
    "        self.requests.append({'latency': latency_ms, 'shown': shown, 'clicked': clicked})\n",
    "        if len(self.requests) >= 100:\n",
    "            recent = self.requests[-100:]\n",
    "            ctr = sum(r['clicked'] for r in recent) / sum(r['shown'] for r in recent)\n",
    "            self.ctr_history.append(ctr)\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        if not self.requests:\n",
    "            return {}\n",
    "        latencies = [r['latency'] for r in self.requests]\n",
    "        return {\n",
    "            'latency_p50': np.percentile(latencies, 50),\n",
    "            'latency_p99': np.percentile(latencies, 99),\n",
    "            'ctr': sum(r['clicked'] for r in self.requests) / sum(r['shown'] for r in self.requests)\n",
    "        }\n",
    "\n",
    "# Simulate\n",
    "rec_monitor = RecommendationMonitor()\n",
    "for i in range(1000):\n",
    "    degraded = i > 700\n",
    "    latency = np.random.lognormal(np.log(100 if degraded else 30), 0.5)\n",
    "    shown = random.randint(5, 10)\n",
    "    base_ctr = 0.05 if degraded else 0.15\n",
    "    clicked = sum(random.random() < base_ctr for _ in range(shown))\n",
    "    rec_monitor.record_request(latency, shown, clicked)\n",
    "\n",
    "metrics = rec_monitor.get_metrics()\n",
    "print('Recommendation Engine Metrics:')\n",
    "print(f'Latency P50: {metrics[\"latency_p50\"]:.1f}ms')\n",
    "print(f'Latency P99: {metrics[\"latency_p99\"]:.1f}ms')\n",
    "print(f'CTR: {metrics[\"ctr\"]*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Key Takeaways\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **ML System Failures**: Data drift, concept drift, model staleness, infrastructure issues\n",
    "2. **Operational Metrics**: Latency percentiles (P50, P95, P99), error rate, throughput, resource utilization\n",
    "3. **ML Metrics**: Prediction distribution, feature statistics, confidence scores\n",
    "4. **Logging**: Structured logs with component, level, metadata for debugging\n",
    "5. **Alerting**: Rules-based alerts with severity levels and cooldowns\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- Monitor both operational AND ML-specific metrics\n",
    "- Set up alerts BEFORE deploying to production\n",
    "- Use percentiles (not averages) for latency monitoring\n",
    "- Log predictions for debugging and retraining\n",
    "- Establish baselines from training data for drift detection\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next tutorial, we will dive deeper into **Data Drift and Model Degradation**, covering statistical tests for drift detection, retraining strategies, and automated monitoring pipelines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
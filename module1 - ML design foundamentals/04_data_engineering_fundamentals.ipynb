{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 04: Data Engineering Fundamentals\n",
    "\n",
    "## Module 3: Data Preparation\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "\n",
    "1. **Understand different data sources** and their characteristics in ML systems\n",
    "2. **Learn about various database types** (RDBMS, NoSQL, Data Warehouses, Data Lakes) and their use cases\n",
    "3. **Distinguish between structured and unstructured data** and know when to use each\n",
    "4. **Identify data types** relevant to ML (numerical, categorical) and handle them appropriately\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction to Data Engineering for ML](#1-introduction)\n",
    "2. [Data Sources](#2-data-sources)\n",
    "3. [Data Storage Types](#3-data-storage-types)\n",
    "4. [Structured vs Unstructured Data](#4-structured-vs-unstructured)\n",
    "5. [Data Types in ML](#5-data-types-in-ml)\n",
    "6. [Hands-on Exercise](#6-hands-on-exercise)\n",
    "7. [Summary and Key Takeaways](#7-summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Introduction to Data Engineering for ML <a id='1-introduction'></a>\n",
    "\n",
    "Data engineering is the backbone of any successful machine learning system. Before we can train models, we need to:\n",
    "\n",
    "- **Collect** data from various sources\n",
    "- **Store** it efficiently for access and processing\n",
    "- **Transform** it into formats suitable for ML algorithms\n",
    "- **Ensure quality** through validation and cleaning\n",
    "\n",
    "### Why Data Engineering Matters in ML\n",
    "\n",
    "**Key Insight**: In production ML systems, data engineers often spend 70-80% of their time on data-related tasks rather than model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Any, Optional\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Data Sources <a id='2-data-sources'></a>\n",
    "\n",
    "Understanding where your data comes from is crucial for building robust ML systems.\n",
    "\n",
    "### 2.1 User-Generated vs System-Generated Data\n",
    "\n",
    "| Type | Description | Examples | ML Use Cases |\n",
    "|------|-------------|----------|-------------|\n",
    "| **User-Generated** | Data created directly by user actions | Reviews, posts, clicks, ratings | Sentiment analysis, recommendations |\n",
    "| **System-Generated** | Data produced by automated systems | Logs, metrics, timestamps | Anomaly detection, forecasting |\n",
    "\n",
    "### 2.2 First-Party vs Third-Party Data\n",
    "\n",
    "| Type | Description | Advantages | Disadvantages |\n",
    "|------|-------------|------------|---------------|\n",
    "| **First-Party** | Collected directly from your users | High quality, owned | Limited scope |\n",
    "| **Third-Party** | Purchased or obtained from external sources | Broader coverage | Quality concerns, compliance issues |\n",
    "\n",
    "### 2.3 Real-Time vs Batch Data\n",
    "\n",
    "| Type | Latency | Use Cases | Processing |\n",
    "|------|---------|-----------|------------|\n",
    "| **Real-Time** | Milliseconds to seconds | Fraud detection, live recommendations | Stream processing |\n",
    "| **Batch** | Hours to days | Reporting, model training | Batch processing |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simulating different data sources for an e-commerce platform\n",
    "\n",
    "class DataSourceSimulator:\n",
    "    \"\"\"Simulates various data sources in an ML system.\"\"\"\n",
    "    \n",
    "    def __init__(self, seed: int = 42):\n",
    "        np.random.seed(seed)\n",
    "        self.users = self._generate_users(100)\n",
    "        self.products = self._generate_products(50)\n",
    "    \n",
    "    def _generate_users(self, n: int) -> pd.DataFrame:\n",
    "        return pd.DataFrame({\n",
    "            'user_id': range(1, n + 1),\n",
    "            'signup_date': pd.date_range('2023-01-01', periods=n, freq='D'),\n",
    "            'country': np.random.choice(['US', 'UK', 'DE', 'FR', 'JP'], n),\n",
    "            'age_group': np.random.choice(['18-24', '25-34', '35-44', '45-54', '55+'], n)\n",
    "        })\n",
    "    \n",
    "    def _generate_products(self, n: int) -> pd.DataFrame:\n",
    "        categories = ['Electronics', 'Clothing', 'Books', 'Home', 'Sports']\n",
    "        return pd.DataFrame({\n",
    "            'product_id': range(1, n + 1),\n",
    "            'category': np.random.choice(categories, n),\n",
    "            'price': np.random.uniform(10, 500, n).round(2),\n",
    "            'stock': np.random.randint(0, 1000, n)\n",
    "        })\n",
    "    \n",
    "    def generate_user_events(self, n_events: int = 1000) -> pd.DataFrame:\n",
    "        event_types = ['page_view', 'add_to_cart', 'purchase', 'review']\n",
    "        weights = [0.6, 0.25, 0.10, 0.05]\n",
    "        \n",
    "        events = pd.DataFrame({\n",
    "            'event_id': range(1, n_events + 1),\n",
    "            'timestamp': [datetime.now() - timedelta(hours=np.random.randint(0, 720)) \n",
    "                         for _ in range(n_events)],\n",
    "            'user_id': np.random.choice(self.users['user_id'], n_events),\n",
    "            'product_id': np.random.choice(self.products['product_id'], n_events),\n",
    "            'event_type': np.random.choice(event_types, n_events, p=weights),\n",
    "            'session_id': [f\"sess_{np.random.randint(1, 200)}\" for _ in range(n_events)]\n",
    "        })\n",
    "        \n",
    "        return events.sort_values('timestamp').reset_index(drop=True)\n",
    "    \n",
    "    def generate_system_logs(self, n_logs: int = 500) -> pd.DataFrame:\n",
    "        services = ['api', 'payment', 'recommendation', 'search', 'inventory']\n",
    "        log_levels = ['INFO', 'WARNING', 'ERROR', 'DEBUG']\n",
    "        weights = [0.7, 0.15, 0.05, 0.10]\n",
    "        \n",
    "        logs = pd.DataFrame({\n",
    "            'log_id': range(1, n_logs + 1),\n",
    "            'timestamp': [datetime.now() - timedelta(minutes=np.random.randint(0, 1440)) \n",
    "                         for _ in range(n_logs)],\n",
    "            'service': np.random.choice(services, n_logs),\n",
    "            'level': np.random.choice(log_levels, n_logs, p=weights),\n",
    "            'response_time_ms': np.random.exponential(100, n_logs).round(2),\n",
    "            'cpu_usage': np.random.uniform(10, 90, n_logs).round(1)\n",
    "        })\n",
    "        \n",
    "        return logs.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Create simulator and generate data\n",
    "simulator = DataSourceSimulator(seed=42)\n",
    "\n",
    "user_events = simulator.generate_user_events(1000)\n",
    "print(\"User-Generated Events (First-Party Data):\")\n",
    "print(user_events.head(10))\n",
    "print(f\"\\nTotal events: {len(user_events)}\")\n",
    "print(f\"Event types: {user_events['event_type'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System-generated data\n",
    "system_logs = simulator.generate_system_logs(500)\n",
    "print(\"System-Generated Logs:\")\n",
    "print(system_logs.head(10))\n",
    "print(f\"\\nLog levels: {system_logs['level'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data source characteristics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# User events by type\n",
    "ax1 = axes[0, 0]\n",
    "user_events['event_type'].value_counts().plot(kind='bar', ax=ax1, color='steelblue')\n",
    "ax1.set_title('User-Generated Events by Type', fontsize=12)\n",
    "ax1.set_xlabel('Event Type')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# System logs by service\n",
    "ax2 = axes[0, 1]\n",
    "system_logs['service'].value_counts().plot(kind='bar', ax=ax2, color='coral')\n",
    "ax2.set_title('System Logs by Service', fontsize=12)\n",
    "ax2.set_xlabel('Service')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Response time distribution\n",
    "ax3 = axes[1, 0]\n",
    "system_logs['response_time_ms'].hist(bins=50, ax=ax3, color='green', alpha=0.7)\n",
    "ax3.set_title('Response Time Distribution', fontsize=12)\n",
    "ax3.set_xlabel('Response Time (ms)')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.axvline(system_logs['response_time_ms'].mean(), color='red', linestyle='--', \n",
    "            label=f\"Mean: {system_logs['response_time_ms'].mean():.1f}ms\")\n",
    "ax3.legend()\n",
    "\n",
    "# Events over time\n",
    "ax4 = axes[1, 1]\n",
    "user_events['date'] = user_events['timestamp'].dt.date\n",
    "events_per_day = user_events.groupby('date').size()\n",
    "events_per_day.plot(ax=ax4, color='purple', linewidth=2)\n",
    "ax4.set_title('User Events Over Time', fontsize=12)\n",
    "ax4.set_xlabel('Date')\n",
    "ax4.set_ylabel('Number of Events')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Data Storage Types <a id='3-data-storage-types'></a>\n",
    "\n",
    "Choosing the right storage solution is critical for ML systems.\n",
    "\n",
    "### 3.1 Relational Databases (RDBMS)\n",
    "\n",
    "**Examples**: PostgreSQL, MySQL, SQLite\n",
    "\n",
    "**Best for**: Structured data with relationships, ACID compliance required\n",
    "\n",
    "### 3.2 NoSQL Databases\n",
    "\n",
    "| Type | Description | Examples | Use Cases |\n",
    "|------|-------------|----------|----------|\n",
    "| **Document** | JSON-like documents | MongoDB, Couchbase | User profiles, catalogs |\n",
    "| **Key-Value** | Simple key-value pairs | Redis, DynamoDB | Caching, sessions |\n",
    "| **Column** | Column-family storage | Cassandra, HBase | Time-series, analytics |\n",
    "| **Graph** | Nodes and relationships | Neo4j, Neptune | Social networks, fraud detection |\n",
    "\n",
    "### 3.3 Data Warehouses\n",
    "\n",
    "**Examples**: Snowflake, BigQuery, Redshift\n",
    "\n",
    "- Optimized for **analytical queries** (OLAP)\n",
    "- **Column-oriented** storage for fast aggregations\n",
    "\n",
    "### 3.4 Data Lakes\n",
    "\n",
    "**Examples**: AWS S3, Azure Data Lake, HDFS\n",
    "\n",
    "- Store **raw, unprocessed data** in any format\n",
    "- **Schema-on-read** approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simulating different storage paradigms\n",
    "\n",
    "class StorageSimulator:\n",
    "    \"\"\"Demonstrates different storage paradigms for ML data.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._relational_db = {}\n",
    "        self._document_db = []\n",
    "        self._key_value_db = {}\n",
    "        self._column_db = {}\n",
    "    \n",
    "    def relational_insert(self, table: str, data: pd.DataFrame) -> None:\n",
    "        self._relational_db[table] = data\n",
    "        print(f\"[RDBMS] Inserted {len(data)} rows into '{table}'\")\n",
    "    \n",
    "    def document_insert(self, document: Dict[str, Any]) -> str:\n",
    "        doc_id = f\"doc_{len(self._document_db) + 1}\"\n",
    "        document['_id'] = doc_id\n",
    "        self._document_db.append(document)\n",
    "        print(f\"[Document DB] Inserted document: {doc_id}\")\n",
    "        return doc_id\n",
    "    \n",
    "    def kv_set(self, key: str, value: Any, ttl: Optional[int] = None) -> None:\n",
    "        self._key_value_db[key] = {'value': value, 'ttl': ttl, 'created_at': datetime.now()}\n",
    "        print(f\"[Key-Value] SET {key}\")\n",
    "    \n",
    "    def kv_get(self, key: str) -> Any:\n",
    "        if key in self._key_value_db:\n",
    "            return self._key_value_db[key]['value']\n",
    "        return None\n",
    "    \n",
    "    def column_insert(self, row_key: str, column_family: str, columns: Dict[str, Any]) -> None:\n",
    "        if row_key not in self._column_db:\n",
    "            self._column_db[row_key] = {}\n",
    "        if column_family not in self._column_db[row_key]:\n",
    "            self._column_db[row_key][column_family] = {}\n",
    "        self._column_db[row_key][column_family].update(columns)\n",
    "        print(f\"[Column DB] Inserted {row_key}:{column_family}\")\n",
    "\n",
    "\n",
    "storage = StorageSimulator()\n",
    "\n",
    "# Example 1: Relational Database\n",
    "print(\"=\" * 60)\n",
    "print(\"RELATIONAL DATABASE EXAMPLE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "users_df = pd.DataFrame({\n",
    "    'user_id': [1, 2, 3],\n",
    "    'name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'email': ['alice@email.com', 'bob@email.com', 'charlie@email.com']\n",
    "})\n",
    "\n",
    "orders_df = pd.DataFrame({\n",
    "    'order_id': [101, 102, 103, 104],\n",
    "    'user_id': [1, 1, 2, 3],\n",
    "    'amount': [99.99, 49.50, 199.00, 75.25],\n",
    "    'status': ['completed', 'completed', 'pending', 'completed']\n",
    "})\n",
    "\n",
    "storage.relational_insert('users', users_df)\n",
    "storage.relational_insert('orders', orders_df)\n",
    "\n",
    "merged = users_df.merge(orders_df, on='user_id')\n",
    "print(\"\\nJOIN Result (users + orders):\")\n",
    "print(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Document Database\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DOCUMENT DATABASE EXAMPLE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "user_profile = {\n",
    "    'name': 'Alice',\n",
    "    'email': 'alice@email.com',\n",
    "    'preferences': {\n",
    "        'theme': 'dark',\n",
    "        'notifications': True,\n",
    "        'favorite_categories': ['Electronics', 'Books']\n",
    "    },\n",
    "    'activity': {\n",
    "        'last_login': '2024-01-15',\n",
    "        'total_purchases': 25,\n",
    "        'average_order_value': 85.50\n",
    "    }\n",
    "}\n",
    "\n",
    "doc_id = storage.document_insert(user_profile)\n",
    "print(f\"\\nDocument stored with flexible schema:\")\n",
    "print(json.dumps(user_profile, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Key-Value Store - Caching ML Model Predictions\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY-VALUE STORE EXAMPLE (Caching)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "prediction_cache = {\n",
    "    'user_1_recommendations': ['prod_42', 'prod_15', 'prod_88'],\n",
    "    'user_1_fraud_score': 0.12,\n",
    "    'user_1_churn_probability': 0.35\n",
    "}\n",
    "\n",
    "for key, value in prediction_cache.items():\n",
    "    storage.kv_set(key, value, ttl=3600)\n",
    "\n",
    "print(\"\\nCached predictions:\")\n",
    "for key in prediction_cache.keys():\n",
    "    print(f\"  {key}: {storage.kv_get(key)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Column Store - Time Series Data\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COLUMN STORE EXAMPLE (Time Series)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "storage.column_insert(\n",
    "    row_key='sensor_001:2024-01-15',\n",
    "    column_family='metrics',\n",
    "    columns={\n",
    "        '10:00:temperature': 72.5,\n",
    "        '10:00:humidity': 45.2,\n",
    "        '10:05:temperature': 72.8,\n",
    "        '10:05:humidity': 45.0\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\nColumn store structure:\")\n",
    "print(json.dumps(storage._column_db, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Structured vs Unstructured Data <a id='4-structured-vs-unstructured'></a>\n",
    "\n",
    "### 4.1 Comparison Table\n",
    "\n",
    "| Aspect | Structured Data | Unstructured Data |\n",
    "|--------|-----------------|-------------------|\n",
    "| **Format** | Tabular (rows/columns) | Text, images, audio, video |\n",
    "| **Schema** | Predefined | No fixed schema |\n",
    "| **Storage** | RDBMS, Data Warehouse | Data Lake, Object Storage |\n",
    "| **Query** | SQL | Specialized tools |\n",
    "| **Examples** | Transactions, CRM data | Emails, social media, images |\n",
    "| **ML Prep** | Direct feature extraction | Requires embedding/encoding |\n",
    "\n",
    "### 4.2 Semi-Structured Data\n",
    "\n",
    "Falls between structured and unstructured:\n",
    "- **JSON/XML documents**\n",
    "- **Log files**\n",
    "- **Sensor data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Working with Different Data Structures\n",
    "\n",
    "class DataStructureHandler:\n",
    "    \"\"\"Demonstrates handling of structured vs unstructured data.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def analyze_structured_data(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        return {\n",
    "            'shape': df.shape,\n",
    "            'columns': list(df.columns),\n",
    "            'dtypes': df.dtypes.to_dict(),\n",
    "            'missing_values': df.isnull().sum().to_dict(),\n",
    "            'memory_usage': f\"{df.memory_usage(deep=True).sum() / 1024:.2f} KB\"\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def analyze_unstructured_text(texts: List[str]) -> Dict[str, Any]:\n",
    "        word_counts = [len(text.split()) for text in texts]\n",
    "        char_counts = [len(text) for text in texts]\n",
    "        \n",
    "        return {\n",
    "            'total_documents': len(texts),\n",
    "            'avg_word_count': np.mean(word_counts),\n",
    "            'avg_char_count': np.mean(char_counts),\n",
    "            'min_word_count': min(word_counts),\n",
    "            'max_word_count': max(word_counts)\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def analyze_semi_structured(json_data: List[Dict]) -> Dict[str, Any]:\n",
    "        all_keys = set()\n",
    "        nested_keys = set()\n",
    "        \n",
    "        for doc in json_data:\n",
    "            for key, value in doc.items():\n",
    "                all_keys.add(key)\n",
    "                if isinstance(value, dict):\n",
    "                    nested_keys.add(key)\n",
    "        \n",
    "        return {\n",
    "            'total_documents': len(json_data),\n",
    "            'unique_fields': len(all_keys),\n",
    "            'fields': list(all_keys),\n",
    "            'nested_fields': list(nested_keys)\n",
    "        }\n",
    "\n",
    "\n",
    "handler = DataStructureHandler()\n",
    "\n",
    "# Structured Data Example\n",
    "print(\"=\" * 60)\n",
    "print(\"STRUCTURED DATA ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "structured_df = pd.DataFrame({\n",
    "    'customer_id': range(1, 101),\n",
    "    'age': np.random.randint(18, 80, 100),\n",
    "    'income': np.random.normal(60000, 20000, 100).round(2),\n",
    "    'credit_score': np.random.randint(300, 850, 100),\n",
    "    'tenure_months': np.random.randint(1, 120, 100),\n",
    "    'is_churned': np.random.choice([0, 1], 100, p=[0.85, 0.15])\n",
    "})\n",
    "\n",
    "structured_analysis = handler.analyze_structured_data(structured_df)\n",
    "print(\"\\nDataset Shape:\", structured_analysis['shape'])\n",
    "print(\"\\nColumn Types:\")\n",
    "for col, dtype in structured_analysis['dtypes'].items():\n",
    "    print(f\"  {col}: {dtype}\")\n",
    "print(f\"\\nMemory Usage: {structured_analysis['memory_usage']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unstructured Data Example\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"UNSTRUCTURED DATA ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "unstructured_texts = [\n",
    "    \"The product quality is excellent. I would definitely recommend it to others!\",\n",
    "    \"Terrible experience. The delivery was late and the item was damaged.\",\n",
    "    \"Good value for money. Works as expected.\",\n",
    "    \"Amazing customer service! They resolved my issue within hours.\",\n",
    "    \"Not worth the price. There are better alternatives available.\"\n",
    "]\n",
    "\n",
    "text_analysis = handler.analyze_unstructured_text(unstructured_texts)\n",
    "print(\"\\nText Statistics:\")\n",
    "for key, value in text_analysis.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semi-Structured Data Example\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SEMI-STRUCTURED DATA ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "semi_structured_data = [\n",
    "    {\n",
    "        'event_type': 'purchase',\n",
    "        'user_id': 'user_123',\n",
    "        'timestamp': '2024-01-15T10:30:00Z',\n",
    "        'metadata': {'device': 'mobile', 'browser': 'Safari'},\n",
    "        'items': ['prod_1', 'prod_2', 'prod_3']\n",
    "    },\n",
    "    {\n",
    "        'event_type': 'page_view',\n",
    "        'user_id': 'user_456',\n",
    "        'timestamp': '2024-01-15T10:35:00Z',\n",
    "        'metadata': {'device': 'desktop', 'browser': 'Chrome'},\n",
    "        'page_url': '/products/electronics'\n",
    "    }\n",
    "]\n",
    "\n",
    "semi_analysis = handler.analyze_semi_structured(semi_structured_data)\n",
    "print(\"\\nJSON Document Analysis:\")\n",
    "for key, value in semi_analysis.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nSample Document:\")\n",
    "print(json.dumps(semi_structured_data[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Data Types in ML <a id='5-data-types-in-ml'></a>\n",
    "\n",
    "### 5.1 Numerical Data\n",
    "\n",
    "| Type | Description | Examples | Considerations |\n",
    "|------|-------------|----------|----------------|\n",
    "| **Continuous** | Infinite possible values | Temperature, price, age | May need scaling |\n",
    "| **Discrete** | Countable values | Count of items, days | May need binning |\n",
    "\n",
    "### 5.2 Categorical Data\n",
    "\n",
    "| Type | Description | Examples | Encoding |\n",
    "|------|-------------|----------|----------|\n",
    "| **Nominal** | No inherent order | Color, country, gender | One-hot encoding |\n",
    "| **Ordinal** | Has meaningful order | Rating (1-5), education level | Label encoding |\n",
    "\n",
    "### 5.3 Temporal Data\n",
    "\n",
    "- **Timestamps** - Point in time\n",
    "- **Durations** - Time intervals\n",
    "- **Cyclic features** - Day of week, month, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Type Detection and Classification\n",
    "\n",
    "class DataTypeDetector:\n",
    "    \"\"\"Automatic detection and classification of data types for ML.\"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df\n",
    "        self.analysis = self._analyze_columns()\n",
    "    \n",
    "    def _analyze_columns(self) -> Dict[str, Dict]:\n",
    "        analysis = {}\n",
    "        for col in self.df.columns:\n",
    "            col_data = self.df[col]\n",
    "            analysis[col] = self._analyze_single_column(col_data)\n",
    "        return analysis\n",
    "    \n",
    "    def _analyze_single_column(self, series: pd.Series) -> Dict:\n",
    "        result = {\n",
    "            'pandas_dtype': str(series.dtype),\n",
    "            'unique_count': series.nunique(),\n",
    "            'unique_ratio': series.nunique() / len(series),\n",
    "            'null_count': series.isnull().sum(),\n",
    "            'null_ratio': series.isnull().mean()\n",
    "        }\n",
    "        \n",
    "        # Determine ML data type\n",
    "        if pd.api.types.is_numeric_dtype(series):\n",
    "            if result['unique_ratio'] < 0.05 and result['unique_count'] < 20:\n",
    "                result['ml_type'] = 'categorical_discrete'\n",
    "            elif pd.api.types.is_integer_dtype(series):\n",
    "                result['ml_type'] = 'numerical_discrete'\n",
    "            else:\n",
    "                result['ml_type'] = 'numerical_continuous'\n",
    "        elif pd.api.types.is_datetime64_any_dtype(series):\n",
    "            result['ml_type'] = 'temporal'\n",
    "        elif pd.api.types.is_bool_dtype(series):\n",
    "            result['ml_type'] = 'binary'\n",
    "        else:\n",
    "            if result['unique_count'] < 10:\n",
    "                result['ml_type'] = 'categorical_nominal'\n",
    "            elif result['unique_count'] < 50:\n",
    "                result['ml_type'] = 'categorical_high_cardinality'\n",
    "            else:\n",
    "                result['ml_type'] = 'text_or_id'\n",
    "        \n",
    "        result['recommendations'] = self._get_recommendations(result['ml_type'])\n",
    "        return result\n",
    "    \n",
    "    def _get_recommendations(self, ml_type: str) -> List[str]:\n",
    "        recommendations = {\n",
    "            'numerical_continuous': ['Consider StandardScaler or MinMaxScaler', 'Check for outliers'],\n",
    "            'numerical_discrete': ['Consider binning for count data', 'May use as-is for tree-based models'],\n",
    "            'categorical_discrete': ['Use Label Encoding or One-Hot Encoding'],\n",
    "            'categorical_nominal': ['Use One-Hot Encoding', 'Consider Target Encoding'],\n",
    "            'categorical_high_cardinality': ['Use Target Encoding or Embeddings'],\n",
    "            'temporal': ['Extract features: year, month, day, day_of_week, hour'],\n",
    "            'binary': ['Use as-is (0/1)'],\n",
    "            'text_or_id': ['If ID: Drop or use for grouping', 'If text: Use TF-IDF or embeddings']\n",
    "        }\n",
    "        return recommendations.get(ml_type, ['Manual inspection recommended'])\n",
    "    \n",
    "    def get_summary(self) -> pd.DataFrame:\n",
    "        summary_data = []\n",
    "        for col, analysis in self.analysis.items():\n",
    "            summary_data.append({\n",
    "                'column': col,\n",
    "                'pandas_dtype': analysis['pandas_dtype'],\n",
    "                'ml_type': analysis['ml_type'],\n",
    "                'unique_count': analysis['unique_count'],\n",
    "                'unique_ratio': f\"{analysis['unique_ratio']:.2%}\",\n",
    "                'null_ratio': f\"{analysis['null_ratio']:.2%}\"\n",
    "            })\n",
    "        return pd.DataFrame(summary_data)\n",
    "    \n",
    "    def print_recommendations(self):\n",
    "        for col, analysis in self.analysis.items():\n",
    "            print(f\"\\n{col} ({analysis['ml_type']}):\")\n",
    "            for rec in analysis['recommendations']:\n",
    "                print(f\"  -> {rec}\")\n",
    "\n",
    "\n",
    "# Create sample dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "\n",
    "sample_df = pd.DataFrame({\n",
    "    'age': np.random.normal(40, 15, n_samples).clip(18, 85),\n",
    "    'income': np.random.lognormal(10.5, 0.8, n_samples),\n",
    "    'num_products': np.random.poisson(3, n_samples),\n",
    "    'country': np.random.choice(['US', 'UK', 'DE', 'FR', 'JP'], n_samples),\n",
    "    'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], n_samples),\n",
    "    'signup_date': pd.date_range('2020-01-01', periods=n_samples, freq='D'),\n",
    "    'is_premium': np.random.choice([True, False], n_samples, p=[0.2, 0.8]),\n",
    "    'user_id': [f'USER_{i:08d}' for i in range(n_samples)]\n",
    "})\n",
    "\n",
    "sample_df.loc[np.random.choice(n_samples, 25), 'income'] = np.nan\n",
    "\n",
    "print(\"Sample Dataset:\")\n",
    "print(sample_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the dataset\n",
    "detector = DataTypeDetector(sample_df)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATA TYPE ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(detector.get_summary().to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print recommendations\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PREPROCESSING RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "detector.print_recommendations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Data Type Distribution\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "ax1 = axes[0, 0]\n",
    "sample_df['income'].hist(bins=50, ax=ax1, color='steelblue', alpha=0.7)\n",
    "ax1.set_title('Continuous: Income', fontsize=11)\n",
    "ax1.set_xlabel('Income')\n",
    "ax1.axvline(sample_df['income'].median(), color='red', linestyle='--', label='Median')\n",
    "ax1.legend()\n",
    "\n",
    "ax2 = axes[0, 1]\n",
    "sample_df['num_products'].value_counts().sort_index().plot(kind='bar', ax=ax2, color='coral', alpha=0.7)\n",
    "ax2.set_title('Discrete: Number of Products', fontsize=11)\n",
    "ax2.tick_params(axis='x', rotation=0)\n",
    "\n",
    "ax3 = axes[0, 2]\n",
    "sample_df['country'].value_counts().plot(kind='bar', ax=ax3, color='green', alpha=0.7)\n",
    "ax3.set_title('Categorical Nominal: Country', fontsize=11)\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "ax4 = axes[1, 0]\n",
    "edu_order = ['High School', 'Bachelor', 'Master', 'PhD']\n",
    "sample_df['education'].value_counts()[edu_order].plot(kind='bar', ax=ax4, color='purple', alpha=0.7)\n",
    "ax4.set_title('Categorical Ordinal: Education', fontsize=11)\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "\n",
    "ax5 = axes[1, 1]\n",
    "sample_df['is_premium'].value_counts().plot(kind='pie', ax=ax5, autopct='%1.1f%%', colors=['lightcoral', 'lightgreen'])\n",
    "ax5.set_title('Binary: Is Premium', fontsize=11)\n",
    "ax5.set_ylabel('')\n",
    "\n",
    "ax6 = axes[1, 2]\n",
    "sample_df['age'].hist(bins=30, ax=ax6, color='teal', alpha=0.7)\n",
    "ax6.set_title('Continuous: Age Distribution', fontsize=11)\n",
    "ax6.set_xlabel('Age')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Hands-on Exercise <a id='6-hands-on-exercise'></a>\n",
    "\n",
    "### Exercise: Analyze a Mixed Dataset\n",
    "\n",
    "In this exercise, you'll work with a realistic e-commerce dataset and:\n",
    "\n",
    "1. Identify data sources and their characteristics\n",
    "2. Determine appropriate storage solutions\n",
    "3. Classify data types for ML\n",
    "4. Provide preprocessing recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise Dataset: E-commerce Customer Data\n",
    "np.random.seed(42)\n",
    "n_customers = 1000\n",
    "\n",
    "# Customers table (structured, first-party)\n",
    "customers = pd.DataFrame({\n",
    "    'customer_id': [f'CUST_{i:06d}' for i in range(n_customers)],\n",
    "    'signup_date': pd.date_range('2020-01-01', periods=n_customers, freq='8H'),\n",
    "    'age': np.random.normal(35, 12, n_customers).clip(18, 80).astype(int),\n",
    "    'gender': np.random.choice(['M', 'F', 'Other'], n_customers, p=[0.45, 0.45, 0.10]),\n",
    "    'country': np.random.choice(['US', 'UK', 'DE', 'FR', 'JP', 'CA', 'AU'], n_customers,\n",
    "                                p=[0.35, 0.15, 0.12, 0.12, 0.10, 0.08, 0.08]),\n",
    "    'membership_tier': np.random.choice(['Bronze', 'Silver', 'Gold', 'Platinum'], n_customers,\n",
    "                                        p=[0.50, 0.30, 0.15, 0.05]),\n",
    "    'email_opt_in': np.random.choice([True, False], n_customers, p=[0.7, 0.3]),\n",
    "    'lifetime_value': np.random.exponential(500, n_customers).round(2)\n",
    "})\n",
    "\n",
    "# Add some missing values\n",
    "customers.loc[np.random.choice(n_customers, 50), 'age'] = np.nan\n",
    "customers.loc[np.random.choice(n_customers, 30), 'country'] = np.nan\n",
    "\n",
    "print(\"E-commerce Customer Dataset:\")\n",
    "print(customers.head(10))\n",
    "print(f\"\\nShape: {customers.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TASK: Analyze the dataset\n",
    "\n",
    "# 1. Create a DataTypeDetector for the customers dataset\n",
    "customer_detector = DataTypeDetector(customers)\n",
    "\n",
    "# 2. Display the summary\n",
    "print(\"DATA TYPE SUMMARY:\")\n",
    "print(customer_detector.get_summary().to_string(index=False))\n",
    "\n",
    "# 3. Print recommendations\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RECOMMENDATIONS:\")\n",
    "customer_detector.print_recommendations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise Questions:\n",
    "\n",
    "questions = \"\"\"\n",
    "EXERCISE QUESTIONS:\n",
    "\n",
    "1. What type of data source is this dataset (user-generated, system-generated, first-party, third-party)?\n",
    "   Answer: First-party, primarily system-generated with some user-generated fields\n",
    "\n",
    "2. What storage solution would you recommend for this data?\n",
    "   Answer: Relational database (PostgreSQL) for structured queries and joins\n",
    "           Data warehouse (Snowflake/BigQuery) for analytical queries\n",
    "\n",
    "3. Which columns have missing values and how would you handle them?\n",
    "   Answer: 'age' and 'country' have missing values\n",
    "           - age: median imputation or model-based imputation\n",
    "           - country: mode imputation or 'Unknown' category\n",
    "\n",
    "4. Which categorical features might need special encoding?\n",
    "   Answer: \n",
    "           - membership_tier: ordinal encoding (Bronze < Silver < Gold < Platinum)\n",
    "           - country: one-hot encoding or target encoding if high cardinality\n",
    "           - gender: one-hot encoding\n",
    "\n",
    "5. What temporal features could you extract from signup_date?\n",
    "   Answer: year, month, day_of_week, is_weekend, days_since_signup\n",
    "\"\"\"\n",
    "\n",
    "print(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Summary and Key Takeaways <a id='7-summary'></a>\n",
    "\n",
    "### Key Concepts Covered\n",
    "\n",
    "1. **Data Sources**\n",
    "   - User-generated vs system-generated data\n",
    "   - First-party vs third-party data\n",
    "   - Real-time vs batch data\n",
    "\n",
    "2. **Data Storage Types**\n",
    "   - Relational databases (RDBMS) for structured data with relationships\n",
    "   - NoSQL databases for flexibility and scale\n",
    "   - Data warehouses for analytics\n",
    "   - Data lakes for raw data storage\n",
    "\n",
    "3. **Data Structures**\n",
    "   - Structured: tabular, predefined schema\n",
    "   - Semi-structured: JSON, logs\n",
    "   - Unstructured: text, images, audio\n",
    "\n",
    "4. **Data Types for ML**\n",
    "   - Numerical: continuous, discrete\n",
    "   - Categorical: nominal, ordinal\n",
    "   - Temporal: timestamps, durations\n",
    "   - Binary: boolean values\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- Always understand your data sources before building ML pipelines\n",
    "- Choose storage solutions based on access patterns and scale requirements\n",
    "- Properly identify and classify data types for appropriate preprocessing\n",
    "- Document data quality issues and handle missing values appropriately\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next tutorial (Tutorial 05: ETL Pipelines), we will learn how to:\n",
    "- Design and implement ETL pipelines for ML workflows\n",
    "- Extract data from multiple sources\n",
    "- Apply data transformations and cleaning techniques\n",
    "- Load processed data to feature stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary visualization\n",
    "print(\"=\" * 70)\n",
    "print(\"TUTORIAL 04 COMPLETE: Data Engineering Fundamentals\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nKey topics covered:\")\n",
    "print(\"  1. Data Sources (user-generated, system-generated, first/third-party)\")\n",
    "print(\"  2. Storage Types (RDBMS, NoSQL, Data Warehouses, Data Lakes)\")\n",
    "print(\"  3. Data Structures (structured, semi-structured, unstructured)\")\n",
    "print(\"  4. ML Data Types (numerical, categorical, temporal, binary)\")\n",
    "print(\"\\nNext: Tutorial 05 - ETL Pipelines\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
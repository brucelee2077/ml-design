{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 05: ETL Pipelines for ML\n",
    "\n",
    "## Module 3: Data Preparation\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "\n",
    "1. **Design and implement ETL pipelines** for ML workflows\n",
    "2. **Handle data extraction** from multiple sources (batch, streaming, API)\n",
    "3. **Apply data transformation** and cleaning techniques\n",
    "4. **Load processed data** to data warehouses and feature stores\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction to ETL for ML](#1-introduction)\n",
    "2. [Extract Phase](#2-extract)\n",
    "3. [Transform Phase](#3-transform)\n",
    "4. [Load Phase](#4-load)\n",
    "5. [Complete ETL Pipeline Example](#5-complete-pipeline)\n",
    "6. [Data Quality and Validation](#6-data-quality)\n",
    "7. [Hands-on Exercise](#7-exercise)\n",
    "8. [Summary and Key Takeaways](#8-summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Introduction to ETL for ML <a id='1-introduction'></a>\n",
    "\n",
    "### What is ETL?\n",
    "\n",
    "**ETL** stands for **Extract, Transform, Load** - a data integration process.\n",
    "\n",
    "### Why ETL Matters for ML\n",
    "\n",
    "- **Data Quality**: Ensures clean, consistent data for training\n",
    "- **Feature Engineering**: Prepares features for model consumption\n",
    "- **Reproducibility**: Creates traceable data pipelines\n",
    "- **Scalability**: Handles growing data volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Any, Optional, Callable\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime, timedelta\n",
    "from abc import ABC, abstractmethod\n",
    "import json\n",
    "import hashlib\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger('ETL')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Extract Phase <a id='2-extract'></a>\n",
    "\n",
    "The Extract phase involves collecting data from various sources.\n",
    "\n",
    "| Method | Description | Latency |\n",
    "|--------|-------------|----------|\n",
    "| **Batch** | Periodic full/incremental loads | Hours/Days |\n",
    "| **Streaming** | Real-time data ingestion | Seconds |\n",
    "| **API** | Pull data from web services | Variable |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Extractor Interface\n",
    "class BaseExtractor(ABC):\n",
    "    def __init__(self, source_name: str):\n",
    "        self.source_name = source_name\n",
    "        self.extraction_timestamp = None\n",
    "        self.records_extracted = 0\n",
    "    \n",
    "    @abstractmethod\n",
    "    def extract(self, **kwargs) -> pd.DataFrame:\n",
    "        pass\n",
    "    \n",
    "    def get_metadata(self) -> Dict[str, Any]:\n",
    "        return {'source': self.source_name, 'timestamp': self.extraction_timestamp, 'records': self.records_extracted}\n",
    "\n",
    "\n",
    "class BatchExtractor(BaseExtractor):\n",
    "    def __init__(self, source_name: str, data_generator: Callable):\n",
    "        super().__init__(source_name)\n",
    "        self.data_generator = data_generator\n",
    "    \n",
    "    def extract(self, mode: str = 'full', since: Optional[datetime] = None) -> pd.DataFrame:\n",
    "        logger.info(f\"Starting {mode} extraction from {self.source_name}\")\n",
    "        self.extraction_timestamp = datetime.now()\n",
    "        df = self.data_generator()\n",
    "        \n",
    "        if mode == 'incremental' and since and 'updated_at' in df.columns:\n",
    "            df = df[df['updated_at'] > since]\n",
    "        \n",
    "        self.records_extracted = len(df)\n",
    "        logger.info(f\"Extracted {self.records_extracted} records\")\n",
    "        return df\n",
    "\n",
    "\n",
    "class APIExtractor(BaseExtractor):\n",
    "    def __init__(self, source_name: str, endpoint: str):\n",
    "        super().__init__(source_name)\n",
    "        self.endpoint = endpoint\n",
    "    \n",
    "    def extract(self, params: Optional[Dict] = None) -> pd.DataFrame:\n",
    "        logger.info(f\"Fetching from API: {self.endpoint}\")\n",
    "        self.extraction_timestamp = datetime.now()\n",
    "        \n",
    "        max_pages = params.get('max_pages', 3) if params else 3\n",
    "        page_size = params.get('page_size', 100) if params else 100\n",
    "        \n",
    "        records = []\n",
    "        for page in range(1, max_pages + 1):\n",
    "            np.random.seed(page * 42)\n",
    "            for i in range(page_size):\n",
    "                records.append({\n",
    "                    'id': f\"api_{(page-1)*page_size + i}\",\n",
    "                    'value': np.random.uniform(0, 100),\n",
    "                    'category': np.random.choice(['A', 'B', 'C'])\n",
    "                })\n",
    "        \n",
    "        df = pd.DataFrame(records)\n",
    "        self.records_extracted = len(df)\n",
    "        return df\n",
    "\n",
    "\n",
    "class StreamExtractor(BaseExtractor):\n",
    "    def extract(self, window_seconds: int = 60, events_per_second: int = 10) -> pd.DataFrame:\n",
    "        logger.info(f\"Consuming stream for {window_seconds}s window\")\n",
    "        self.extraction_timestamp = datetime.now()\n",
    "        \n",
    "        total_events = window_seconds * events_per_second\n",
    "        events = []\n",
    "        for i in range(total_events):\n",
    "            events.append({\n",
    "                'event_id': f\"evt_{i:08d}\",\n",
    "                'event_type': np.random.choice(['click', 'view', 'purchase'], p=[0.5, 0.35, 0.15]),\n",
    "                'user_id': f\"user_{np.random.randint(1, 1000)}\",\n",
    "                'value': np.random.exponential(10)\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(events)\n",
    "        self.records_extracted = len(df)\n",
    "        return df\n",
    "\n",
    "print(\"Extractor classes defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo Batch Extraction\n",
    "def generate_customer_data() -> pd.DataFrame:\n",
    "    np.random.seed(42)\n",
    "    n = 500\n",
    "    return pd.DataFrame({\n",
    "        'customer_id': [f'CUST_{i:05d}' for i in range(n)],\n",
    "        'name': [f'Customer_{i}' for i in range(n)],\n",
    "        'country': np.random.choice(['US', 'UK', 'DE', 'FR', 'JP'], n),\n",
    "        'created_at': pd.date_range('2023-01-01', periods=n, freq='2H'),\n",
    "        'updated_at': [datetime.now() - timedelta(days=np.random.randint(0, 30)) for _ in range(n)],\n",
    "        'lifetime_value': np.random.exponential(500, n).round(2)\n",
    "    })\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BATCH EXTRACTION DEMO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "batch_extractor = BatchExtractor('customer_db', generate_customer_data)\n",
    "df_full = batch_extractor.extract(mode='full')\n",
    "print(f\"Full extraction: {len(df_full)} records\")\n",
    "print(df_full.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo API Extraction\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"API EXTRACTION DEMO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "api_extractor = APIExtractor('third_party_api', 'https://api.example.com/data')\n",
    "df_api = api_extractor.extract(params={'max_pages': 2, 'page_size': 50})\n",
    "print(f\"API extraction: {len(df_api)} records\")\n",
    "print(df_api.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo Stream Extraction\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STREAM EXTRACTION DEMO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "stream_extractor = StreamExtractor('event_stream')\n",
    "df_stream = stream_extractor.extract(window_seconds=10, events_per_second=5)\n",
    "print(f\"Stream extraction: {len(df_stream)} events\")\n",
    "print(df_stream['event_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Transform Phase <a id='3-transform'></a>\n",
    "\n",
    "The Transform phase processes extracted data.\n",
    "\n",
    "| Category | Operations | Purpose |\n",
    "|----------|------------|----------|\n",
    "| **Cleaning** | Remove duplicates, handle nulls | Data quality |\n",
    "| **Validation** | Type checking, range validation | Data integrity |\n",
    "| **Enrichment** | Add derived fields | Feature creation |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transformer\n",
    "@dataclass\n",
    "class TransformResult:\n",
    "    data: pd.DataFrame\n",
    "    rows_before: int\n",
    "    rows_after: int\n",
    "    transformations: List[str] = field(default_factory=list)\n",
    "\n",
    "\n",
    "class DataTransformer:\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.original_df = df.copy()\n",
    "        self.df = df.copy()\n",
    "        self.transformations = []\n",
    "    \n",
    "    def remove_duplicates(self, subset: Optional[List[str]] = None) -> 'DataTransformer':\n",
    "        before = len(self.df)\n",
    "        self.df = self.df.drop_duplicates(subset=subset)\n",
    "        removed = before - len(self.df)\n",
    "        if removed > 0:\n",
    "            self.transformations.append(f\"remove_duplicates: {removed} removed\")\n",
    "        return self\n",
    "    \n",
    "    def handle_missing(self, strategy: Dict[str, str]) -> 'DataTransformer':\n",
    "        for column, strat in strategy.items():\n",
    "            if column not in self.df.columns:\n",
    "                continue\n",
    "            null_count = self.df[column].isnull().sum()\n",
    "            if null_count == 0:\n",
    "                continue\n",
    "            \n",
    "            if strat == 'drop':\n",
    "                self.df = self.df.dropna(subset=[column])\n",
    "            elif strat == 'mean':\n",
    "                self.df[column] = self.df[column].fillna(self.df[column].mean())\n",
    "            elif strat == 'median':\n",
    "                self.df[column] = self.df[column].fillna(self.df[column].median())\n",
    "            elif strat == 'mode':\n",
    "                self.df[column] = self.df[column].fillna(self.df[column].mode()[0])\n",
    "            else:\n",
    "                self.df[column] = self.df[column].fillna(strat)\n",
    "            \n",
    "            self.transformations.append(f\"handle_missing({column}): {null_count} -> {strat}\")\n",
    "        return self\n",
    "    \n",
    "    def remove_outliers(self, columns: List[str], method: str = 'iqr', factor: float = 1.5) -> 'DataTransformer':\n",
    "        before = len(self.df)\n",
    "        for col in columns:\n",
    "            if col not in self.df.columns or not pd.api.types.is_numeric_dtype(self.df[col]):\n",
    "                continue\n",
    "            \n",
    "            if method == 'iqr':\n",
    "                Q1, Q3 = self.df[col].quantile([0.25, 0.75])\n",
    "                IQR = Q3 - Q1\n",
    "                self.df = self.df[(self.df[col] >= Q1 - factor*IQR) & (self.df[col] <= Q3 + factor*IQR)]\n",
    "            elif method == 'zscore':\n",
    "                z = np.abs((self.df[col] - self.df[col].mean()) / self.df[col].std())\n",
    "                self.df = self.df[z < factor]\n",
    "        \n",
    "        removed = before - len(self.df)\n",
    "        if removed > 0:\n",
    "            self.transformations.append(f\"remove_outliers: {removed} removed\")\n",
    "        return self\n",
    "    \n",
    "    def validate_range(self, column: str, min_val: float = None, max_val: float = None) -> 'DataTransformer':\n",
    "        if column not in self.df.columns:\n",
    "            return self\n",
    "        before = len(self.df)\n",
    "        if min_val is not None:\n",
    "            self.df = self.df[self.df[column] >= min_val]\n",
    "        if max_val is not None:\n",
    "            self.df = self.df[self.df[column] <= max_val]\n",
    "        removed = before - len(self.df)\n",
    "        if removed > 0:\n",
    "            self.transformations.append(f\"validate_range({column}): {removed} removed\")\n",
    "        return self\n",
    "    \n",
    "    def add_column(self, name: str, func: Callable) -> 'DataTransformer':\n",
    "        self.df[name] = func(self.df)\n",
    "        self.transformations.append(f\"add_column: {name}\")\n",
    "        return self\n",
    "    \n",
    "    def add_date_features(self, date_column: str) -> 'DataTransformer':\n",
    "        if date_column not in self.df.columns:\n",
    "            return self\n",
    "        self.df[date_column] = pd.to_datetime(self.df[date_column])\n",
    "        self.df[f'{date_column}_year'] = self.df[date_column].dt.year\n",
    "        self.df[f'{date_column}_month'] = self.df[date_column].dt.month\n",
    "        self.df[f'{date_column}_dayofweek'] = self.df[date_column].dt.dayofweek\n",
    "        self.transformations.append(f\"add_date_features: {date_column}\")\n",
    "        return self\n",
    "    \n",
    "    def standardize_text(self, columns: List[str]) -> 'DataTransformer':\n",
    "        for col in columns:\n",
    "            if col in self.df.columns:\n",
    "                self.df[col] = self.df[col].str.strip().str.lower()\n",
    "        self.transformations.append(f\"standardize_text: {columns}\")\n",
    "        return self\n",
    "    \n",
    "    def get_result(self) -> TransformResult:\n",
    "        return TransformResult(\n",
    "            data=self.df,\n",
    "            rows_before=len(self.original_df),\n",
    "            rows_after=len(self.df),\n",
    "            transformations=self.transformations\n",
    "        )\n",
    "\n",
    "print(\"DataTransformer class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Demo\n",
    "print(\"=\" * 60)\n",
    "print(\"TRANSFORMATION DEMO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create data with issues\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "raw_data = pd.DataFrame({\n",
    "    'customer_id': [f'CUST_{i:05d}' for i in range(n)],\n",
    "    'name': [f'  Customer {i}  ' if np.random.random() > 0.1 else None for i in range(n)],\n",
    "    'email': [f'customer{i}@EMAIL.COM' for i in range(n)],\n",
    "    'age': np.random.normal(40, 15, n),\n",
    "    'income': np.random.lognormal(10, 1, n),\n",
    "    'signup_date': pd.date_range('2020-01-01', periods=n, freq='6H'),\n",
    "    'country': np.random.choice(['US', 'UK', 'DE', 'FR', None], n, p=[0.35, 0.25, 0.2, 0.15, 0.05])\n",
    "})\n",
    "\n",
    "# Add duplicates and outliers\n",
    "raw_data = pd.concat([raw_data, raw_data.iloc[:50]], ignore_index=True)\n",
    "raw_data.loc[np.random.choice(len(raw_data), 20), 'income'] = np.random.uniform(1e8, 1e9, 20)\n",
    "raw_data.loc[np.random.choice(len(raw_data), 10), 'age'] = np.random.choice([-5, 150], 10)\n",
    "\n",
    "print(f\"Raw data: {len(raw_data)} rows\")\n",
    "print(f\"Missing values: {raw_data.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations\n",
    "transformer = DataTransformer(raw_data)\n",
    "\n",
    "result = (\n",
    "    transformer\n",
    "    .remove_duplicates(subset=['customer_id'])\n",
    "    .handle_missing({'name': 'Unknown', 'country': 'mode', 'age': 'median'})\n",
    "    .validate_range('age', min_val=0, max_val=120)\n",
    "    .remove_outliers(['income'], method='iqr', factor=3)\n",
    "    .standardize_text(['email'])\n",
    "    .add_date_features('signup_date')\n",
    "    .add_column('income_category', \n",
    "        lambda df: pd.cut(df['income'], bins=[0, 30000, 60000, 100000, float('inf')],\n",
    "                         labels=['Low', 'Medium', 'High', 'Very High']))\n",
    "    .get_result()\n",
    ")\n",
    "\n",
    "print(f\"\\nRows before: {result.rows_before}\")\n",
    "print(f\"Rows after: {result.rows_after}\")\n",
    "print(f\"\\nTransformations applied:\")\n",
    "for t in result.transformations:\n",
    "    print(f\"  - {t}\")\n",
    "\n",
    "print(\"\\nTransformed data sample:\")\n",
    "print(result.data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize transformation effects\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "ax1 = axes[0, 0]\n",
    "raw_data['income'].hist(bins=50, ax=ax1, alpha=0.5, label='Before', color='red')\n",
    "result.data['income'].hist(bins=50, ax=ax1, alpha=0.5, label='After', color='green')\n",
    "ax1.set_title('Income: Before vs After Outlier Removal')\n",
    "ax1.legend()\n",
    "\n",
    "ax2 = axes[0, 1]\n",
    "result.data['age'].hist(bins=30, ax=ax2, color='steelblue')\n",
    "ax2.set_title('Age After Validation')\n",
    "\n",
    "ax3 = axes[1, 0]\n",
    "result.data['signup_date_dayofweek'].value_counts().sort_index().plot(kind='bar', ax=ax3, color='coral')\n",
    "ax3.set_title('Signups by Day of Week')\n",
    "ax3.set_xlabel('Day (0=Mon)')\n",
    "\n",
    "ax4 = axes[1, 1]\n",
    "result.data['income_category'].value_counts().plot(kind='pie', ax=ax4, autopct='%1.1f%%')\n",
    "ax4.set_title('Income Categories')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Load Phase <a id='4-load'></a>\n",
    "\n",
    "The Load phase stores transformed data in target systems.\n",
    "\n",
    "| Strategy | Description | Use Case |\n",
    "|----------|-------------|----------|\n",
    "| **Full** | Replace entire table | Initial load |\n",
    "| **Incremental** | Add new records | Large tables |\n",
    "| **Upsert** | Insert or update | Dimension tables |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader\n",
    "class DataLoader:\n",
    "    def __init__(self, target_name: str):\n",
    "        self.target_name = target_name\n",
    "        self._storage = {}\n",
    "        self.load_history = []\n",
    "    \n",
    "    def full_load(self, table: str, data: pd.DataFrame) -> Dict[str, Any]:\n",
    "        old_count = len(self._storage.get(table, pd.DataFrame()))\n",
    "        self._storage[table] = data.copy()\n",
    "        result = {'operation': 'full_load', 'table': table, 'rows_before': old_count, 'rows_after': len(data)}\n",
    "        self.load_history.append(result)\n",
    "        logger.info(f\"Full load to {table}: {len(data)} rows\")\n",
    "        return result\n",
    "    \n",
    "    def incremental_load(self, table: str, data: pd.DataFrame, key_column: str) -> Dict[str, Any]:\n",
    "        if table not in self._storage:\n",
    "            self._storage[table] = pd.DataFrame()\n",
    "        \n",
    "        existing = self._storage[table]\n",
    "        existing_keys = set(existing[key_column]) if len(existing) > 0 else set()\n",
    "        new_records = data[~data[key_column].isin(existing_keys)]\n",
    "        self._storage[table] = pd.concat([existing, new_records], ignore_index=True)\n",
    "        \n",
    "        result = {'operation': 'incremental', 'table': table, 'rows_new': len(new_records), 'rows_total': len(self._storage[table])}\n",
    "        self.load_history.append(result)\n",
    "        logger.info(f\"Incremental load to {table}: {len(new_records)} new rows\")\n",
    "        return result\n",
    "    \n",
    "    def upsert(self, table: str, data: pd.DataFrame, key_column: str) -> Dict[str, Any]:\n",
    "        if table not in self._storage:\n",
    "            self._storage[table] = pd.DataFrame()\n",
    "        \n",
    "        existing = self._storage[table]\n",
    "        \n",
    "        if len(existing) > 0:\n",
    "            existing_keys = set(existing[key_column])\n",
    "            incoming_keys = set(data[key_column])\n",
    "            update_keys = existing_keys & incoming_keys\n",
    "            unchanged = existing[~existing[key_column].isin(update_keys)]\n",
    "            self._storage[table] = pd.concat([unchanged, data], ignore_index=True)\n",
    "            updates = len(update_keys)\n",
    "            inserts = len(incoming_keys - existing_keys)\n",
    "        else:\n",
    "            self._storage[table] = data.copy()\n",
    "            updates, inserts = 0, len(data)\n",
    "        \n",
    "        result = {'operation': 'upsert', 'table': table, 'updates': updates, 'inserts': inserts, 'rows_total': len(self._storage[table])}\n",
    "        self.load_history.append(result)\n",
    "        logger.info(f\"Upsert to {table}: {inserts} inserts, {updates} updates\")\n",
    "        return result\n",
    "    \n",
    "    def get_table(self, table: str) -> pd.DataFrame:\n",
    "        return self._storage.get(table, pd.DataFrame())\n",
    "\n",
    "print(\"DataLoader class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Demo\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA LOADING DEMO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "clean_data = result.data.copy()\n",
    "loader = DataLoader('data_warehouse')\n",
    "\n",
    "# Full Load\n",
    "print(\"\\n--- FULL LOAD ---\")\n",
    "full_result = loader.full_load('customers', clean_data)\n",
    "print(f\"Result: {full_result}\")\n",
    "\n",
    "# Incremental Load\n",
    "print(\"\\n--- INCREMENTAL LOAD ---\")\n",
    "new_customers = pd.DataFrame({\n",
    "    'customer_id': [f'CUST_NEW_{i:03d}' for i in range(50)],\n",
    "    'name': [f'New Customer {i}' for i in range(50)],\n",
    "    'email': [f'newcust{i}@email.com' for i in range(50)],\n",
    "    'age': np.random.randint(20, 60, 50).astype(float),\n",
    "    'income': np.random.lognormal(10, 0.5, 50),\n",
    "    'signup_date': pd.date_range('2024-01-01', periods=50, freq='D'),\n",
    "    'country': np.random.choice(['US', 'UK'], 50)\n",
    "})\n",
    "incr_result = loader.incremental_load('customers', new_customers, 'customer_id')\n",
    "print(f\"Result: {incr_result}\")\n",
    "\n",
    "# Upsert\n",
    "print(\"\\n--- UPSERT ---\")\n",
    "updates = clean_data.head(20).copy()\n",
    "updates['income'] = updates['income'] * 1.1\n",
    "upsert_result = loader.upsert('customers', updates, 'customer_id')\n",
    "print(f\"Result: {upsert_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Complete ETL Pipeline <a id='5-complete-pipeline'></a>\n",
    "\n",
    "Now let's build a complete ETL pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete ETL Pipeline\n",
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    name: str\n",
    "    target_table: str\n",
    "    load_strategy: str = 'full'\n",
    "    key_column: Optional[str] = None\n",
    "\n",
    "\n",
    "class ETLPipeline:\n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.metrics = {'status': 'pending', 'errors': []}\n",
    "    \n",
    "    def run(self, extractor: BaseExtractor, loader: DataLoader, \n",
    "            transform_func: Callable[[pd.DataFrame], pd.DataFrame] = None) -> Dict[str, Any]:\n",
    "        \n",
    "        self.metrics['start_time'] = datetime.now()\n",
    "        self.metrics['status'] = 'running'\n",
    "        \n",
    "        logger.info(f\"Starting pipeline: {self.config.name}\")\n",
    "        \n",
    "        try:\n",
    "            # EXTRACT\n",
    "            logger.info(\"[EXTRACT] Starting...\")\n",
    "            raw_data = extractor.extract()\n",
    "            self.metrics['extract_records'] = len(raw_data)\n",
    "            \n",
    "            # TRANSFORM\n",
    "            logger.info(\"[TRANSFORM] Starting...\")\n",
    "            if transform_func:\n",
    "                clean_data = transform_func(raw_data)\n",
    "            else:\n",
    "                clean_data = raw_data\n",
    "            self.metrics['transform_records'] = len(clean_data)\n",
    "            \n",
    "            # LOAD\n",
    "            logger.info(\"[LOAD] Starting...\")\n",
    "            if self.config.load_strategy == 'full':\n",
    "                load_result = loader.full_load(self.config.target_table, clean_data)\n",
    "            elif self.config.load_strategy == 'incremental':\n",
    "                load_result = loader.incremental_load(self.config.target_table, clean_data, self.config.key_column)\n",
    "            elif self.config.load_strategy == 'upsert':\n",
    "                load_result = loader.upsert(self.config.target_table, clean_data, self.config.key_column)\n",
    "            \n",
    "            self.metrics['load_records'] = load_result.get('rows_after') or load_result.get('rows_total', 0)\n",
    "            self.metrics['status'] = 'success'\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.metrics['status'] = 'failed'\n",
    "            self.metrics['errors'].append(str(e))\n",
    "            logger.error(f\"Pipeline failed: {e}\")\n",
    "        \n",
    "        self.metrics['end_time'] = datetime.now()\n",
    "        self.metrics['duration_seconds'] = (self.metrics['end_time'] - self.metrics['start_time']).total_seconds()\n",
    "        \n",
    "        logger.info(f\"Pipeline {self.metrics['status']}: {self.metrics['duration_seconds']:.2f}s\")\n",
    "        return self.metrics\n",
    "\n",
    "print(\"ETLPipeline class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Complete Pipeline\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPLETE ETL PIPELINE DEMO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def transform_customers(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    transformer = DataTransformer(df)\n",
    "    result = (\n",
    "        transformer\n",
    "        .remove_duplicates(subset=['customer_id'])\n",
    "        .handle_missing({'name': 'Unknown', 'country': 'mode'})\n",
    "        .remove_outliers(['lifetime_value'], method='iqr', factor=2)\n",
    "        .add_date_features('created_at')\n",
    "        .get_result()\n",
    "    )\n",
    "    return result.data\n",
    "\n",
    "# Configure pipeline\n",
    "config = PipelineConfig(\n",
    "    name='customer_etl',\n",
    "    target_table='dim_customers',\n",
    "    load_strategy='full'\n",
    ")\n",
    "\n",
    "# Create components\n",
    "extractor = BatchExtractor('customer_db', generate_customer_data)\n",
    "loader = DataLoader('analytics_warehouse')\n",
    "pipeline = ETLPipeline(config)\n",
    "\n",
    "# Run pipeline\n",
    "metrics = pipeline.run(extractor, loader, transform_customers)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PIPELINE METRICS\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in metrics.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Data Quality and Validation <a id='6-data-quality'></a>\n",
    "\n",
    "Data quality is crucial for ML systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality Validator\n",
    "class DataQualityValidator:\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df\n",
    "        self.results = []\n",
    "    \n",
    "    def check_nulls(self, max_null_pct: float = 0.05) -> 'DataQualityValidator':\n",
    "        for col in self.df.columns:\n",
    "            null_pct = self.df[col].isnull().mean()\n",
    "            passed = null_pct <= max_null_pct\n",
    "            self.results.append({\n",
    "                'check': 'null_check',\n",
    "                'column': col,\n",
    "                'value': f\"{null_pct:.2%}\",\n",
    "                'threshold': f\"{max_null_pct:.2%}\",\n",
    "                'passed': passed\n",
    "            })\n",
    "        return self\n",
    "    \n",
    "    def check_unique(self, columns: List[str]) -> 'DataQualityValidator':\n",
    "        for col in columns:\n",
    "            if col not in self.df.columns:\n",
    "                continue\n",
    "            is_unique = self.df[col].nunique() == len(self.df)\n",
    "            self.results.append({\n",
    "                'check': 'uniqueness',\n",
    "                'column': col,\n",
    "                'value': f\"{self.df[col].nunique()}/{len(self.df)}\",\n",
    "                'threshold': 'all unique',\n",
    "                'passed': is_unique\n",
    "            })\n",
    "        return self\n",
    "    \n",
    "    def check_range(self, column: str, min_val: float, max_val: float) -> 'DataQualityValidator':\n",
    "        if column not in self.df.columns:\n",
    "            return self\n",
    "        in_range = ((self.df[column] >= min_val) & (self.df[column] <= max_val)).all()\n",
    "        actual_min = self.df[column].min()\n",
    "        actual_max = self.df[column].max()\n",
    "        self.results.append({\n",
    "            'check': 'range',\n",
    "            'column': column,\n",
    "            'value': f\"[{actual_min:.2f}, {actual_max:.2f}]\",\n",
    "            'threshold': f\"[{min_val}, {max_val}]\",\n",
    "            'passed': in_range\n",
    "        })\n",
    "        return self\n",
    "    \n",
    "    def get_report(self) -> pd.DataFrame:\n",
    "        return pd.DataFrame(self.results)\n",
    "    \n",
    "    def all_passed(self) -> bool:\n",
    "        return all(r['passed'] for r in self.results)\n",
    "\n",
    "\n",
    "# Run validation\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA QUALITY VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "clean_data = loader.get_table('dim_customers')\n",
    "\n",
    "validator = DataQualityValidator(clean_data)\n",
    "report = (\n",
    "    validator\n",
    "    .check_nulls(max_null_pct=0.05)\n",
    "    .check_unique(['customer_id'])\n",
    "    .check_range('lifetime_value', 0, 100000)\n",
    "    .get_report()\n",
    ")\n",
    "\n",
    "print(\"\\nValidation Report:\")\n",
    "print(report.to_string(index=False))\n",
    "print(f\"\\nAll checks passed: {validator.all_passed()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Hands-on Exercise <a id='7-exercise'></a>\n",
    "\n",
    "Build your own ETL pipeline for transaction data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Build a Transaction ETL Pipeline\n",
    "\n",
    "def generate_transactions() -> pd.DataFrame:\n",
    "    np.random.seed(42)\n",
    "    n = 2000\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'transaction_id': [f'TXN_{i:08d}' for i in range(n)],\n",
    "        'customer_id': [f'CUST_{np.random.randint(0, 500):05d}' for _ in range(n)],\n",
    "        'amount': np.random.lognormal(4, 1, n),\n",
    "        'currency': np.random.choice(['USD', 'EUR', 'GBP', None], n, p=[0.6, 0.2, 0.15, 0.05]),\n",
    "        'category': np.random.choice(['electronics', 'clothing', 'food', 'services'], n),\n",
    "        'timestamp': pd.date_range('2024-01-01', periods=n, freq='30min'),\n",
    "        'status': np.random.choice(['completed', 'pending', 'failed'], n, p=[0.85, 0.10, 0.05])\n",
    "    })\n",
    "    \n",
    "    # Add duplicates\n",
    "    df = pd.concat([df, df.iloc[:30]], ignore_index=True)\n",
    "    # Add outliers\n",
    "    df.loc[np.random.choice(len(df), 10), 'amount'] = np.random.uniform(100000, 1000000, 10)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# YOUR TASK: Complete the transform function\n",
    "def transform_transactions(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    transformer = DataTransformer(df)\n",
    "    result = (\n",
    "        transformer\n",
    "        .remove_duplicates(subset=['transaction_id'])\n",
    "        .handle_missing({'currency': 'USD'})\n",
    "        .remove_outliers(['amount'], method='iqr', factor=3)\n",
    "        .add_date_features('timestamp')\n",
    "        .add_column('amount_usd', lambda x: x['amount'])  # Simplified - no conversion\n",
    "        .get_result()\n",
    "    )\n",
    "    return result.data\n",
    "\n",
    "\n",
    "# Run the pipeline\n",
    "print(\"=\" * 60)\n",
    "print(\"EXERCISE: TRANSACTION ETL PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "txn_config = PipelineConfig(\n",
    "    name='transaction_etl',\n",
    "    target_table='fact_transactions',\n",
    "    load_strategy='full'\n",
    ")\n",
    "\n",
    "txn_extractor = BatchExtractor('transactions_db', generate_transactions)\n",
    "txn_loader = DataLoader('analytics_warehouse')\n",
    "txn_pipeline = ETLPipeline(txn_config)\n",
    "\n",
    "txn_metrics = txn_pipeline.run(txn_extractor, txn_loader, transform_transactions)\n",
    "\n",
    "print(\"\\nPipeline Results:\")\n",
    "for key, value in txn_metrics.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Validate\n",
    "txn_data = txn_loader.get_table('fact_transactions')\n",
    "print(f\"\\nFinal data shape: {txn_data.shape}\")\n",
    "print(txn_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Summary and Key Takeaways <a id='8-summary'></a>\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Extract Phase**\n",
    "   - Batch extraction for periodic loads\n",
    "   - API extraction for external data\n",
    "   - Stream extraction for real-time data\n",
    "\n",
    "2. **Transform Phase**\n",
    "   - Data cleaning (duplicates, missing values, outliers)\n",
    "   - Data validation (type checking, range validation)\n",
    "   - Data enrichment (derived columns, date features)\n",
    "\n",
    "3. **Load Phase**\n",
    "   - Full load for complete replacement\n",
    "   - Incremental load for appending\n",
    "   - Upsert for insert-or-update\n",
    "\n",
    "4. **Data Quality**\n",
    "   - Null checks, uniqueness checks, range validation\n",
    "   - Automated quality gates\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- Design for idempotency\n",
    "- Add comprehensive logging\n",
    "- Implement data quality checks\n",
    "- Use incremental loads when possible\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In Tutorial 06, we'll learn about Feature Engineering Operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TUTORIAL 05 COMPLETE: ETL Pipelines\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nKey topics covered:\")\n",
    "print(\"  1. Extract (batch, API, streaming)\")\n",
    "print(\"  2. Transform (cleaning, validation, enrichment)\")\n",
    "print(\"  3. Load (full, incremental, upsert)\")\n",
    "print(\"  4. Data Quality Validation\")\n",
    "print(\"  5. Complete ETL Pipeline\")\n",
    "print(\"\\nNext: Tutorial 06 - Feature Engineering Operations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
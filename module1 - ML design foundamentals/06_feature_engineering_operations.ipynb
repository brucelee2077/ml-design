{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 06: Feature Engineering Operations\n",
    "\n",
    "## Module 3: Data Preparation\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "\n",
    "1. **Master techniques for handling missing values** (deletion, imputation methods)\n",
    "2. **Apply various feature scaling methods** (normalization, standardization, log scaling)\n",
    "3. **Implement discretization and bucketing** strategies\n",
    "4. **Encode categorical variables effectively** (one-hot, target encoding, embeddings)\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction to Feature Engineering](#1-introduction)\n",
    "2. [Handling Missing Values](#2-missing-values)\n",
    "3. [Feature Scaling](#3-feature-scaling)\n",
    "4. [Discretization and Bucketing](#4-discretization)\n",
    "5. [Encoding Categorical Features](#5-encoding)\n",
    "6. [Feature Engineering Pipeline](#6-pipeline)\n",
    "7. [Hands-on Exercise](#7-exercise)\n",
    "8. [Summary and Key Takeaways](#8-summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Introduction to Feature Engineering <a id='1-introduction'></a>\n",
    "\n",
    "Feature engineering transforms raw data into features that better represent the problem.\n",
    "\n",
    "### Why Feature Engineering Matters\n",
    "\n",
    "- **Better features = Better models**: Good features make simple models perform well\n",
    "- **Domain knowledge**: Encodes expert understanding into the model\n",
    "- **Data quality**: Handles missing values, outliers, and inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample dataset\n",
    "def create_sample_dataset(n_samples: int = 1000) -> pd.DataFrame:\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'age': np.random.normal(40, 15, n_samples).clip(18, 85),\n",
    "        'income': np.random.lognormal(10.5, 0.8, n_samples),\n",
    "        'credit_score': np.random.normal(700, 100, n_samples).clip(300, 850),\n",
    "        'years_employed': np.random.exponential(5, n_samples).clip(0, 40),\n",
    "        'num_products': np.random.poisson(3, n_samples),\n",
    "        'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], n_samples, p=[0.3, 0.4, 0.2, 0.1]),\n",
    "        'employment_type': np.random.choice(['Full-time', 'Part-time', 'Self-employed', 'Unemployed'], n_samples, p=[0.6, 0.15, 0.2, 0.05]),\n",
    "        'region': np.random.choice(['North', 'South', 'East', 'West', 'Central'], n_samples),\n",
    "        'customer_segment': np.random.choice(['Bronze', 'Silver', 'Gold', 'Platinum'], n_samples, p=[0.5, 0.3, 0.15, 0.05]),\n",
    "        'churned': np.random.choice([0, 1], n_samples, p=[0.85, 0.15])\n",
    "    })\n",
    "    \n",
    "    # Add missing values\n",
    "    for col in ['age', 'income', 'credit_score', 'education', 'region']:\n",
    "        mask = np.random.random(n_samples) < 0.1\n",
    "        df.loc[mask, col] = np.nan\n",
    "    \n",
    "    # Add outliers\n",
    "    outlier_idx = np.random.choice(n_samples, 20, replace=False)\n",
    "    df.loc[outlier_idx, 'income'] = np.random.uniform(1e6, 5e6, 20)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = create_sample_dataset(1000)\n",
    "print(\"Sample Dataset:\")\n",
    "print(df.head())\n",
    "print(f\"\\nShape: {df.shape}\")\n",
    "print(f\"\\nMissing Values:\\n{df.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Handling Missing Values <a id='2-missing-values'></a>\n",
    "\n",
    "| Strategy | When to Use | Pros | Cons |\n",
    "|----------|-------------|------|------|\n",
    "| **Deletion** | MCAR, few missing | Simple | Loses data |\n",
    "| **Mean/Median** | Numerical, MCAR | Fast | Ignores relationships |\n",
    "| **Mode** | Categorical | Simple | May not represent well |\n",
    "| **KNN** | Complex patterns | Captures relationships | Slow |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Value Handler\n",
    "class MissingValueHandler:\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.original_df = df.copy()\n",
    "        self.df = df.copy()\n",
    "    \n",
    "    def analyze(self) -> pd.DataFrame:\n",
    "        analysis = []\n",
    "        for col in self.df.columns:\n",
    "            missing_count = self.df[col].isnull().sum()\n",
    "            missing_pct = missing_count / len(self.df) * 100\n",
    "            analysis.append({'column': col, 'missing_count': missing_count, 'missing_pct': f\"{missing_pct:.1f}%\"})\n",
    "        return pd.DataFrame(analysis)\n",
    "    \n",
    "    def impute_mean(self, columns: List[str]) -> 'MissingValueHandler':\n",
    "        for col in columns:\n",
    "            if col in self.df.columns and pd.api.types.is_numeric_dtype(self.df[col]):\n",
    "                self.df[col] = self.df[col].fillna(self.df[col].mean())\n",
    "        return self\n",
    "    \n",
    "    def impute_median(self, columns: List[str]) -> 'MissingValueHandler':\n",
    "        for col in columns:\n",
    "            if col in self.df.columns and pd.api.types.is_numeric_dtype(self.df[col]):\n",
    "                self.df[col] = self.df[col].fillna(self.df[col].median())\n",
    "        return self\n",
    "    \n",
    "    def impute_mode(self, columns: List[str]) -> 'MissingValueHandler':\n",
    "        for col in columns:\n",
    "            if col in self.df.columns:\n",
    "                self.df[col] = self.df[col].fillna(self.df[col].mode()[0])\n",
    "        return self\n",
    "    \n",
    "    def impute_knn(self, columns: List[str], n_neighbors: int = 5) -> 'MissingValueHandler':\n",
    "        num_cols = [c for c in columns if c in self.df.columns and pd.api.types.is_numeric_dtype(self.df[c])]\n",
    "        if num_cols:\n",
    "            imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "            self.df[num_cols] = imputer.fit_transform(self.df[num_cols])\n",
    "        return self\n",
    "    \n",
    "    def get_result(self) -> pd.DataFrame:\n",
    "        return self.df\n",
    "\n",
    "# Demo\n",
    "print(\"Missing Value Analysis:\")\n",
    "handler = MissingValueHandler(df)\n",
    "print(handler.analyze().to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare imputation strategies\n",
    "print(\"\\nComparing Imputation Strategies:\")\n",
    "\n",
    "original_income = df['income'].dropna()\n",
    "\n",
    "df_mean = df.copy()\n",
    "df_mean['income'] = df_mean['income'].fillna(df_mean['income'].mean())\n",
    "\n",
    "df_median = df.copy()\n",
    "df_median['income'] = df_median['income'].fillna(df_median['income'].median())\n",
    "\n",
    "print(f\"Original: mean={original_income.mean():.2f}, std={original_income.std():.2f}\")\n",
    "print(f\"Mean Imp: mean={df_mean['income'].mean():.2f}, std={df_mean['income'].std():.2f}\")\n",
    "print(f\"Median Imp: mean={df_median['income'].mean():.2f}, std={df_median['income'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize imputation effects\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].hist(original_income, bins=50, alpha=0.7, color='blue')\n",
    "axes[0].set_title('Original (Non-null)')\n",
    "\n",
    "axes[1].hist(df_mean['income'], bins=50, alpha=0.7, color='green')\n",
    "axes[1].set_title('Mean Imputation')\n",
    "\n",
    "axes[2].hist(df_median['income'], bins=50, alpha=0.7, color='orange')\n",
    "axes[2].set_title('Median Imputation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Feature Scaling <a id='3-feature-scaling'></a>\n",
    "\n",
    "| Method | Formula | Range | Best For |\n",
    "|--------|---------|-------|----------|\n",
    "| **StandardScaler** | (x - mean) / std | Unbounded | Normal distributions |\n",
    "| **MinMaxScaler** | (x - min) / (max - min) | [0, 1] | Bounded features |\n",
    "| **RobustScaler** | (x - median) / IQR | Unbounded | Data with outliers |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare clean data\n",
    "num_cols = ['age', 'income', 'credit_score', 'years_employed']\n",
    "df_clean = df.copy()\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df_clean[num_cols] = imputer.fit_transform(df_clean[num_cols])\n",
    "\n",
    "# Apply different scalers\n",
    "income = df_clean['income'].values.reshape(-1, 1)\n",
    "\n",
    "income_standard = StandardScaler().fit_transform(income)\n",
    "income_minmax = MinMaxScaler().fit_transform(income)\n",
    "income_robust = RobustScaler().fit_transform(income)\n",
    "income_log = np.log1p(income)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "axes[0, 0].hist(income, bins=50, alpha=0.7, color='blue')\n",
    "axes[0, 0].set_title(f'Original\\nMean: {income.mean():.0f}')\n",
    "\n",
    "axes[0, 1].hist(income_standard, bins=50, alpha=0.7, color='green')\n",
    "axes[0, 1].set_title(f'StandardScaler\\nMean: {income_standard.mean():.2f}')\n",
    "\n",
    "axes[1, 0].hist(income_minmax, bins=50, alpha=0.7, color='orange')\n",
    "axes[1, 0].set_title(f'MinMaxScaler\\nRange: [{income_minmax.min():.2f}, {income_minmax.max():.2f}]')\n",
    "\n",
    "axes[1, 1].hist(income_log, bins=50, alpha=0.7, color='red')\n",
    "axes[1, 1].set_title(f'Log Transform\\nMean: {income_log.mean():.2f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impact of scaling on model performance\n",
    "print(\"Impact of Scaling on Logistic Regression:\")\n",
    "\n",
    "X = df_clean[num_cols].copy()\n",
    "y = df_clean['churned']\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "\n",
    "results = []\n",
    "for name, scaler in [('No Scaling', None), ('Standard', StandardScaler()), ('MinMax', MinMaxScaler()), ('Robust', RobustScaler())]:\n",
    "    X_scaled = scaler.fit_transform(X) if scaler else X\n",
    "    scores = cross_val_score(lr, X_scaled, y, cv=5)\n",
    "    results.append({'Method': name, 'Mean CV Score': f\"{scores.mean():.4f}\", 'Std': f\"{scores.std():.4f}\"})\n",
    "\n",
    "print(pd.DataFrame(results).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Discretization and Bucketing <a id='4-discretization'></a>\n",
    "\n",
    "| Method | Description | Use Case |\n",
    "|--------|-------------|----------|\n",
    "| **Equal-width** | Same range per bin | Uniform distribution |\n",
    "| **Equal-frequency** | Same count per bin | Skewed distribution |\n",
    "| **Custom** | Domain-specific bins | Business rules |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretization demo\n",
    "df_binned = df_clean.copy()\n",
    "\n",
    "# Age: Custom bins\n",
    "df_binned['age_bin'] = pd.cut(df_binned['age'], bins=[0, 25, 35, 50, 65, 100],\n",
    "                               labels=['Young Adult', 'Adult', 'Middle Age', 'Senior', 'Elderly'])\n",
    "\n",
    "# Income: Equal-frequency bins\n",
    "df_binned['income_bin'] = pd.qcut(df_binned['income'], q=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    "# Credit Score: Custom bins\n",
    "df_binned['credit_bin'] = pd.cut(df_binned['credit_score'], bins=[300, 580, 670, 740, 800, 850],\n",
    "                                  labels=['Poor', 'Fair', 'Good', 'Very Good', 'Excellent'])\n",
    "\n",
    "print(\"Discretized Features:\")\n",
    "print(df_binned[['age', 'age_bin', 'income', 'income_bin', 'credit_score', 'credit_bin']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize bins\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "df_binned['age'].hist(bins=30, ax=axes[0, 0], color='steelblue', alpha=0.7)\n",
    "axes[0, 0].set_title('Age Distribution')\n",
    "\n",
    "df_binned['age_bin'].value_counts().plot(kind='bar', ax=axes[0, 1], color='coral')\n",
    "axes[0, 1].set_title('Age Bins')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "df_binned['credit_score'].hist(bins=30, ax=axes[1, 0], color='green', alpha=0.7)\n",
    "axes[1, 0].set_title('Credit Score Distribution')\n",
    "\n",
    "order = ['Poor', 'Fair', 'Good', 'Very Good', 'Excellent']\n",
    "df_binned['credit_bin'].value_counts().reindex(order).plot(kind='bar', ax=axes[1, 1], color='purple')\n",
    "axes[1, 1].set_title('Credit Score Bins')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Encoding Categorical Features <a id='5-encoding'></a>\n",
    "\n",
    "| Method | Description | Best For |\n",
    "|--------|-------------|----------|\n",
    "| **Label Encoding** | Integer values | Ordinal, tree models |\n",
    "| **One-Hot Encoding** | Binary columns | Nominal, linear models |\n",
    "| **Target Encoding** | Mean target per category | High cardinality |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Demo\n",
    "df_encoded = df_clean.copy()\n",
    "\n",
    "# Fill missing categorical values\n",
    "for col in ['education', 'region']:\n",
    "    df_encoded[col] = df_encoded[col].fillna(df_encoded[col].mode()[0])\n",
    "\n",
    "# Label Encoding\n",
    "le = LabelEncoder()\n",
    "df_encoded['region_label'] = le.fit_transform(df_encoded['region'])\n",
    "\n",
    "# Ordinal Encoding for education\n",
    "edu_order = {'High School': 0, 'Bachelor': 1, 'Master': 2, 'PhD': 3}\n",
    "df_encoded['education_ordinal'] = df_encoded['education'].map(edu_order)\n",
    "\n",
    "# One-Hot Encoding\n",
    "employment_dummies = pd.get_dummies(df_encoded['employment_type'], prefix='emp')\n",
    "df_encoded = pd.concat([df_encoded, employment_dummies], axis=1)\n",
    "\n",
    "# Target Encoding\n",
    "target_means = df_encoded.groupby('region')['churned'].mean()\n",
    "df_encoded['region_target'] = df_encoded['region'].map(target_means)\n",
    "\n",
    "# Frequency Encoding\n",
    "freq = df_encoded['region'].value_counts(normalize=True)\n",
    "df_encoded['region_freq'] = df_encoded['region'].map(freq)\n",
    "\n",
    "print(\"Encoded Features:\")\n",
    "cols_to_show = ['region', 'region_label', 'region_target', 'region_freq', 'education', 'education_ordinal']\n",
    "print(df_encoded[cols_to_show].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize encoding\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "df_encoded['region'].value_counts().plot(kind='bar', ax=axes[0], color='steelblue')\n",
    "axes[0].set_title('Original Region Distribution')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "target_means.sort_values().plot(kind='bar', ax=axes[1], color='coral')\n",
    "axes[1].set_title('Target Encoded (Churn Rate)')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "employment_dummies.sum().plot(kind='bar', ax=axes[2], color='green')\n",
    "axes[2].set_title('One-Hot: Employment Type')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare encoding impact\n",
    "print(\"\\nEncoding Impact on Model Performance:\")\n",
    "\n",
    "y = df_encoded['churned']\n",
    "\n",
    "# Label encoding\n",
    "X_label = df_encoded[['age', 'income', 'credit_score', 'years_employed']].copy()\n",
    "for col in ['education', 'employment_type', 'region', 'customer_segment']:\n",
    "    le = LabelEncoder()\n",
    "    X_label[col] = le.fit_transform(df_encoded[col])\n",
    "\n",
    "# One-hot encoding\n",
    "X_onehot = df_encoded[['age', 'income', 'credit_score', 'years_employed']].copy()\n",
    "for col in ['education', 'employment_type', 'region', 'customer_segment']:\n",
    "    dummies = pd.get_dummies(df_encoded[col], prefix=col)\n",
    "    X_onehot = pd.concat([X_onehot, dummies], axis=1)\n",
    "\n",
    "results = []\n",
    "for name, X in [('Label Encoding', X_label), ('One-Hot Encoding', X_onehot)]:\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "    scores = cross_val_score(LogisticRegression(max_iter=1000), X_scaled, y, cv=5)\n",
    "    results.append({'Encoding': name, 'Mean CV': f\"{scores.mean():.4f}\", 'Std': f\"{scores.std():.4f}\"})\n",
    "\n",
    "print(pd.DataFrame(results).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Feature Engineering Pipeline <a id='6-pipeline'></a>\n",
    "\n",
    "Combining all operations into a complete pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Pipeline\n",
    "class FeatureEngineeringPipeline:\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "        self.steps = []\n",
    "    \n",
    "    def handle_missing(self, num_strategy: str = 'median', cat_strategy: str = 'mode') -> 'FeatureEngineeringPipeline':\n",
    "        for col in self.df.columns:\n",
    "            if self.df[col].isnull().sum() > 0:\n",
    "                if pd.api.types.is_numeric_dtype(self.df[col]):\n",
    "                    fill_val = self.df[col].median() if num_strategy == 'median' else self.df[col].mean()\n",
    "                    self.df[col] = self.df[col].fillna(fill_val)\n",
    "                else:\n",
    "                    self.df[col] = self.df[col].fillna(self.df[col].mode()[0] if cat_strategy == 'mode' else 'Unknown')\n",
    "        self.steps.append('handle_missing')\n",
    "        return self\n",
    "    \n",
    "    def remove_outliers(self, columns: List[str], factor: float = 3) -> 'FeatureEngineeringPipeline':\n",
    "        for col in columns:\n",
    "            if col in self.df.columns and pd.api.types.is_numeric_dtype(self.df[col]):\n",
    "                Q1, Q3 = self.df[col].quantile([0.25, 0.75])\n",
    "                IQR = Q3 - Q1\n",
    "                self.df = self.df[(self.df[col] >= Q1 - factor*IQR) & (self.df[col] <= Q3 + factor*IQR)]\n",
    "        self.steps.append('remove_outliers')\n",
    "        return self\n",
    "    \n",
    "    def scale(self, columns: List[str], method: str = 'standard') -> 'FeatureEngineeringPipeline':\n",
    "        scaler = StandardScaler() if method == 'standard' else MinMaxScaler() if method == 'minmax' else RobustScaler()\n",
    "        cols = [c for c in columns if c in self.df.columns]\n",
    "        if cols:\n",
    "            self.df[cols] = scaler.fit_transform(self.df[cols])\n",
    "        self.steps.append(f'scale_{method}')\n",
    "        return self\n",
    "    \n",
    "    def encode(self, columns: List[str], method: str = 'onehot') -> 'FeatureEngineeringPipeline':\n",
    "        for col in columns:\n",
    "            if col not in self.df.columns:\n",
    "                continue\n",
    "            if method == 'onehot':\n",
    "                dummies = pd.get_dummies(self.df[col], prefix=col)\n",
    "                self.df = pd.concat([self.df.drop(columns=[col]), dummies], axis=1)\n",
    "            elif method == 'label':\n",
    "                self.df[col] = LabelEncoder().fit_transform(self.df[col].astype(str))\n",
    "        self.steps.append(f'encode_{method}')\n",
    "        return self\n",
    "    \n",
    "    def get_result(self) -> Tuple[pd.DataFrame, List[str]]:\n",
    "        return self.df, self.steps\n",
    "\n",
    "print(\"FeatureEngineeringPipeline defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run complete pipeline\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPLETE PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "pipeline = FeatureEngineeringPipeline(df)\n",
    "\n",
    "df_processed, steps = (\n",
    "    pipeline\n",
    "    .handle_missing(num_strategy='median', cat_strategy='mode')\n",
    "    .remove_outliers(['income', 'years_employed'], factor=3)\n",
    "    .scale(['age', 'income', 'credit_score', 'years_employed'], method='standard')\n",
    "    .encode(['education', 'employment_type', 'region', 'customer_segment'], method='onehot')\n",
    "    .get_result()\n",
    ")\n",
    "\n",
    "print(f\"\\nOriginal shape: {df.shape}\")\n",
    "print(f\"Processed shape: {df_processed.shape}\")\n",
    "print(f\"\\nSteps applied: {steps}\")\n",
    "print(f\"\\nColumns: {list(df_processed.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Hands-on Exercise <a id='7-exercise'></a>\n",
    "\n",
    "Apply feature engineering to a loan dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Loan Dataset\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "\n",
    "loan_df = pd.DataFrame({\n",
    "    'loan_amount': np.random.lognormal(10, 0.5, n),\n",
    "    'interest_rate': np.random.uniform(5, 25, n),\n",
    "    'term_months': np.random.choice([12, 24, 36, 48, 60], n),\n",
    "    'annual_income': np.random.lognormal(11, 0.6, n),\n",
    "    'debt_to_income': np.random.uniform(0.1, 0.6, n),\n",
    "    'credit_history': np.random.choice(['Short', 'Medium', 'Long'], n, p=[0.3, 0.4, 0.3]),\n",
    "    'home_ownership': np.random.choice(['Rent', 'Own', 'Mortgage', 'Other'], n, p=[0.35, 0.2, 0.4, 0.05]),\n",
    "    'purpose': np.random.choice(['Debt Consolidation', 'Home Improvement', 'Medical', 'Education', 'Other'], n),\n",
    "    'default': np.random.choice([0, 1], n, p=[0.88, 0.12])\n",
    "})\n",
    "\n",
    "# Add missing values\n",
    "for col in ['annual_income', 'debt_to_income', 'credit_history']:\n",
    "    mask = np.random.random(n) < 0.08\n",
    "    loan_df.loc[mask, col] = np.nan\n",
    "\n",
    "print(\"Loan Dataset:\")\n",
    "print(loan_df.head())\n",
    "print(f\"\\nMissing: {loan_df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TASK: Apply feature engineering\n",
    "loan_pipeline = FeatureEngineeringPipeline(loan_df)\n",
    "\n",
    "loan_processed, loan_steps = (\n",
    "    loan_pipeline\n",
    "    .handle_missing()\n",
    "    .remove_outliers(['loan_amount', 'annual_income'], factor=3)\n",
    "    .scale(['loan_amount', 'interest_rate', 'annual_income', 'debt_to_income'], method='robust')\n",
    "    .encode(['credit_history', 'home_ownership', 'purpose'], method='onehot')\n",
    "    .get_result()\n",
    ")\n",
    "\n",
    "print(f\"Original: {loan_df.shape} -> Processed: {loan_processed.shape}\")\n",
    "print(f\"Steps: {loan_steps}\")\n",
    "\n",
    "# Train model\n",
    "X = loan_processed.drop(columns=['default'])\n",
    "y = loan_processed['default']\n",
    "\n",
    "scores = cross_val_score(RandomForestClassifier(n_estimators=100, random_state=42), X, y, cv=5)\n",
    "print(f\"\\nRandom Forest CV Score: {scores.mean():.4f} (+/- {scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Summary and Key Takeaways <a id='8-summary'></a>\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Missing Values**: Mean, median, mode, KNN imputation\n",
    "2. **Scaling**: StandardScaler, MinMaxScaler, RobustScaler\n",
    "3. **Discretization**: Equal-width, equal-frequency, custom bins\n",
    "4. **Encoding**: Label, one-hot, target, frequency encoding\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- Analyze data before choosing strategies\n",
    "- Use RobustScaler for data with outliers\n",
    "- Use target encoding for high-cardinality categories\n",
    "- Fit transformers on training data only\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Tutorial 07: Feature Engineering for Unstructured Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TUTORIAL 06 COMPLETE: Feature Engineering Operations\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nTopics covered:\")\n",
    "print(\"  1. Handling Missing Values\")\n",
    "print(\"  2. Feature Scaling\")\n",
    "print(\"  3. Discretization and Bucketing\")\n",
    "print(\"  4. Categorical Encoding\")\n",
    "print(\"  5. Complete Pipeline\")\n",
    "print(\"\\nNext: Tutorial 07 - Feature Engineering for Unstructured Data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
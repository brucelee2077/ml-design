
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 10: Model Training and Optimization\n",
    "\n",
    "## Module 4: Model Development\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "\n",
    "1. **Choose appropriate loss functions** for different ML tasks\n",
    "2. **Implement regularization techniques** to prevent overfitting\n",
    "3. **Apply optimization algorithms** effectively\n",
    "4. **Use training strategies** like learning rate scheduling\n",
    "5. **Decide between training from scratch vs fine-tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from typing import List, Tuple, Callable\n",
    "\n",
    "from sklearn.datasets import make_classification, make_regression, load_digits\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso, ElasticNet, SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, log_loss\n",
    "\n",
    "np.random.seed(42)\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: PyTorch for advanced examples\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "    TORCH_AVAILABLE = True\n",
    "    print(\"PyTorch available\")\n",
    "except ImportError:\n",
    "    TORCH_AVAILABLE = False\n",
    "    print(\"PyTorch not available - using sklearn alternatives\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loss Functions\n",
    "\n",
    "Loss functions measure how well a model's predictions match the actual values.\n",
    "\n",
    "### Loss Function Selection Guide\n",
    "\n",
    "| Task | Loss Function | When to Use |\n",
    "|------|---------------|-------------|\n",
    "| Binary Classification | Binary Cross-Entropy | Standard binary classification |\n",
    "| Multi-class Classification | Categorical Cross-Entropy | Mutually exclusive classes |\n",
    "| Regression | MSE | Standard regression, sensitive to outliers |\n",
    "| Regression | MAE | Robust to outliers |\n",
    "| Regression | Huber | Balance between MSE and MAE |\n",
    "| Ranking | Pairwise Loss | Learning to rank |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Classification Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunctions:\n",
    "    \"\"\"Common loss function implementations.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def binary_cross_entropy(y_true, y_pred, epsilon=1e-15):\n",
    "        \"\"\"Binary cross-entropy loss.\"\"\"\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    \n",
    "    @staticmethod\n",
    "    def categorical_cross_entropy(y_true, y_pred, epsilon=1e-15):\n",
    "        \"\"\"Categorical cross-entropy loss.\"\"\"\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "    \n",
    "    @staticmethod\n",
    "    def mse(y_true, y_pred):\n",
    "        \"\"\"Mean Squared Error.\"\"\"\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def mae(y_true, y_pred):\n",
    "        \"\"\"Mean Absolute Error.\"\"\"\n",
    "        return np.mean(np.abs(y_true - y_pred))\n",
    "    \n",
    "    @staticmethod\n",
    "    def huber_loss(y_true, y_pred, delta=1.0):\n",
    "        \"\"\"Huber loss - combines MSE and MAE.\"\"\"\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = np.abs(error) <= delta\n",
    "        squared_loss = 0.5 * error ** 2\n",
    "        linear_loss = delta * (np.abs(error) - 0.5 * delta)\n",
    "        return np.mean(np.where(is_small_error, squared_loss, linear_loss))\n",
    "    \n",
    "    @staticmethod\n",
    "    def hinge_loss(y_true, y_pred):\n",
    "        \"\"\"Hinge loss for SVM.\"\"\"\n",
    "        # y_true should be -1 or 1\n",
    "        return np.mean(np.maximum(0, 1 - y_true * y_pred))\n",
    "\n",
    "print(\"LossFunctions class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate loss functions\n",
    "y_true = np.array([1, 0, 1, 1, 0])\n",
    "y_pred_good = np.array([0.9, 0.1, 0.8, 0.95, 0.2])\n",
    "y_pred_bad = np.array([0.4, 0.6, 0.3, 0.5, 0.7])\n",
    "\n",
    "print(\"Binary Cross-Entropy Loss:\")\n",
    "print(f\"  Good predictions: {LossFunctions.binary_cross_entropy(y_true, y_pred_good):.4f}\")\n",
    "print(f\"  Bad predictions: {LossFunctions.binary_cross_entropy(y_true, y_pred_bad):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize BCE loss\n",
    "probs = np.linspace(0.01, 0.99, 100)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss when true label is 1\n",
    "loss_for_1 = -np.log(probs)\n",
    "axes[0].plot(probs, loss_for_1, 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('Predicted Probability')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('BCE Loss (True Label = 1)')\n",
    "axes[0].axvline(x=0.5, color='gray', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Loss when true label is 0\n",
    "loss_for_0 = -np.log(1 - probs)\n",
    "axes[1].plot(probs, loss_for_0, 'r-', linewidth=2)\n",
    "axes[1].set_xlabel('Predicted Probability')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('BCE Loss (True Label = 0)')\n",
    "axes[1].axvline(x=0.5, color='gray', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Regression Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare regression losses with outliers\n",
    "y_true_reg = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 100.0])  # Last one is outlier\n",
    "y_pred_reg = np.array([1.1, 2.2, 2.8, 4.1, 5.2, 10.0])\n",
    "\n",
    "print(\"Regression Loss Comparison (with outlier):\")\n",
    "print(f\"  MSE: {LossFunctions.mse(y_true_reg, y_pred_reg):.4f}\")\n",
    "print(f\"  MAE: {LossFunctions.mae(y_true_reg, y_pred_reg):.4f}\")\n",
    "print(f\"  Huber (delta=1): {LossFunctions.huber_loss(y_true_reg, y_pred_reg, delta=1.0):.4f}\")\n",
    "print(f\"  Huber (delta=10): {LossFunctions.huber_loss(y_true_reg, y_pred_reg, delta=10.0):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize regression losses\n",
    "errors = np.linspace(-5, 5, 100)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(errors, errors**2, 'b-', label='MSE (Squared)', linewidth=2)\n",
    "ax.plot(errors, np.abs(errors), 'r-', label='MAE (Absolute)', linewidth=2)\n",
    "\n",
    "# Huber loss\n",
    "delta = 1.0\n",
    "huber = np.where(np.abs(errors) <= delta, 0.5 * errors**2, delta * (np.abs(errors) - 0.5 * delta))\n",
    "ax.plot(errors, huber, 'g-', label=f'Huber (delta={delta})', linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Error (y_true - y_pred)')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Comparison of Regression Loss Functions')\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 10])\n",
    "ax.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Ranking Loss (Pairwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_ranking_loss(scores, relevance):\n",
    "    \"\"\"\n",
    "    Compute pairwise ranking loss.\n",
    "    Loss increases when a less relevant item is scored higher than a more relevant one.\n",
    "    \"\"\"\n",
    "    n = len(scores)\n",
    "    loss = 0\n",
    "    pairs = 0\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            if relevance[i] != relevance[j]:\n",
    "                # Item with higher relevance should have higher score\n",
    "                if relevance[i] > relevance[j]:\n",
    "                    diff = scores[j] - scores[i]  # Should be negative\n",
    "                else:\n",
    "                    diff = scores[i] - scores[j]  # Should be negative\n",
    "                \n",
    "                loss += max(0, 1 + diff)  # Hinge-like loss\n",
    "                pairs += 1\n",
    "    \n",
    "    return loss / pairs if pairs > 0 else 0\n",
    "\n",
    "# Example: Search ranking\n",
    "relevance = np.array([3, 2, 1, 0])  # Higher is more relevant\n",
    "good_scores = np.array([0.9, 0.7, 0.4, 0.1])  # Correctly ordered\n",
    "bad_scores = np.array([0.2, 0.5, 0.8, 0.9])   # Incorrectly ordered\n",
    "\n",
    "print(\"Pairwise Ranking Loss:\")\n",
    "print(f\"  Good ranking: {pairwise_ranking_loss(good_scores, relevance):.4f}\")\n",
    "print(f\"  Bad ranking: {pairwise_ranking_loss(bad_scores, relevance):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regularization Techniques\n",
    "\n",
    "Regularization prevents overfitting by adding constraints to the model.\n",
    "\n",
    "| Technique | Description | Effect |\n",
    "|-----------|-------------|--------|\n",
    "| **L1 (Lasso)** | Adds absolute value of weights | Sparse weights, feature selection |\n",
    "| **L2 (Ridge)** | Adds squared weights | Small weights, prevents extreme values |\n",
    "| **ElasticNet** | Combines L1 and L2 | Balance between sparsity and smoothness |\n",
    "| **Dropout** | Randomly zero activations | Prevents co-adaptation |\n",
    "| **Early Stopping** | Stop training before overfitting | Optimal complexity |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 L1, L2, and ElasticNet Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with some irrelevant features\n",
    "X, y = make_regression(n_samples=200, n_features=20, n_informative=5, noise=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Compare regularization methods\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "models = {\n",
    "    'No Regularization': LinearRegression(),\n",
    "    'L2 (Ridge)': Ridge(alpha=1.0),\n",
    "    'L1 (Lasso)': Lasso(alpha=0.1),\n",
    "    'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    train_score = model.score(X_train_scaled, y_train)\n",
    "    test_score = model.score(X_test_scaled, y_test)\n",
    "    n_nonzero = np.sum(np.abs(model.coef_) > 0.01)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Train R2': train_score,\n",
    "        'Test R2': test_score,\n",
    "        'Non-zero Coefs': n_nonzero\n",
    "    })\n",
    "\n",
    "print(\"Regularization Comparison:\")\n",
    "print(pd.DataFrame(results).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize coefficient magnitudes\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for ax, (name, model) in zip(axes.flat, models.items()):\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    coefs = model.coef_\n",
    "    \n",
    "    colors = ['steelblue' if c != 0 else 'lightgray' for c in coefs]\n",
    "    ax.bar(range(len(coefs)), np.abs(coefs), color=colors)\n",
    "    ax.set_xlabel('Feature Index')\n",
    "    ax.set_ylabel('|Coefficient|')\n",
    "    ax.set_title(f'{name} - {np.sum(np.abs(coefs) > 0.01)} non-zero')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization strength analysis\n",
    "alphas = np.logspace(-4, 2, 50)\n",
    "ridge_train, ridge_test = [], []\n",
    "lasso_train, lasso_test = [], []\n",
    "\n",
    "for alpha in alphas:\n",
    "    # Ridge\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train_scaled, y_train)\n",
    "    ridge_train.append(ridge.score(X_train_scaled, y_train))\n",
    "    ridge_test.append(ridge.score(X_test_scaled, y_test))\n",
    "    \n",
    "    # Lasso\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000)\n",
    "    lasso.fit(X_train_scaled, y_train)\n",
    "    lasso_train.append(lasso.score(X_train_scaled, y_train))\n",
    "    lasso_test.append(lasso.score(X_test_scaled, y_test))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].semilogx(alphas, ridge_train, 'b-', label='Train', linewidth=2)\n",
    "axes[0].semilogx(alphas, ridge_test, 'r-', label='Test', linewidth=2)\n",
    "axes[0].set_xlabel('Alpha (Regularization Strength)')\n",
    "axes[0].set_ylabel('R² Score')\n",
    "axes[0].set_title('Ridge Regularization')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].semilogx(alphas, lasso_train, 'b-', label='Train', linewidth=2)\n",
    "axes[1].semilogx(alphas, lasso_test, 'r-', label='Test', linewidth=2)\n",
    "axes[1].set_xlabel('Alpha (Regularization Strength)')\n",
    "axes[1].set_ylabel('R² Score')\n",
    "axes[1].set_title('Lasso Regularization')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_dropout(X, dropout_rate=0.5):\n",
    "    \"\"\"Apply dropout to activations.\"\"\"\n",
    "    mask = np.random.binomial(1, 1 - dropout_rate, X.shape)\n",
    "    # Scale by 1/(1-p) to maintain expected value\n",
    "    return X * mask / (1 - dropout_rate)\n",
    "\n",
    "# Demonstrate dropout\n",
    "activations = np.random.randn(5, 10)\n",
    "print(\"Original activations:\")\n",
    "print(activations[0])\n",
    "print(\"\\nAfter dropout (50%):\")\n",
    "print(apply_dropout(activations, 0.5)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare MLP with and without dropout-like regularization\n",
    "X_clf, y_clf = make_classification(n_samples=500, n_features=20, n_informative=10, random_state=42)\n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n",
    "    X_clf, y_clf, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "scaler_clf = StandardScaler()\n",
    "X_train_clf_s = scaler_clf.fit_transform(X_train_clf)\n",
    "X_test_clf_s = scaler_clf.transform(X_test_clf)\n",
    "\n",
    "# No regularization\n",
    "mlp_no_reg = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, alpha=0.0, random_state=42)\n",
    "mlp_no_reg.fit(X_train_clf_s, y_train_clf)\n",
    "\n",
    "# With L2 regularization (alpha parameter)\n",
    "mlp_with_reg = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, alpha=0.01, random_state=42)\n",
    "mlp_with_reg.fit(X_train_clf_s, y_train_clf)\n",
    "\n",
    "print(\"MLP Regularization Comparison:\")\n",
    "print(f\"No regularization - Train: {mlp_no_reg.score(X_train_clf_s, y_train_clf):.4f}, Test: {mlp_no_reg.score(X_test_clf_s, y_test_clf):.4f}\")\n",
    "print(f\"With L2 (alpha=0.01) - Train: {mlp_with_reg.score(X_train_clf_s, y_train_clf):.4f}, Test: {mlp_with_reg.score(X_test_clf_s, y_test_clf):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to prevent overfitting.\"\"\"\n",
    "    \n",
    "    def __init__(self, patience=5, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.should_stop = False\n",
    "    \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.should_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        \n",
    "        return self.should_stop\n",
    "\n",
    "# Simulate training with early stopping\n",
    "np.random.seed(42)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Simulated losses\n",
    "    train_loss = 1.0 / (1 + 0.1 * epoch) + np.random.randn() * 0.02\n",
    "    val_loss = 1.0 / (1 + 0.08 * epoch) + np.random.randn() * 0.02\n",
    "    if epoch > 30:\n",
    "        val_loss += 0.01 * (epoch - 30)  # Overfitting\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "# Apply early stopping\n",
    "early_stop = EarlyStopping(patience=5, min_delta=0.001)\n",
    "stop_epoch = len(val_losses)\n",
    "\n",
    "for epoch, val_loss in enumerate(val_losses):\n",
    "    if early_stop(val_loss):\n",
    "        stop_epoch = epoch\n",
    "        break\n",
    "\n",
    "print(f\"Early stopping triggered at epoch {stop_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize early stopping\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "ax.plot(val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "ax.axvline(x=stop_epoch, color='green', linestyle='--', label=f'Early Stop (epoch {stop_epoch})', linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Early Stopping Demonstration')\n",
    "ax.legend()\n",
    "ax.fill_between(range(stop_epoch, 100), 0, max(val_losses), alpha=0.2, color='red', label='Overfitting region')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimization Algorithms\n",
    "\n",
    "| Optimizer | Description | Best For |\n",
    "|-----------|-------------|----------|\n",
    "| **SGD** | Classic gradient descent | Simple, well-understood |\n",
    "| **Momentum** | SGD with momentum | Faster convergence |\n",
    "| **Adam** | Adaptive learning rates | Most problems, default choice |\n",
    "| **RMSprop** | Root mean square propagation | RNNs |\n",
    "| **AdaGrad** | Adaptive gradients | Sparse features |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    \"\"\"Base optimizer class.\"\"\"\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "    \n",
    "    def step(self, params, grads):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    \"\"\"Stochastic Gradient Descent.\"\"\"\n",
    "    def step(self, params, grads):\n",
    "        return params - self.lr * grads\n",
    "\n",
    "class MomentumSGD(Optimizer):\n",
    "    \"\"\"SGD with Momentum.\"\"\"\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        super().__init__(lr)\n",
    "        self.momentum = momentum\n",
    "        self.velocity = None\n",
    "    \n",
    "    def step(self, params, grads):\n",
    "        if self.velocity is None:\n",
    "            self.velocity = np.zeros_like(params)\n",
    "        self.velocity = self.momentum * self.velocity - self.lr * grads\n",
    "        return params + self.velocity\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    \"\"\"Adam optimizer.\"\"\"\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        super().__init__(lr)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.t = 0\n",
    "    \n",
    "    def step(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m = np.zeros_like(params)\n",
    "            self.v = np.zeros_like(params)\n",
    "        \n",
    "        self.t += 1\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * grads\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * (grads ** 2)\n",
    "        \n",
    "        m_hat = self.m / (1 - self.beta1 ** self.t)\n",
    "        v_hat = self.v / (1 - self.beta2 ** self.t)\n",
    "        \n",
    "        return params - self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "\n",
    "print(\"Optimizers defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare optimizers on a simple quadratic function\n",
    "def quadratic_loss(params):\n",
    "    \"\"\"f(x, y) = x^2 + 10*y^2\"\"\"\n",
    "    return params[0]**2 + 10 * params[1]**2\n",
    "\n",
    "def quadratic_grad(params):\n",
    "    return np.array([2 * params[0], 20 * params[1]])\n",
    "\n",
    "# Track optimization paths\n",
    "optimizers = {\n",
    "    'SGD': SGD(lr=0.05),\n",
    "    'Momentum': MomentumSGD(lr=0.05, momentum=0.9),\n",
    "    'Adam': Adam(lr=0.2)\n",
    "}\n",
    "\n",
    "paths = {}\n",
    "n_steps = 50\n",
    "start = np.array([5.0, 2.0])\n",
    "\n",
    "for name, opt in optimizers.items():\n",
    "    params = start.copy()\n",
    "    path = [params.copy()]\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        grads = quadratic_grad(params)\n",
    "        params = opt.step(params, grads)\n",
    "        path.append(params.copy())\n",
    "    \n",
    "    paths[name] = np.array(path)\n",
    "\n",
    "# Visualize optimization paths\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Contour plot\n",
    "x = np.linspace(-6, 6, 100)\n",
    "y = np.linspace(-3, 3, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = X**2 + 10 * Y**2\n",
    "\n",
    "ax.contour(X, Y, Z, levels=30, alpha=0.5)\n",
    "\n",
    "colors = {'SGD': 'blue', 'Momentum': 'red', 'Adam': 'green'}\n",
    "for name, path in paths.items():\n",
    "    ax.plot(path[:, 0], path[:, 1], 'o-', color=colors[name], label=name, markersize=3, linewidth=1.5)\n",
    "\n",
    "ax.plot(0, 0, 'k*', markersize=15, label='Optimum')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('Optimizer Comparison on f(x,y) = x² + 10y²')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare convergence speed\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for name, path in paths.items():\n",
    "    losses = [quadratic_loss(p) for p in path]\n",
    "    ax.plot(losses, label=name, linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Convergence Comparison')\n",
    "ax.legend()\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Strategies\n",
    "\n",
    "### 4.1 Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRScheduler:\n",
    "    \"\"\"Learning rate schedulers.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def step_decay(initial_lr, epoch, drop_rate=0.5, epochs_drop=10):\n",
    "        \"\"\"Step decay: reduce LR by factor every N epochs.\"\"\"\n",
    "        return initial_lr * (drop_rate ** (epoch // epochs_drop))\n",
    "    \n",
    "    @staticmethod\n",
    "    def exponential_decay(initial_lr, epoch, decay_rate=0.95):\n",
    "        \"\"\"Exponential decay.\"\"\"\n",
    "        return initial_lr * (decay_rate ** epoch)\n",
    "    \n",
    "    @staticmethod\n",
    "    def cosine_annealing(initial_lr, epoch, total_epochs):\n",
    "        \"\"\"Cosine annealing.\"\"\"\n",
    "        return initial_lr * (1 + np.cos(np.pi * epoch / total_epochs)) / 2\n",
    "    \n",
    "    @staticmethod\n",
    "    def warmup_linear(initial_lr, epoch, warmup_epochs=5, total_epochs=100):\n",
    "        \"\"\"Linear warmup followed by linear decay.\"\"\"\n",
    "        if epoch < warmup_epochs:\n",
    "            return initial_lr * epoch / warmup_epochs\n",
    "        else:\n",
    "            return initial_lr * (1 - (epoch - warmup_epochs) / (total_epochs - warmup_epochs))\n",
    "\n",
    "# Visualize schedulers\n",
    "epochs = np.arange(100)\n",
    "initial_lr = 0.1\n",
    "\n",
    "schedulers = {\n",
    "    'Step Decay': [LRScheduler.step_decay(initial_lr, e) for e in epochs],\n",
    "    'Exponential': [LRScheduler.exponential_decay(initial_lr, e) for e in epochs],\n",
    "    'Cosine Annealing': [LRScheduler.cosine_annealing(initial_lr, e, 100) for e in epochs],\n",
    "    'Warmup + Linear': [LRScheduler.warmup_linear(initial_lr, e) for e in epochs]\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for name, lrs in schedulers.items():\n",
    "    ax.plot(epochs, lrs, label=name, linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Learning Rate')\n",
    "ax.set_title('Learning Rate Schedulers')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Batch Size Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate effect of batch size\n",
    "batch_sizes = [8, 32, 128, 512]\n",
    "\n",
    "print(\"Batch Size Trade-offs:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Batch Size':<12} {'Gradient Noise':<18} {'Memory':<12} {'Speed':<12}\")\n",
    "print(\"-\" * 60)\n",
    "for bs in batch_sizes:\n",
    "    noise = 'High' if bs < 32 else 'Medium' if bs < 128 else 'Low'\n",
    "    memory = 'Low' if bs < 64 else 'Medium' if bs < 256 else 'High'\n",
    "    speed = 'Slow' if bs < 32 else 'Medium' if bs < 128 else 'Fast'\n",
    "    print(f\"{bs:<12} {noise:<18} {memory:<12} {speed:<12}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate batch size effect with SGDClassifier\n",
    "X, y = make_classification(n_samples=5000, n_features=20, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "# Different partial_fit iterations to simulate batch sizes\n",
    "import time\n",
    "\n",
    "results = []\n",
    "for n_iter in [10, 50, 100, 200]:\n",
    "    clf = SGDClassifier(max_iter=n_iter, tol=None, random_state=42)\n",
    "    \n",
    "    start = time.time()\n",
    "    clf.fit(X_train_s, y_train)\n",
    "    train_time = time.time() - start\n",
    "    \n",
    "    train_acc = clf.score(X_train_s, y_train)\n",
    "    test_acc = clf.score(X_test_s, y_test)\n",
    "    \n",
    "    results.append({\n",
    "        'Iterations': n_iter,\n",
    "        'Train Acc': train_acc,\n",
    "        'Test Acc': test_acc,\n",
    "        'Time (s)': train_time\n",
    "    })\n",
    "\n",
    "print(\"\\nTraining Iterations Effect:\")\n",
    "print(pd.DataFrame(results).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradients(grads, max_norm=1.0):\n",
    "    \"\"\"Clip gradients to prevent exploding gradients.\"\"\"\n",
    "    total_norm = np.sqrt(sum(np.sum(g**2) for g in grads))\n",
    "    clip_coef = max_norm / (total_norm + 1e-6)\n",
    "    \n",
    "    if clip_coef < 1:\n",
    "        return [g * clip_coef for g in grads], total_norm, True\n",
    "    return grads, total_norm, False\n",
    "\n",
    "# Demonstrate gradient clipping\n",
    "gradients = [np.array([3.0, 4.0]), np.array([5.0, 12.0])]\n",
    "\n",
    "clipped, norm, was_clipped = clip_gradients(gradients, max_norm=5.0)\n",
    "\n",
    "print(\"Gradient Clipping Demo:\")\n",
    "print(f\"Original gradient norm: {norm:.4f}\")\n",
    "print(f\"Was clipped: {was_clipped}\")\n",
    "if was_clipped:\n",
    "    new_norm = np.sqrt(sum(np.sum(g**2) for g in clipped))\n",
    "    print(f\"Clipped gradient norm: {new_norm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training from Scratch vs Fine-tuning\n",
    "\n",
    "| Approach | When to Use | Advantages |\n",
    "|----------|-------------|------------|\n",
    "| **From Scratch** | Unique domain, lots of data | Full control, no bias |\n",
    "| **Fine-tuning** | Limited data, similar domain | Faster, better performance |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate transfer learning scenario\n",
    "# Source task: larger dataset\n",
    "X_source, y_source = make_classification(n_samples=5000, n_features=20, n_informative=15, random_state=42)\n",
    "\n",
    "# Target task: smaller dataset (simulated as similar distribution)\n",
    "X_target, y_target = make_classification(n_samples=200, n_features=20, n_informative=15, random_state=43)\n",
    "\n",
    "X_train_target, X_test_target, y_train_target, y_test_target = train_test_split(\n",
    "    X_target, y_target, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_source_s = scaler.fit_transform(X_source)\n",
    "X_train_target_s = scaler.transform(X_train_target)\n",
    "X_test_target_s = scaler.transform(X_test_target)\n",
    "\n",
    "print(f\"Source dataset: {len(X_source)} samples\")\n",
    "print(f\"Target dataset: {len(X_target)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 1: Train from scratch on target only\n",
    "model_scratch = MLPClassifier(hidden_layer_sizes=(50, 25), max_iter=500, random_state=42)\n",
    "model_scratch.fit(X_train_target_s, y_train_target)\n",
    "scratch_score = model_scratch.score(X_test_target_s, y_test_target)\n",
    "\n",
    "# Approach 2: Pre-train on source, fine-tune on target\n",
    "model_pretrain = MLPClassifier(hidden_layer_sizes=(50, 25), max_iter=500, random_state=42)\n",
    "model_pretrain.fit(X_source_s, y_source)  # Pre-train on source\n",
    "pretrain_score_before = model_pretrain.score(X_test_target_s, y_test_target)\n",
    "\n",
    "# Fine-tune with warm_start (simulating transfer learning)\n",
    "model_finetune = MLPClassifier(hidden_layer_sizes=(50, 25), max_iter=100, warm_start=True, random_state=42)\n",
    "model_finetune.coefs_ = model_pretrain.coefs_\n",
    "model_finetune.intercepts_ = model_pretrain.intercepts_\n",
    "model_finetune.fit(X_train_target_s, y_train_target)\n",
    "finetune_score = model_finetune.score(X_test_target_s, y_test_target)\n",
    "\n",
    "print(\"Transfer Learning Comparison:\")\n",
    "print(f\"  From Scratch (target only): {scratch_score:.4f}\")\n",
    "print(f\"  Pre-trained (before fine-tune): {pretrain_score_before:.4f}\")\n",
    "print(f\"  Fine-tuned: {finetune_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze effect of target dataset size\n",
    "target_sizes = [50, 100, 200, 500, 1000]\n",
    "scratch_scores = []\n",
    "finetune_scores = []\n",
    "\n",
    "for size in target_sizes:\n",
    "    X_t, y_t = X_target[:size], y_target[:size]\n",
    "    X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(X_t, y_t, test_size=0.3, random_state=42)\n",
    "    \n",
    "    X_train_t_s = scaler.transform(X_train_t)\n",
    "    X_test_t_s = scaler.transform(X_test_t)\n",
    "    \n",
    "    # From scratch\n",
    "    m1 = MLPClassifier(hidden_layer_sizes=(50, 25), max_iter=500, random_state=42)\n",
    "    m1.fit(X_train_t_s, y_train_t)\n",
    "    scratch_scores.append(m1.score(X_test_t_s, y_test_t))\n",
    "    \n",
    "    # Pre-trained\n",
    "    m2 = MLPClassifier(hidden_layer_sizes=(50, 25), max_iter=100, random_state=42)\n",
    "    m2.coefs_ = model_pretrain.coefs_\n",
    "    m2.intercepts_ = model_pretrain.intercepts_\n",
    "    m2.fit(X_train_t_s, y_train_t)\n",
    "    finetune_scores.append(m2.score(X_test_t_s, y_test_t))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(target_sizes, scratch_scores, 'b-o', label='From Scratch', linewidth=2, markersize=8)\n",
    "ax.plot(target_sizes, finetune_scores, 'r-o', label='Fine-tuned', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Target Dataset Size')\n",
    "ax.set_ylabel('Test Accuracy')\n",
    "ax.set_title('Transfer Learning Benefit vs Dataset Size')\n",
    "ax.legend()\n",
    "ax.fill_between(target_sizes, scratch_scores, finetune_scores, alpha=0.2, color='green')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hands-on Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Complete Training Pipeline\n",
    "print(\"Exercise: Build a Complete Training Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load dataset\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_val_s = scaler.transform(X_val)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Baseline\n",
    "baseline = LogisticRegression(max_iter=1000, random_state=42)\n",
    "baseline.fit(X_train_s, y_train)\n",
    "print(f\"Step 1 - Baseline (Logistic Regression): {baseline.score(X_test_s, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Compare regularization\n",
    "reg_strengths = [0.001, 0.01, 0.1, 1.0]\n",
    "best_alpha, best_score = 0, 0\n",
    "\n",
    "for alpha in reg_strengths:\n",
    "    model = MLPClassifier(hidden_layer_sizes=(100, 50), alpha=alpha, max_iter=200, random_state=42)\n",
    "    model.fit(X_train_s, y_train)\n",
    "    val_score = model.score(X_val_s, y_val)\n",
    "    if val_score > best_score:\n",
    "        best_score = val_score\n",
    "        best_alpha = alpha\n",
    "    print(f\"  Alpha={alpha}: Val Acc={val_score:.4f}\")\n",
    "\n",
    "print(f\"\\nStep 2 - Best regularization: alpha={best_alpha}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train final model with early stopping\n",
    "final_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50),\n",
    "    alpha=best_alpha,\n",
    "    max_iter=500,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "final_model.fit(X_train_s, y_train)\n",
    "print(f\"Step 3 - Final model stopped at {final_model.n_iter_} iterations\")\n",
    "print(f\"  Validation Accuracy: {final_model.score(X_val_s, y_val):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Final evaluation\n",
    "test_acc = final_model.score(X_test_s, y_test)\n",
    "print(f\"\\nStep 4 - Final Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Improvement over baseline: {(test_acc - baseline.score(X_test_s, y_test))*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training curve\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(final_model.loss_curve_, 'b-', linewidth=2)\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training Loss Curve')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Loss Functions**: Choose based on task type and data characteristics\n",
    "2. **Regularization**: Prevents overfitting - use L1 for sparsity, L2 for smoothness\n",
    "3. **Optimizers**: Adam is a good default, but SGD with momentum works well for large-scale\n",
    "4. **Learning Rate**: Use scheduling for better convergence\n",
    "5. **Transfer Learning**: Highly effective when target data is limited\n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "| Scenario | Recommendation |\n",
    "|----------|----------------|\n",
    "| Classification | Cross-entropy loss, Adam optimizer |\n",
    "| Regression with outliers | Huber loss or MAE |\n",
    "| Overfitting | Increase regularization, early stopping |\n",
    "| Slow convergence | Reduce learning rate, use momentum |\n",
    "| Limited data | Transfer learning with fine-tuning |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
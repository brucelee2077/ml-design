{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 13: Online Evaluation and A/B Testing\n",
    "\n",
    "## Module 5: Evaluation\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "\n",
    "1. **Design online evaluation experiments** - understand online metrics and experiment setup\n",
    "2. **Implement A/B testing frameworks** - sample size calculation, randomization, analysis\n",
    "3. **Analyze experiment results** - statistical significance, confidence intervals\n",
    "4. **Apply advanced testing methods** - multi-armed bandits, interleaving, canary releases\n",
    "5. **Avoid common pitfalls** - peeking, multiple testing, novelty effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple\n",
    "from dataclasses import dataclass\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, ttest_ind\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Introduction to Online Evaluation\n",
    "\n",
    "### Why Online Evaluation?\n",
    "\n",
    "Offline metrics don't always correlate with real-world performance:\n",
    "- **Offline metrics** measure model quality on historical data\n",
    "- **Online metrics** measure actual business impact in production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_offline_online_gap():\n",
    "    \"\"\"Demonstrate the gap between offline and online metrics.\"\"\"\n",
    "    \n",
    "    models = [\n",
    "        {\"name\": \"Model A\", \"offline_auc\": 0.92, \"online_ctr\": 0.045, \"revenue\": 2.1},\n",
    "        {\"name\": \"Model B\", \"offline_auc\": 0.89, \"online_ctr\": 0.052, \"revenue\": 2.8},\n",
    "        {\"name\": \"Model C\", \"offline_auc\": 0.95, \"online_ctr\": 0.041, \"revenue\": 1.9},\n",
    "    ]\n",
    "    \n",
    "    print(\"Offline vs Online Performance Gap\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'Model':<10} {'Offline AUC':<15} {'Online CTR':<15} {'Revenue/User':<15}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for m in models:\n",
    "        print(f\"{m['name']:<10} {m['offline_auc']:<15.3f} {m['online_ctr']:<15.3f} ${m['revenue']:<14.2f}\")\n",
    "    \n",
    "    print(\"\\nKey Insight: Model C has best offline AUC but worst online performance!\")\n",
    "\n",
    "demonstrate_offline_online_gap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Online Metrics\n",
    "\n",
    "| Metric Type | Examples | Use Case |\n",
    "|-------------|----------|----------|\n",
    "| Engagement | CTR, Time on Site | Content platforms |\n",
    "| Conversion | Sign-ups, Purchases | E-commerce |\n",
    "| Revenue | ARPU, LTV | Monetization |\n",
    "| Quality | Error Rate, Bounce Rate | User experience |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class OnlineMetrics:\n",
    "    \"\"\"Container for common online metrics.\"\"\"\n",
    "    impressions: int\n",
    "    clicks: int\n",
    "    conversions: int\n",
    "    revenue: float\n",
    "    sessions: int\n",
    "    \n",
    "    @property\n",
    "    def ctr(self):\n",
    "        return self.clicks / self.impressions if self.impressions > 0 else 0.0\n",
    "    \n",
    "    @property\n",
    "    def conversion_rate(self):\n",
    "        return self.conversions / self.clicks if self.clicks > 0 else 0.0\n",
    "    \n",
    "    @property\n",
    "    def revenue_per_session(self):\n",
    "        return self.revenue / self.sessions if self.sessions > 0 else 0.0\n",
    "    \n",
    "    def display(self):\n",
    "        print(\"=\" * 50)\n",
    "        print(\"Online Metrics Report\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Impressions: {self.impressions:,}\")\n",
    "        print(f\"Clicks: {self.clicks:,}\")\n",
    "        print(f\"Conversions: {self.conversions:,}\")\n",
    "        print(f\"Revenue: ${self.revenue:,.2f}\")\n",
    "        print(f\"CTR: {self.ctr:.2%}\")\n",
    "        print(f\"Conversion Rate: {self.conversion_rate:.2%}\")\n",
    "        print(f\"Revenue/Session: ${self.revenue_per_session:.2f}\")\n",
    "\n",
    "metrics = OnlineMetrics(\n",
    "    impressions=100000, clicks=4500, conversions=450, revenue=22500, sessions=50000\n",
    ")\n",
    "metrics.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. A/B Testing Fundamentals\n",
    "\n",
    "A/B testing compares two versions:\n",
    "- **Control (A)**: Current production version\n",
    "- **Treatment (B)**: New version with changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABTest:\n",
    "    \"\"\"A/B Test implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, name, control_ratio=0.5):\n",
    "        self.name = name\n",
    "        self.control_ratio = control_ratio\n",
    "        self.control_data = []\n",
    "        self.treatment_data = []\n",
    "    \n",
    "    def assign_user(self, user_id):\n",
    "        hash_val = hash(f\"{self.name}_{user_id}\") % 100\n",
    "        return \"control\" if hash_val < self.control_ratio * 100 else \"treatment\"\n",
    "    \n",
    "    def record_outcome(self, group, value):\n",
    "        if group == \"control\":\n",
    "            self.control_data.append(value)\n",
    "        else:\n",
    "            self.treatment_data.append(value)\n",
    "    \n",
    "    def get_summary(self):\n",
    "        control = np.array(self.control_data)\n",
    "        treatment = np.array(self.treatment_data)\n",
    "        return {\n",
    "            \"control\": {\"n\": len(control), \"mean\": np.mean(control) if len(control) > 0 else 0},\n",
    "            \"treatment\": {\"n\": len(treatment), \"mean\": np.mean(treatment) if len(treatment) > 0 else 0}\n",
    "        }\n",
    "\n",
    "# Simulate an A/B test\n",
    "test = ABTest(\"recommendation_v2\")\n",
    "np.random.seed(42)\n",
    "\n",
    "for i in range(10000):\n",
    "    user_id = f\"user_{i}\"\n",
    "    group = test.assign_user(user_id)\n",
    "    \n",
    "    if group == \"control\":\n",
    "        outcome = np.random.binomial(1, 0.05)  # 5% conversion\n",
    "    else:\n",
    "        outcome = np.random.binomial(1, 0.055)  # 5.5% conversion (10% lift)\n",
    "    \n",
    "    test.record_outcome(group, outcome)\n",
    "\n",
    "summary = test.get_summary()\n",
    "print(\"A/B Test Summary\")\n",
    "print(\"=\" * 50)\n",
    "for group, stats in summary.items():\n",
    "    print(f\"{group.upper()}: n={stats['n']}, rate={stats['mean']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Statistical Analysis\n",
    "\n",
    "- **Null Hypothesis (H0)**: No difference between control and treatment\n",
    "- **p-value**: Probability of observing results if H0 is true\n",
    "- **Significance Level (alpha)**: Threshold for rejecting H0 (typically 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABTestAnalyzer:\n",
    "    \"\"\"Statistical analysis for A/B tests.\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.05):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def z_test_proportions(self, n1, p1, n2, p2):\n",
    "        \"\"\"Two-proportion z-test for comparing conversion rates.\"\"\"\n",
    "        p_pool = (n1 * p1 + n2 * p2) / (n1 + n2)\n",
    "        se = np.sqrt(p_pool * (1 - p_pool) * (1/n1 + 1/n2))\n",
    "        z_stat = (p2 - p1) / se if se > 0 else 0\n",
    "        p_value = 2 * (1 - norm.cdf(abs(z_stat)))\n",
    "        \n",
    "        se_diff = np.sqrt(p1 * (1-p1) / n1 + p2 * (1-p2) / n2)\n",
    "        z_crit = norm.ppf(1 - self.alpha/2)\n",
    "        ci_lower = (p2 - p1) - z_crit * se_diff\n",
    "        ci_upper = (p2 - p1) + z_crit * se_diff\n",
    "        \n",
    "        return {\n",
    "            \"z_statistic\": z_stat,\n",
    "            \"p_value\": p_value,\n",
    "            \"significant\": p_value < self.alpha,\n",
    "            \"absolute_lift\": p2 - p1,\n",
    "            \"relative_lift\": (p2 - p1) / p1 if p1 > 0 else 0,\n",
    "            \"ci_lower\": ci_lower,\n",
    "            \"ci_upper\": ci_upper,\n",
    "        }\n",
    "\n",
    "analyzer = ABTestAnalyzer(alpha=0.05)\n",
    "results = analyzer.z_test_proportions(\n",
    "    n1=summary[\"control\"][\"n\"], p1=summary[\"control\"][\"mean\"],\n",
    "    n2=summary[\"treatment\"][\"n\"], p2=summary[\"treatment\"][\"mean\"]\n",
    ")\n",
    "\n",
    "print(\"Statistical Analysis Results\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Control Rate: {summary['control']['mean']:.4f}\")\n",
    "print(f\"Treatment Rate: {summary['treatment']['mean']:.4f}\")\n",
    "print(f\"Absolute Lift: {results['absolute_lift']:.4f}\")\n",
    "print(f\"Relative Lift: {results['relative_lift']:.2%}\")\n",
    "print(f\"Z-Statistic: {results['z_statistic']:.4f}\")\n",
    "print(f\"P-Value: {results['p_value']:.4f}\")\n",
    "print(f\"95% CI: [{results['ci_lower']:.4f}, {results['ci_upper']:.4f}]\")\n",
    "print(f\"Significant: {'YES' if results['significant'] else 'NO'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_ab_results(control_rate, treatment_rate, control_n, treatment_n, alpha=0.05):\n",
    "    \"\"\"Visualize A/B test results.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Bar chart\n",
    "    groups = ['Control', 'Treatment']\n",
    "    rates = [control_rate, treatment_rate]\n",
    "    colors = ['steelblue', 'coral']\n",
    "    \n",
    "    se_control = np.sqrt(control_rate * (1 - control_rate) / control_n)\n",
    "    se_treatment = np.sqrt(treatment_rate * (1 - treatment_rate) / treatment_n)\n",
    "    z = norm.ppf(1 - alpha/2)\n",
    "    errors = [z * se_control, z * se_treatment]\n",
    "    \n",
    "    bars = axes[0].bar(groups, rates, color=colors, alpha=0.7)\n",
    "    axes[0].errorbar(groups, rates, yerr=errors, fmt='none', color='black', capsize=5)\n",
    "    axes[0].set_ylabel('Conversion Rate')\n",
    "    axes[0].set_title('Control vs Treatment')\n",
    "    \n",
    "    for bar, rate in zip(bars, rates):\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.002,\n",
    "                     f'{rate:.2%}', ha='center', va='bottom')\n",
    "    \n",
    "    # Distribution of difference\n",
    "    diff = treatment_rate - control_rate\n",
    "    se_diff = np.sqrt(se_control**2 + se_treatment**2)\n",
    "    \n",
    "    x = np.linspace(diff - 4*se_diff, diff + 4*se_diff, 100)\n",
    "    y = norm.pdf(x, diff, se_diff)\n",
    "    \n",
    "    axes[1].plot(x, y, 'b-', linewidth=2)\n",
    "    axes[1].fill_between(x, y, alpha=0.3)\n",
    "    axes[1].axvline(x=0, color='red', linestyle='--', label='No Effect')\n",
    "    axes[1].axvline(x=diff, color='green', linestyle='-', label=f'Observed: {diff:.4f}')\n",
    "    axes[1].set_xlabel('Difference (Treatment - Control)')\n",
    "    axes[1].set_ylabel('Probability Density')\n",
    "    axes[1].set_title('Distribution of Effect Size')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_ab_results(\n",
    "    summary['control']['mean'], summary['treatment']['mean'],\n",
    "    summary['control']['n'], summary['treatment']['n']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Sample Size Calculation\n",
    "\n",
    "Key parameters:\n",
    "- **Alpha**: Probability of false positive (0.05)\n",
    "- **Power**: Probability of detecting true effect (0.80)\n",
    "- **MDE**: Minimum Detectable Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleSizeCalculator:\n",
    "    \"\"\"Calculate required sample sizes for A/B tests.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def for_proportions(baseline_rate, mde_relative, alpha=0.05, power=0.80):\n",
    "        \"\"\"Sample size for conversion rate test.\"\"\"\n",
    "        p1 = baseline_rate\n",
    "        p2 = baseline_rate * (1 + mde_relative)\n",
    "        \n",
    "        z_alpha = norm.ppf(1 - alpha/2)\n",
    "        z_beta = norm.ppf(power)\n",
    "        p_avg = (p1 + p2) / 2\n",
    "        \n",
    "        numerator = (z_alpha * np.sqrt(2 * p_avg * (1 - p_avg)) +\n",
    "                     z_beta * np.sqrt(p1 * (1 - p1) + p2 * (1 - p2)))**2\n",
    "        denominator = (p2 - p1)**2\n",
    "        \n",
    "        return int(np.ceil(numerator / denominator))\n",
    "\n",
    "# Example calculations\n",
    "calc = SampleSizeCalculator()\n",
    "baseline_cvr = 0.05\n",
    "\n",
    "print(\"Sample Size Calculator\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Baseline conversion rate: {baseline_cvr:.0%}\")\n",
    "print()\n",
    "print(f\"{'MDE (Relative)':<20} {'Sample Size/Group':<20} {'Total Sample':<15}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for mde in [0.05, 0.10, 0.20]:\n",
    "    n = calc.for_proportions(baseline_cvr, mde)\n",
    "    print(f\"{mde:>10.0%}           {n:>15,}      {2*n:>12,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_size_sensitivity(baseline_rate=0.05):\n",
    "    \"\"\"Plot sample size requirements for different MDE values.\"\"\"\n",
    "    calc = SampleSizeCalculator()\n",
    "    mde_range = np.linspace(0.02, 0.30, 30)\n",
    "    sample_sizes = [calc.for_proportions(baseline_rate, mde) for mde in mde_range]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(mde_range * 100, sample_sizes, 'b-', linewidth=2)\n",
    "    \n",
    "    for mde in [0.05, 0.10, 0.20]:\n",
    "        n = calc.for_proportions(baseline_rate, mde)\n",
    "        plt.plot(mde * 100, n, 'ro', markersize=10)\n",
    "        plt.annotate(f'{n:,}', (mde * 100, n), textcoords=\"offset points\",\n",
    "                     xytext=(10, 5), fontsize=10)\n",
    "    \n",
    "    plt.xlabel('Minimum Detectable Effect (%)')\n",
    "    plt.ylabel('Sample Size per Group')\n",
    "    plt.title(f'Sample Size vs MDE (Baseline Rate = {baseline_rate:.0%})')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_sample_size_sensitivity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Multi-Armed Bandits\n",
    "\n",
    "Bandits balance exploration (learning) and exploitation (using best variant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyBandit:\n",
    "    \"\"\"Epsilon-Greedy Multi-Armed Bandit.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_arms, epsilon=0.1):\n",
    "        self.n_arms = n_arms\n",
    "        self.epsilon = epsilon\n",
    "        self.counts = np.zeros(n_arms)\n",
    "        self.values = np.zeros(n_arms)\n",
    "    \n",
    "    def select_arm(self):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_arms)\n",
    "        return np.argmax(self.values)\n",
    "    \n",
    "    def update(self, arm, reward):\n",
    "        self.counts[arm] += 1\n",
    "        n = self.counts[arm]\n",
    "        self.values[arm] = self.values[arm] + (reward - self.values[arm]) / n\n",
    "\n",
    "\n",
    "class ThompsonSamplingBandit:\n",
    "    \"\"\"Thompson Sampling for Bernoulli bandits.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_arms):\n",
    "        self.n_arms = n_arms\n",
    "        self.alpha = np.ones(n_arms)\n",
    "        self.beta = np.ones(n_arms)\n",
    "    \n",
    "    def select_arm(self):\n",
    "        samples = [np.random.beta(self.alpha[i], self.beta[i]) for i in range(self.n_arms)]\n",
    "        return np.argmax(samples)\n",
    "    \n",
    "    def update(self, arm, reward):\n",
    "        if reward == 1:\n",
    "            self.alpha[arm] += 1\n",
    "        else:\n",
    "            self.beta[arm] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_bandit_comparison(n_rounds=10000):\n",
    "    \"\"\"Compare different bandit algorithms.\"\"\"\n",
    "    true_probs = [0.04, 0.05, 0.06, 0.045]\n",
    "    n_arms = len(true_probs)\n",
    "    \n",
    "    bandits = {\n",
    "        \"Epsilon-Greedy\": EpsilonGreedyBandit(n_arms, epsilon=0.1),\n",
    "        \"Thompson Sampling\": ThompsonSamplingBandit(n_arms),\n",
    "    }\n",
    "    \n",
    "    cumulative_rewards = {name: [] for name in bandits}\n",
    "    arm_selections = {name: np.zeros(n_arms) for name in bandits}\n",
    "    \n",
    "    for t in range(n_rounds):\n",
    "        for name, bandit in bandits.items():\n",
    "            arm = bandit.select_arm()\n",
    "            reward = np.random.binomial(1, true_probs[arm])\n",
    "            bandit.update(arm, reward)\n",
    "            \n",
    "            arm_selections[name][arm] += 1\n",
    "            prev = cumulative_rewards[name][-1] if cumulative_rewards[name] else 0\n",
    "            cumulative_rewards[name].append(prev + reward)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    for name, rewards in cumulative_rewards.items():\n",
    "        axes[0].plot(rewards, label=name, linewidth=2)\n",
    "    axes[0].set_xlabel('Round')\n",
    "    axes[0].set_ylabel('Cumulative Reward')\n",
    "    axes[0].set_title('Cumulative Reward Over Time')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    x = np.arange(n_arms)\n",
    "    width = 0.35\n",
    "    for i, (name, selections) in enumerate(arm_selections.items()):\n",
    "        axes[1].bar(x + i * width, selections / n_rounds, width, label=name)\n",
    "    axes[1].set_xlabel('Arm')\n",
    "    axes[1].set_ylabel('Selection Proportion')\n",
    "    axes[1].set_title('Arm Selection Distribution')\n",
    "    axes[1].set_xticks(x + width/2)\n",
    "    axes[1].set_xticklabels([f'Arm {i} (p={p})' for i, p in enumerate(true_probs)])\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nBandit Summary (Best arm: Arm 2, p=0.06)\")\n",
    "    print(\"=\" * 50)\n",
    "    for name, selections in arm_selections.items():\n",
    "        best_pct = selections[2] / n_rounds\n",
    "        total = cumulative_rewards[name][-1]\n",
    "        print(f\"{name}: Best arm {best_pct:.1%}, Total reward {total}\")\n",
    "\n",
    "simulate_bandit_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Common Pitfalls\n",
    "\n",
    "### 7.1 Peeking Problem\n",
    "\n",
    "Looking at results too early inflates false positive rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_peeking_problem(n_simulations=1000):\n",
    "    \"\"\"Show how peeking inflates false positives.\"\"\"\n",
    "    n_samples = 5000\n",
    "    peek_points = [500, 1000, 2000, 3000, 4000, 5000]\n",
    "    true_rate = 0.05\n",
    "    \n",
    "    false_positives_any = 0\n",
    "    false_positives_final = 0\n",
    "    \n",
    "    for _ in range(n_simulations):\n",
    "        # Generate A/A test data (no real difference)\n",
    "        control = np.random.binomial(1, true_rate, n_samples)\n",
    "        treatment = np.random.binomial(1, true_rate, n_samples)\n",
    "        \n",
    "        significant_at_any = False\n",
    "        significant_at_final = False\n",
    "        \n",
    "        for peek in peek_points:\n",
    "            c_sum = control[:peek].sum()\n",
    "            t_sum = treatment[:peek].sum()\n",
    "            \n",
    "            p_c = c_sum / peek\n",
    "            p_t = t_sum / peek\n",
    "            \n",
    "            if peek == 0:\n",
    "                continue\n",
    "            \n",
    "            p_pool = (c_sum + t_sum) / (2 * peek)\n",
    "            se = np.sqrt(2 * p_pool * (1 - p_pool) / peek)\n",
    "            \n",
    "            if se > 0:\n",
    "                z = abs(p_t - p_c) / se\n",
    "                p_value = 2 * (1 - norm.cdf(z))\n",
    "                \n",
    "                if p_value < 0.05:\n",
    "                    significant_at_any = True\n",
    "                    if peek == n_samples:\n",
    "                        significant_at_final = True\n",
    "        \n",
    "        if significant_at_any:\n",
    "            false_positives_any += 1\n",
    "        if significant_at_final:\n",
    "            false_positives_final += 1\n",
    "    \n",
    "    print(\"Peeking Problem Demonstration\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Simulations: {n_simulations}\")\n",
    "    print(f\"\\nFalse Positive Rate:\")\n",
    "    print(f\"  Without peeking (final only): {false_positives_final/n_simulations:.1%}\")\n",
    "    print(f\"  With peeking (any peek): {false_positives_any/n_simulations:.1%}\")\n",
    "    print(f\"\\nExpected rate: 5%\")\n",
    "    print(f\"\\nConclusion: Peeking inflates false positive rate significantly!\")\n",
    "\n",
    "demonstrate_peeking_problem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Multiple Testing Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_multiple_testing():\n",
    "    \"\"\"Show how testing multiple metrics inflates false positives.\"\"\"\n",
    "    n_metrics = [1, 5, 10, 20]\n",
    "    alpha = 0.05\n",
    "    \n",
    "    print(\"Multiple Testing Problem\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"{'# Metrics':<15} {'P(At least 1 FP)':<20} {'Correction Needed'}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for n in n_metrics:\n",
    "        # P(at least one false positive) = 1 - P(no false positives)\n",
    "        p_at_least_one_fp = 1 - (1 - alpha) ** n\n",
    "        bonferroni_alpha = alpha / n\n",
    "        \n",
    "        print(f\"{n:<15} {p_at_least_one_fp:<20.1%} alpha = {bonferroni_alpha:.4f}\")\n",
    "    \n",
    "    print(\"\\nSolution: Use Bonferroni correction (alpha / n_tests)\")\n",
    "\n",
    "demonstrate_multiple_testing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Hands-on Exercises\n",
    "\n",
    "### Exercise 1: Complete A/B Test Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_complete_ab_analysis(control_visitors, control_conversions,\n",
    "                              treatment_visitors, treatment_conversions,\n",
    "                              alpha=0.05):\n",
    "    \"\"\"Run complete A/B test analysis.\"\"\"\n",
    "    \n",
    "    p_c = control_conversions / control_visitors\n",
    "    p_t = treatment_conversions / treatment_visitors\n",
    "    \n",
    "    print(\"A/B Test Analysis Report\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Basic stats\n",
    "    print(\"\\n1. Sample Summary\")\n",
    "    print(f\"   Control: {control_visitors:,} visitors, {control_conversions:,} conversions\")\n",
    "    print(f\"   Treatment: {treatment_visitors:,} visitors, {treatment_conversions:,} conversions\")\n",
    "    \n",
    "    print(\"\\n2. Conversion Rates\")\n",
    "    print(f\"   Control: {p_c:.4f} ({p_c:.2%})\")\n",
    "    print(f\"   Treatment: {p_t:.4f} ({p_t:.2%})\")\n",
    "    \n",
    "    # Statistical test\n",
    "    analyzer = ABTestAnalyzer(alpha=alpha)\n",
    "    results = analyzer.z_test_proportions(control_visitors, p_c, treatment_visitors, p_t)\n",
    "    \n",
    "    print(\"\\n3. Lift Analysis\")\n",
    "    print(f\"   Absolute Lift: {results['absolute_lift']:.4f}\")\n",
    "    print(f\"   Relative Lift: {results['relative_lift']:.2%}\")\n",
    "    \n",
    "    print(\"\\n4. Statistical Significance\")\n",
    "    print(f\"   Z-Statistic: {results['z_statistic']:.4f}\")\n",
    "    print(f\"   P-Value: {results['p_value']:.4f}\")\n",
    "    print(f\"   95% CI: [{results['ci_lower']:.4f}, {results['ci_upper']:.4f}]\")\n",
    "    print(f\"   Significant at alpha={alpha}: {'YES' if results['significant'] else 'NO'}\")\n",
    "    \n",
    "    # Recommendation\n",
    "    print(\"\\n5. Recommendation\")\n",
    "    if results['significant'] and results['absolute_lift'] > 0:\n",
    "        print(\"   -> SHIP IT! Treatment shows significant improvement.\")\n",
    "    elif results['significant'] and results['absolute_lift'] < 0:\n",
    "        print(\"   -> DO NOT SHIP. Treatment performs significantly worse.\")\n",
    "    else:\n",
    "        print(\"   -> No significant difference. Consider running longer or with more traffic.\")\n",
    "\n",
    "# Example usage\n",
    "run_complete_ab_analysis(\n",
    "    control_visitors=50000,\n",
    "    control_conversions=2500,\n",
    "    treatment_visitors=50000,\n",
    "    treatment_conversions=2750\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Power Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_analysis_table(baseline_rate, daily_traffic, traffic_allocation=0.5):\n",
    "    \"\"\"Generate power analysis table for different scenarios.\"\"\"\n",
    "    \n",
    "    calc = SampleSizeCalculator()\n",
    "    mde_options = [0.05, 0.10, 0.15, 0.20]\n",
    "    \n",
    "    print(f\"Power Analysis (Baseline = {baseline_rate:.1%}, Daily Traffic = {daily_traffic:,})\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"{'MDE':<10} {'Sample/Group':<15} {'Total Sample':<15} {'Duration (days)':<15}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for mde in mde_options:\n",
    "        n = calc.for_proportions(baseline_rate, mde)\n",
    "        total = 2 * n\n",
    "        daily_per_group = daily_traffic * traffic_allocation * 0.5\n",
    "        days = int(np.ceil(n / daily_per_group))\n",
    "        \n",
    "        print(f\"{mde:<10.0%} {n:<15,} {total:<15,} {days:<15}\")\n",
    "\n",
    "power_analysis_table(baseline_rate=0.05, daily_traffic=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Online vs Offline Metrics**\n",
    "   - Offline metrics don't always correlate with business impact\n",
    "   - Always validate with online experiments\n",
    "\n",
    "2. **A/B Testing Fundamentals**\n",
    "   - Randomize users properly\n",
    "   - Calculate sample size before running\n",
    "   - Wait for statistical significance\n",
    "\n",
    "3. **Statistical Analysis**\n",
    "   - Use appropriate tests (z-test for proportions, t-test for means)\n",
    "   - Report confidence intervals, not just p-values\n",
    "   - Consider practical significance\n",
    "\n",
    "4. **Common Pitfalls**\n",
    "   - Don't peek at results early\n",
    "   - Correct for multiple testing\n",
    "   - Account for novelty effects\n",
    "\n",
    "5. **Advanced Methods**\n",
    "   - Multi-armed bandits for exploration/exploitation\n",
    "   - Shadow deployment for risk-free testing\n",
    "   - Canary releases for gradual rollout\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next module, we'll cover **Deployment Strategies** to learn how to deploy models to production safely and efficiently."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
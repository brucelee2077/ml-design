{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 14: Deployment Strategies\n",
    "\n",
    "## Module 6: Deployment and Serving\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "\n",
    "1. **Compare cloud vs on-device deployment** - Understand trade-offs between centralized and edge deployments\n",
    "2. **Implement batch and online prediction** - Build systems for different prediction requirements\n",
    "3. **Design prediction pipelines** - Create end-to-end serving architectures\n",
    "4. **Apply deployment patterns** - Use shadow, canary, and blue-green deployments\n",
    "5. **Build production-ready APIs** - Create scalable model serving endpoints\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction to ML Deployment](#1-introduction)\n",
    "2. [Cloud vs On-Device Deployment](#2-cloud-vs-device)\n",
    "3. [Batch Prediction Systems](#3-batch-prediction)\n",
    "4. [Online Prediction Systems](#4-online-prediction)\n",
    "5. [Deployment Patterns](#5-deployment-patterns)\n",
    "6. [Building Model Serving APIs](#6-model-serving-apis)\n",
    "7. [Prediction Pipeline Design](#7-pipeline-design)\n",
    "8. [Hands-on Exercises](#8-exercises)\n",
    "9. [Summary](#9-summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Introduction to ML Deployment <a name=\"1-introduction\"></a>\n",
    "\n",
    "Deploying ML models to production is where the real value is created. A model that only exists in a Jupyter notebook does not help users. This tutorial covers strategies and patterns for successfully deploying ML systems.\n",
    "\n",
    "### Key Deployment Challenges\n",
    "\n",
    "| Challenge | Description |\n",
    "|-----------|-------------|\n",
    "| Environment Differences | Dev vs prod dependencies |\n",
    "| Scale | 1 request to millions/sec |\n",
    "| Reliability | High availability requirements |\n",
    "| Latency | Real-time response needs |\n",
    "| Updates | Safe model updates/rollbacks |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Cloud vs On-Device Deployment <a name=\"2-cloud-vs-device\"></a>\n",
    "\n",
    "The first major decision in deployment is where the model runs.\n",
    "\n",
    "| Aspect | Cloud | On-Device |\n",
    "|--------|-------|----------|\n",
    "| Latency | Higher (network) | Lower (local) |\n",
    "| Model Size | Unlimited | Limited |\n",
    "| Compute | Powerful GPUs | Limited |\n",
    "| Privacy | Data sent to server | Data stays local |\n",
    "| Offline | Requires network | Works offline |\n",
    "| Updates | Easy | App store approval |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DeploymentOption:\n",
    "    \"\"\"Represents a deployment location option.\"\"\"\n",
    "    name: str\n",
    "    latency_ms: float\n",
    "    max_model_size_mb: float\n",
    "    requires_network: bool\n",
    "    privacy_level: str\n",
    "    update_ease: str\n",
    "    compute_power: str\n",
    "    cost_per_inference: float\n",
    "\n",
    "\n",
    "class DeploymentDecisionFramework:\n",
    "    \"\"\"Framework for deciding between deployment options.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.options = {\n",
    "            'cloud_gpu': DeploymentOption('Cloud (GPU)', 50, 10000, True, 'low', 'easy', 'very_high', 0.0001),\n",
    "            'cloud_cpu': DeploymentOption('Cloud (CPU)', 100, 5000, True, 'low', 'easy', 'high', 0.00001),\n",
    "            'edge_server': DeploymentOption('Edge Server', 20, 2000, True, 'medium', 'medium', 'high', 0.00005),\n",
    "            'mobile': DeploymentOption('Mobile Device', 10, 100, False, 'high', 'hard', 'low', 0),\n",
    "            'browser': DeploymentOption('Browser (WebML)', 15, 50, False, 'high', 'medium', 'low', 0)\n",
    "        }\n",
    "    \n",
    "    def evaluate_requirements(self, max_latency_ms, model_size_mb, requires_offline, privacy_req, budget):\n",
    "        \"\"\"Evaluate deployment options against requirements.\"\"\"\n",
    "        privacy_scores = {'low': 1, 'medium': 2, 'high': 3}\n",
    "        results = []\n",
    "        \n",
    "        for key, opt in self.options.items():\n",
    "            score = 100.0\n",
    "            reasons = []\n",
    "            \n",
    "            if opt.latency_ms > max_latency_ms:\n",
    "                score -= 30\n",
    "                reasons.append('Latency too high')\n",
    "            if model_size_mb > opt.max_model_size_mb:\n",
    "                score -= 50\n",
    "                reasons.append('Model too large')\n",
    "            if requires_offline and opt.requires_network:\n",
    "                score -= 40\n",
    "                reasons.append('Requires network')\n",
    "            if privacy_scores.get(opt.privacy_level, 1) < privacy_scores.get(privacy_req, 1):\n",
    "                score -= 25\n",
    "                reasons.append('Privacy insufficient')\n",
    "            if opt.cost_per_inference * 1e6 > budget:\n",
    "                score -= 20\n",
    "                reasons.append('Over budget')\n",
    "            \n",
    "            results.append((key, opt, max(0, score), reasons))\n",
    "        \n",
    "        return sorted(results, key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    def visualize_comparison(self):\n",
    "        \"\"\"Visualize deployment options.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        opts = list(self.options.values())\n",
    "        names = [o.name for o in opts]\n",
    "        colors = plt.cm.Set2(np.linspace(0, 1, len(names)))\n",
    "        \n",
    "        axes[0,0].barh(names, [o.latency_ms for o in opts], color=colors)\n",
    "        axes[0,0].set_xlabel('Latency (ms)')\n",
    "        axes[0,0].set_title('Inference Latency')\n",
    "        \n",
    "        axes[0,1].barh(names, [o.max_model_size_mb for o in opts], color=colors)\n",
    "        axes[0,1].set_xlabel('Max Model Size (MB)')\n",
    "        axes[0,1].set_title('Model Size Capacity')\n",
    "        axes[0,1].set_xscale('log')\n",
    "        \n",
    "        axes[1,0].barh(names, [o.cost_per_inference * 1e6 for o in opts], color=colors)\n",
    "        axes[1,0].set_xlabel('Cost per Million ($)')\n",
    "        axes[1,0].set_title('Infrastructure Cost')\n",
    "        \n",
    "        compute_map = {'low': 1, 'medium': 2, 'high': 3, 'very_high': 4}\n",
    "        privacy_map = {'low': 1, 'medium': 2, 'high': 3}\n",
    "        x = np.arange(len(names))\n",
    "        axes[1,1].bar(x - 0.2, [compute_map[o.compute_power] for o in opts], 0.4, label='Compute', color='steelblue')\n",
    "        axes[1,1].bar(x + 0.2, [privacy_map[o.privacy_level] for o in opts], 0.4, label='Privacy', color='forestgreen')\n",
    "        axes[1,1].set_xticks(x)\n",
    "        axes[1,1].set_xticklabels(names, rotation=45, ha='right')\n",
    "        axes[1,1].legend()\n",
    "        axes[1,1].set_title('Compute and Privacy')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "framework = DeploymentDecisionFramework()\n",
    "framework.visualize_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate deployment options for different use cases\n",
    "print(\"USE CASE 1: Real-time Image Classification for Mobile\")\n",
    "print(\"=\"*55)\n",
    "results = framework.evaluate_requirements(100, 50, True, 'medium', 10.0)\n",
    "for i, (key, opt, score, reasons) in enumerate(results, 1):\n",
    "    status = 'PASS' if score >= 70 else 'WARN' if score >= 40 else 'FAIL'\n",
    "    print(f\"{i}. {opt.name:<18} Score: {score:>5.0f}  [{status}]\")\n",
    "    for r in reasons:\n",
    "        print(f\"   - {r}\")\n",
    "\n",
    "print(\"\\nUSE CASE 2: Large Language Model for Enterprise\")\n",
    "print(\"=\"*55)\n",
    "results2 = framework.evaluate_requirements(500, 5000, False, 'low', 100.0)\n",
    "for i, (key, opt, score, reasons) in enumerate(results2, 1):\n",
    "    status = 'PASS' if score >= 70 else 'WARN' if score >= 40 else 'FAIL'\n",
    "    print(f\"{i}. {opt.name:<18} Score: {score:>5.0f}  [{status}]\")\n",
    "    for r in reasons:\n",
    "        print(f\"   - {r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Batch Prediction Systems <a name=\"3-batch-prediction\"></a>\n",
    "\n",
    "Batch prediction processes large volumes of data at scheduled intervals.\n",
    "\n",
    "**Use Cases:** Recommendations, credit scoring, email targeting, fraud detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchPredictionPipeline:\n",
    "    \"\"\"Production-ready batch prediction pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, batch_size=1000):\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "        self.metrics = {'processed': 0, 'time': 0, 'batches': 0}\n",
    "    \n",
    "    def process_batch(self, data):\n",
    "        start = time.time()\n",
    "        preds = self.model.predict(data)\n",
    "        proba = self.model.predict_proba(data) if hasattr(self.model, 'predict_proba') else None\n",
    "        elapsed = time.time() - start\n",
    "        return preds, proba, {'size': len(data), 'time': elapsed, 'throughput': len(data)/elapsed}\n",
    "    \n",
    "    def run_pipeline(self, data, verbose=True):\n",
    "        start = time.time()\n",
    "        n_samples = len(data)\n",
    "        n_batches = (n_samples + self.batch_size - 1) // self.batch_size\n",
    "        \n",
    "        all_preds, all_proba, batch_metrics = [], [], []\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Starting: {n_samples:,} samples, {n_batches} batches\")\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            batch = data[i*self.batch_size:(i+1)*self.batch_size]\n",
    "            preds, proba, metrics = self.process_batch(batch)\n",
    "            all_preds.extend(preds)\n",
    "            if proba is not None:\n",
    "                all_proba.extend(proba)\n",
    "            batch_metrics.append(metrics)\n",
    "        \n",
    "        self.metrics['time'] = time.time() - start\n",
    "        self.metrics['processed'] = n_samples\n",
    "        \n",
    "        results = pd.DataFrame({'sample_id': range(n_samples), 'prediction': all_preds})\n",
    "        if all_proba:\n",
    "            for j in range(len(all_proba[0])):\n",
    "                results[f'prob_{j}'] = [p[j] for p in all_proba]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Completed in {self.metrics['time']:.2f}s ({n_samples/self.metrics['time']:.0f} samples/sec)\")\n",
    "        \n",
    "        return results, batch_metrics\n",
    "\n",
    "\n",
    "# Create sample data and train model\n",
    "X, y = make_classification(n_samples=10000, n_features=20, n_classes=3, n_informative=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Run batch prediction\n",
    "pipeline = BatchPredictionPipeline(model, batch_size=500)\n",
    "results, batch_metrics = pipeline.run_pipeline(X_test)\n",
    "print(\"\\nSample predictions:\")\n",
    "print(results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize batch performance\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "times = [m['time'] for m in batch_metrics]\n",
    "axes[0].bar(range(len(times)), times, color='steelblue', alpha=0.7)\n",
    "axes[0].axhline(np.mean(times), color='red', linestyle='--', label=f'Mean: {np.mean(times):.3f}s')\n",
    "axes[0].set_xlabel('Batch')\n",
    "axes[0].set_ylabel('Time (s)')\n",
    "axes[0].set_title('Processing Time per Batch')\n",
    "axes[0].legend()\n",
    "\n",
    "throughputs = [m['throughput'] for m in batch_metrics]\n",
    "axes[1].plot(throughputs, marker='o', color='forestgreen')\n",
    "axes[1].axhline(np.mean(throughputs), color='red', linestyle='--')\n",
    "axes[1].set_xlabel('Batch')\n",
    "axes[1].set_ylabel('Throughput')\n",
    "axes[1].set_title('Throughput per Batch')\n",
    "\n",
    "results['prediction'].value_counts().sort_index().plot(kind='bar', ax=axes[2], color='coral')\n",
    "axes[2].set_xlabel('Class')\n",
    "axes[2].set_ylabel('Count')\n",
    "axes[2].set_title('Prediction Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Online Prediction Systems <a name=\"4-online-prediction\"></a>\n",
    "\n",
    "Online prediction serves real-time requests with low latency.\n",
    "\n",
    "**Requirements:** Latency <100ms (p99), Availability 99.9%+, Throughput 10,000+ QPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionCache:\n",
    "    \"\"\"LRU cache for predictions.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_size=10000, ttl=300):\n",
    "        self.max_size = max_size\n",
    "        self.ttl = ttl\n",
    "        self.cache = {}\n",
    "        self.order = []\n",
    "        self.stats = {'hits': 0, 'misses': 0}\n",
    "    \n",
    "    def _key(self, features):\n",
    "        return hashlib.md5(features.tobytes()).hexdigest()\n",
    "    \n",
    "    def get(self, features):\n",
    "        key = self._key(features)\n",
    "        if key in self.cache:\n",
    "            val, ts = self.cache[key]\n",
    "            if time.time() - ts < self.ttl:\n",
    "                self.stats['hits'] += 1\n",
    "                self.order.remove(key)\n",
    "                self.order.append(key)\n",
    "                return val\n",
    "            del self.cache[key]\n",
    "            self.order.remove(key)\n",
    "        self.stats['misses'] += 1\n",
    "        return None\n",
    "    \n",
    "    def set(self, features, prediction):\n",
    "        key = self._key(features)\n",
    "        while len(self.cache) >= self.max_size:\n",
    "            oldest = self.order.pop(0)\n",
    "            del self.cache[oldest]\n",
    "        self.cache[key] = (prediction, time.time())\n",
    "        self.order.append(key)\n",
    "    \n",
    "    @property\n",
    "    def hit_rate(self):\n",
    "        total = self.stats['hits'] + self.stats['misses']\n",
    "        return self.stats['hits'] / total if total else 0\n",
    "\n",
    "\n",
    "class OnlinePredictionService:\n",
    "    \"\"\"Online prediction service with caching.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, cache_enabled=True):\n",
    "        self.model = model\n",
    "        self.cache = PredictionCache() if cache_enabled else None\n",
    "        self.latencies = []\n",
    "        self.requests = 0\n",
    "        self.errors = 0\n",
    "    \n",
    "    def predict(self, features):\n",
    "        start = time.time()\n",
    "        self.requests += 1\n",
    "        \n",
    "        try:\n",
    "            if self.cache:\n",
    "                cached = self.cache.get(features)\n",
    "                if cached is not None:\n",
    "                    latency = time.time() - start\n",
    "                    self.latencies.append(latency)\n",
    "                    return {'prediction': cached['pred'], 'probability': cached['prob'],\n",
    "                            'latency_ms': latency * 1000, 'cache_hit': True}\n",
    "            \n",
    "            features_2d = features.reshape(1, -1)\n",
    "            pred = self.model.predict(features_2d)[0]\n",
    "            prob = self.model.predict_proba(features_2d)[0].tolist() if hasattr(self.model, 'predict_proba') else None\n",
    "            \n",
    "            if self.cache:\n",
    "                self.cache.set(features, {'pred': int(pred), 'prob': prob})\n",
    "            \n",
    "            latency = time.time() - start\n",
    "            self.latencies.append(latency)\n",
    "            return {'prediction': int(pred), 'probability': prob, 'latency_ms': latency * 1000, 'cache_hit': False}\n",
    "        except Exception as e:\n",
    "            self.errors += 1\n",
    "            return {'error': str(e)}\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        lats = [l * 1000 for l in self.latencies]\n",
    "        return {\n",
    "            'requests': self.requests,\n",
    "            'error_rate': self.errors / self.requests if self.requests else 0,\n",
    "            'p50_ms': np.percentile(lats, 50) if lats else 0,\n",
    "            'p95_ms': np.percentile(lats, 95) if lats else 0,\n",
    "            'p99_ms': np.percentile(lats, 99) if lats else 0,\n",
    "            'cache_hit_rate': self.cache.hit_rate if self.cache else 0\n",
    "        }\n",
    "\n",
    "\n",
    "# Create service and simulate requests\n",
    "service = OnlinePredictionService(model, cache_enabled=True)\n",
    "\n",
    "print(\"Simulating 1000 online requests...\")\n",
    "unique_samples = X_test[:200]\n",
    "indices = np.random.choice(200, 1000, replace=True)\n",
    "results_online = [service.predict(unique_samples[i]) for i in indices]\n",
    "\n",
    "metrics = service.get_metrics()\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"  Requests: {metrics['requests']}\")\n",
    "print(f\"  Error rate: {metrics['error_rate']:.2%}\")\n",
    "print(f\"  P50: {metrics['p50_ms']:.3f}ms\")\n",
    "print(f\"  P95: {metrics['p95_ms']:.3f}ms\")\n",
    "print(f\"  P99: {metrics['p99_ms']:.3f}ms\")\n",
    "print(f\"  Cache hit rate: {metrics['cache_hit_rate']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize online performance\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "lats = [r['latency_ms'] for r in results_online if 'latency_ms' in r]\n",
    "axes[0].hist(lats, bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0].axvline(metrics['p50_ms'], color='orange', linestyle='--', label='P50')\n",
    "axes[0].axvline(metrics['p99_ms'], color='red', linestyle='--', label='P99')\n",
    "axes[0].set_xlabel('Latency (ms)')\n",
    "axes[0].set_title('Latency Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "hits = sum(1 for r in results_online if r.get('cache_hit', False))\n",
    "axes[1].pie([hits, len(results_online)-hits], labels=['Hit', 'Miss'], autopct='%1.1f%%', colors=['forestgreen', 'coral'])\n",
    "axes[1].set_title('Cache Hit Rate')\n",
    "\n",
    "rolling = pd.Series(lats).rolling(50).mean()\n",
    "axes[2].plot(rolling, color='steelblue')\n",
    "axes[2].set_xlabel('Request')\n",
    "axes[2].set_ylabel('Rolling Avg Latency (ms)')\n",
    "axes[2].set_title('Latency Over Time')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Deployment Patterns <a name=\"5-deployment-patterns\"></a>\n",
    "\n",
    "Safe deployment patterns minimize risk when updating models.\n",
    "\n",
    "| Pattern | Risk | Rollback | Use Case |\n",
    "|---------|------|----------|----------|\n",
    "| Shadow | Very Low | N/A | Testing new model |\n",
    "| Canary | Low | Fast | Gradual rollout |\n",
    "| Blue-Green | Medium | Instant | Quick switches |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShadowDeployment:\n",
    "    \"\"\"Shadow deployment for comparing models.\"\"\"\n",
    "    \n",
    "    def __init__(self, primary, shadow):\n",
    "        self.primary = primary\n",
    "        self.shadow = shadow\n",
    "        self.log = []\n",
    "    \n",
    "    def predict(self, features):\n",
    "        features_2d = features.reshape(1, -1)\n",
    "        \n",
    "        start = time.time()\n",
    "        p_pred = self.primary.predict(features_2d)[0]\n",
    "        p_time = time.time() - start\n",
    "        \n",
    "        start = time.time()\n",
    "        s_pred = self.shadow.predict(features_2d)[0]\n",
    "        s_time = time.time() - start\n",
    "        \n",
    "        self.log.append({'primary': int(p_pred), 'shadow': int(s_pred), 'match': p_pred == s_pred})\n",
    "        return {'prediction': int(p_pred), 'latency_ms': p_time * 1000}\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        if not self.log:\n",
    "            return {}\n",
    "        df = pd.DataFrame(self.log)\n",
    "        return {'total': len(df), 'agreement_rate': df['match'].mean()}\n",
    "\n",
    "\n",
    "class CanaryDeployment:\n",
    "    \"\"\"Canary deployment for gradual rollout.\"\"\"\n",
    "    \n",
    "    def __init__(self, stable, canary, canary_pct=5.0):\n",
    "        self.stable = stable\n",
    "        self.canary = canary\n",
    "        self.canary_pct = canary_pct\n",
    "        self.stable_count = 0\n",
    "        self.canary_count = 0\n",
    "    \n",
    "    def predict(self, features):\n",
    "        features_2d = features.reshape(1, -1)\n",
    "        use_canary = np.random.random() * 100 < self.canary_pct\n",
    "        \n",
    "        model = self.canary if use_canary else self.stable\n",
    "        if use_canary:\n",
    "            self.canary_count += 1\n",
    "        else:\n",
    "            self.stable_count += 1\n",
    "        \n",
    "        pred = model.predict(features_2d)[0]\n",
    "        return {'prediction': int(pred), 'model': 'canary' if use_canary else 'stable'}\n",
    "    \n",
    "    def increase_canary(self, increment=5.0):\n",
    "        self.canary_pct = min(100, self.canary_pct + increment)\n",
    "        return self.canary_pct\n",
    "\n",
    "\n",
    "class BlueGreenDeployment:\n",
    "    \"\"\"Blue-green deployment for instant switches.\"\"\"\n",
    "    \n",
    "    def __init__(self, blue, green):\n",
    "        self.blue = blue\n",
    "        self.green = green\n",
    "        self.active = 'blue'\n",
    "    \n",
    "    def predict(self, features):\n",
    "        features_2d = features.reshape(1, -1)\n",
    "        model = self.blue if self.active == 'blue' else self.green\n",
    "        pred = model.predict(features_2d)[0]\n",
    "        return {'prediction': int(pred), 'environment': self.active}\n",
    "    \n",
    "    def switch(self):\n",
    "        self.active = 'green' if self.active == 'blue' else 'blue'\n",
    "        return self.active\n",
    "\n",
    "\n",
    "# Train alternative models\n",
    "model_v1 = RandomForestClassifier(n_estimators=30, random_state=42)\n",
    "model_v1.fit(X_train, y_train)\n",
    "\n",
    "model_v2 = GradientBoostingClassifier(n_estimators=50, random_state=42)\n",
    "model_v2.fit(X_train, y_train)\n",
    "\n",
    "print(\"Models trained for deployment patterns demonstration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Shadow Deployment\n",
    "print(\"SHADOW DEPLOYMENT\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "shadow = ShadowDeployment(model_v1, model_v2)\n",
    "for i in range(500):\n",
    "    shadow.predict(X_test[i])\n",
    "\n",
    "metrics = shadow.get_metrics()\n",
    "print(f\"Predictions: {metrics['total']}\")\n",
    "print(f\"Agreement rate: {metrics['agreement_rate']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Canary Deployment\n",
    "print(\"CANARY DEPLOYMENT\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "canary = CanaryDeployment(model_v1, model_v2, canary_pct=10.0)\n",
    "rollout_data = []\n",
    "\n",
    "for phase in range(5):\n",
    "    canary.stable_count = 0\n",
    "    canary.canary_count = 0\n",
    "    \n",
    "    for i in range(200):\n",
    "        canary.predict(X_test[i % len(X_test)])\n",
    "    \n",
    "    rollout_data.append({'phase': phase+1, 'pct': canary.canary_pct, \n",
    "                        'stable': canary.stable_count, 'canary': canary.canary_count})\n",
    "    print(f\"Phase {phase+1}: Canary {canary.canary_pct:.0f}% - Stable: {canary.stable_count}, Canary: {canary.canary_count}\")\n",
    "    canary.increase_canary(20)\n",
    "\n",
    "# Visualize rollout\n",
    "df = pd.DataFrame(rollout_data)\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.bar(df['phase'] - 0.2, df['stable'], 0.4, label='Stable', color='steelblue')\n",
    "ax.bar(df['phase'] + 0.2, df['canary'], 0.4, label='Canary', color='coral')\n",
    "ax.set_xlabel('Phase')\n",
    "ax.set_ylabel('Requests')\n",
    "ax.set_title('Canary Deployment: Traffic Shift')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Blue-Green Deployment\n",
    "print(\"BLUE-GREEN DEPLOYMENT\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "bg = BlueGreenDeployment(model_v1, model_v2)\n",
    "print(f\"Initial: {bg.active}\")\n",
    "\n",
    "results = [bg.predict(X_test[i]) for i in range(5)]\n",
    "print(f\"Predictions from {results[0]['environment']}\")\n",
    "\n",
    "new_env = bg.switch()\n",
    "print(f\"Switched to: {new_env}\")\n",
    "\n",
    "results2 = [bg.predict(X_test[i]) for i in range(5)]\n",
    "print(f\"Predictions from {results2[0]['environment']}\")\n",
    "\n",
    "rolled_back = bg.switch()\n",
    "print(f\"Rolled back to: {rolled_back}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Building Model Serving APIs <a name=\"6-model-serving-apis\"></a>\n",
    "\n",
    "Creating production-ready APIs for model serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelServingAPI:\n",
    "    \"\"\"Simulated FastAPI-style model serving.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, name='default', version='1.0'):\n",
    "        self.model = model\n",
    "        self.name = name\n",
    "        self.version = version\n",
    "        self.requests = 0\n",
    "        self.errors = 0\n",
    "        self.latencies = []\n",
    "    \n",
    "    def health(self):\n",
    "        \"\"\"GET /health\"\"\"\n",
    "        return {'status': 'healthy', 'model': self.name, 'version': self.version}\n",
    "    \n",
    "    def predict(self, request):\n",
    "        \"\"\"POST /predict\"\"\"\n",
    "        start = time.time()\n",
    "        self.requests += 1\n",
    "        \n",
    "        try:\n",
    "            if 'features' not in request:\n",
    "                return {'error': 'Missing features', 'status': 400}\n",
    "            \n",
    "            features = np.array(request['features'])\n",
    "            if features.ndim == 1:\n",
    "                features = features.reshape(1, -1)\n",
    "            \n",
    "            preds = self.model.predict(features)\n",
    "            proba = self.model.predict_proba(features) if hasattr(self.model, 'predict_proba') else None\n",
    "            \n",
    "            latency = time.time() - start\n",
    "            self.latencies.append(latency)\n",
    "            \n",
    "            response = {'predictions': preds.tolist(), 'latency_ms': latency * 1000, 'status': 200}\n",
    "            if proba is not None:\n",
    "                response['probabilities'] = proba.tolist()\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            self.errors += 1\n",
    "            return {'error': str(e), 'status': 500}\n",
    "    \n",
    "    def metrics(self):\n",
    "        \"\"\"GET /metrics\"\"\"\n",
    "        lats = [l * 1000 for l in self.latencies]\n",
    "        return {\n",
    "            'requests_total': self.requests,\n",
    "            'errors_total': self.errors,\n",
    "            'latency_p50': np.percentile(lats, 50) if lats else 0,\n",
    "            'latency_p99': np.percentile(lats, 99) if lats else 0\n",
    "        }\n",
    "\n",
    "\n",
    "# Create and test API\n",
    "api = ModelServingAPI(model, 'classifier', '1.0.0')\n",
    "\n",
    "print(\"Health Check:\", api.health())\n",
    "print(\"\\nPrediction:\", api.predict({'features': X_test[0].tolist()}))\n",
    "\n",
    "# Load test\n",
    "print(\"\\nLoad testing (1000 requests)...\")\n",
    "for i in range(1000):\n",
    "    api.predict({'features': X_test[i % len(X_test)].tolist()})\n",
    "\n",
    "print(\"Metrics:\", api.metrics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Prediction Pipeline Design <a name=\"7-pipeline-design\"></a>\n",
    "\n",
    "End-to-end pipelines include preprocessing, prediction, and postprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionPipeline:\n",
    "    \"\"\"Complete prediction pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self, preprocessor, model):\n",
    "        self.preprocessor = preprocessor\n",
    "        self.model = model\n",
    "        self.timings = {'preprocess': [], 'predict': [], 'postprocess': []}\n",
    "    \n",
    "    def preprocess(self, raw_input):\n",
    "        start = time.time()\n",
    "        features = self.preprocessor.transform(raw_input) if self.preprocessor else raw_input\n",
    "        self.timings['preprocess'].append(time.time() - start)\n",
    "        return features\n",
    "    \n",
    "    def predict(self, features):\n",
    "        start = time.time()\n",
    "        if features.ndim == 1:\n",
    "            features = features.reshape(1, -1)\n",
    "        preds = self.model.predict(features)\n",
    "        proba = self.model.predict_proba(features) if hasattr(self.model, 'predict_proba') else None\n",
    "        self.timings['predict'].append(time.time() - start)\n",
    "        return preds, proba\n",
    "    \n",
    "    def postprocess(self, preds, proba):\n",
    "        start = time.time()\n",
    "        class_names = {0: 'Class A', 1: 'Class B', 2: 'Class C'}\n",
    "        results = []\n",
    "        for i, pred in enumerate(preds):\n",
    "            result = {'class_id': int(pred), 'class_name': class_names.get(int(pred), 'Unknown')}\n",
    "            if proba is not None:\n",
    "                result['confidence'] = float(max(proba[i]))\n",
    "            results.append(result)\n",
    "        self.timings['postprocess'].append(time.time() - start)\n",
    "        return results\n",
    "    \n",
    "    def run(self, raw_input):\n",
    "        features = self.preprocess(raw_input)\n",
    "        preds, proba = self.predict(features)\n",
    "        return self.postprocess(preds, proba)\n",
    "    \n",
    "    def get_timing(self):\n",
    "        return {k: np.mean(v) * 1000 if v else 0 for k, v in self.timings.items()}\n",
    "\n",
    "\n",
    "# Create pipeline\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "pipeline = PredictionPipeline(scaler, model)\n",
    "\n",
    "# Run predictions\n",
    "for i in range(100):\n",
    "    pipeline.run(X_test[i:i+1])\n",
    "\n",
    "example = pipeline.run(X_test[0:1])[0]\n",
    "print(\"Example Result:\")\n",
    "print(f\"  Class: {example['class_name']}\")\n",
    "print(f\"  Confidence: {example['confidence']:.2%}\")\n",
    "\n",
    "print(\"\\nTiming Breakdown:\")\n",
    "timing = pipeline.get_timing()\n",
    "total = sum(timing.values())\n",
    "for stage, ms in timing.items():\n",
    "    print(f\"  {stage}: {ms:.3f}ms ({ms/total*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize pipeline timing\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.pie(timing.values(), labels=timing.keys(), autopct='%1.1f%%', colors=['steelblue', 'coral', 'forestgreen'])\n",
    "ax.set_title('Pipeline Stage Timing Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Hands-on Exercises <a name=\"8-exercises\"></a>\n",
    "\n",
    "### Exercise 1: Deployment Decision\n",
    "Evaluate deployment options for a voice assistant that needs:\n",
    "- Max latency: 50ms\n",
    "- Model size: 200MB\n",
    "- Must work offline\n",
    "- High privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 Solution\n",
    "print(\"Exercise 1: Voice Assistant Deployment\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "results = framework.evaluate_requirements(\n",
    "    max_latency_ms=50,\n",
    "    model_size_mb=200,\n",
    "    requires_offline=True,\n",
    "    privacy_req='high',\n",
    "    budget=50.0\n",
    ")\n",
    "\n",
    "for i, (key, opt, score, reasons) in enumerate(results[:3], 1):\n",
    "    print(f\"{i}. {opt.name}: Score {score:.0f}\")\n",
    "    for r in reasons:\n",
    "        print(f\"   - {r}\")\n",
    "\n",
    "print(\"\\nRecommendation: Consider model compression or edge server hybrid approach\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement A/B Testing\n",
    "Create an A/B testing deployment that routes 50% traffic to each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 Solution\n",
    "class ABTestDeployment:\n",
    "    \"\"\"A/B testing deployment.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_a, model_b, split=0.5):\n",
    "        self.model_a = model_a\n",
    "        self.model_b = model_b\n",
    "        self.split = split\n",
    "        self.results_a = []\n",
    "        self.results_b = []\n",
    "    \n",
    "    def predict(self, features, user_id=None):\n",
    "        # Deterministic assignment if user_id provided\n",
    "        if user_id:\n",
    "            use_a = hash(user_id) % 100 < self.split * 100\n",
    "        else:\n",
    "            use_a = np.random.random() < self.split\n",
    "        \n",
    "        features_2d = features.reshape(1, -1)\n",
    "        model = self.model_a if use_a else self.model_b\n",
    "        \n",
    "        pred = model.predict(features_2d)[0]\n",
    "        variant = 'A' if use_a else 'B'\n",
    "        \n",
    "        if use_a:\n",
    "            self.results_a.append(pred)\n",
    "        else:\n",
    "            self.results_b.append(pred)\n",
    "        \n",
    "        return {'prediction': int(pred), 'variant': variant}\n",
    "    \n",
    "    def get_stats(self):\n",
    "        return {\n",
    "            'variant_a_count': len(self.results_a),\n",
    "            'variant_b_count': len(self.results_b),\n",
    "            'split_actual': len(self.results_a) / (len(self.results_a) + len(self.results_b)) if self.results_a or self.results_b else 0\n",
    "        }\n",
    "\n",
    "\n",
    "# Test A/B deployment\n",
    "ab = ABTestDeployment(model_v1, model_v2, split=0.5)\n",
    "\n",
    "for i in range(1000):\n",
    "    ab.predict(X_test[i % len(X_test)])\n",
    "\n",
    "stats = ab.get_stats()\n",
    "print(f\"A/B Test Results:\")\n",
    "print(f\"  Variant A: {stats['variant_a_count']} requests\")\n",
    "print(f\"  Variant B: {stats['variant_b_count']} requests\")\n",
    "print(f\"  Actual split: {stats['split_actual']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Summary <a name=\"9-summary\"></a>\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Deployment Location**: Choose between cloud, edge, and on-device based on latency, privacy, and compute needs\n",
    "\n",
    "2. **Batch vs Online**: Use batch for high-throughput, scheduled workloads; online for real-time, low-latency needs\n",
    "\n",
    "3. **Deployment Patterns**:\n",
    "   - Shadow: Test new models without user impact\n",
    "   - Canary: Gradual rollout with quick rollback\n",
    "   - Blue-Green: Instant switches between versions\n",
    "\n",
    "4. **Production APIs**: Include health checks, metrics, proper error handling\n",
    "\n",
    "5. **Pipelines**: Design end-to-end with preprocessing, prediction, postprocessing\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- Always start with shadow deployment for new models\n",
    "- Use caching to reduce latency and compute costs\n",
    "- Monitor latency percentiles (p50, p95, p99)\n",
    "- Design for rollback from day one\n",
    "- Track model version in all predictions\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Tutorial 15: Model Compression Techniques\n",
    "- Tutorial 16: Serving and Prediction Pipelines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
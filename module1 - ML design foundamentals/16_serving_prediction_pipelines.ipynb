{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 16: Serving and Prediction Pipelines\n",
    "\n",
    "## Module 6: Deployment and Serving\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "\n",
    "1. **Design scalable serving systems** - Build high-throughput prediction services\n",
    "2. **Implement model versioning** - Manage multiple model versions safely\n",
    "3. **Build feature stores** - Create and use feature stores for serving\n",
    "4. **Create prediction pipelines** - End-to-end serving with pre/post processing\n",
    "5. **Handle batch and streaming** - Support different prediction patterns\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction to Model Serving](#1-introduction)\n",
    "2. [Serving Architecture Patterns](#2-architecture)\n",
    "3. [Model Versioning and Registry](#3-versioning)\n",
    "4. [Feature Stores](#4-feature-stores)\n",
    "5. [Prediction Pipelines](#5-pipelines)\n",
    "6. [Scaling and Load Balancing](#6-scaling)\n",
    "7. [Hands-on Exercises](#7-exercises)\n",
    "8. [Summary](#8-summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Introduction to Model Serving <a name=\"1-introduction\"></a>\n",
    "\n",
    "Model serving deploys ML models to handle prediction requests.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "| Requirement | Target |\n",
    "|-------------|--------|\n",
    "| Latency | <100ms p99 |\n",
    "| Availability | 99.9%+ |\n",
    "| Throughput | 10,000+ QPS |\n",
    "| Scalability | Auto-scale |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "X, y = make_classification(n_samples=5000, n_features=20, n_informative=15,\n",
    "                          n_classes=3, n_clusters_per_class=2, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train models\n",
    "model_v1 = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "model_v1.fit(X_train_scaled, y_train)\n",
    "\n",
    "model_v2 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_v2.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Data: {X_train.shape[0]} train, {X_test.shape[0]} test\")\n",
    "print(f\"Model v1 acc: {accuracy_score(y_test, model_v1.predict(X_test_scaled)):.4f}\")\n",
    "print(f\"Model v2 acc: {accuracy_score(y_test, model_v2.predict(X_test_scaled)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Serving Architecture Patterns <a name=\"2-architecture\"></a>\n",
    "\n",
    "| Pattern | Latency | Scalability | Use Case |\n",
    "|---------|---------|-------------|----------|\n",
    "| Monolithic | Low | Limited | Simple apps |\n",
    "| Microservices | Medium | High | Complex systems |\n",
    "| Serverless | Variable | Auto | Bursty traffic |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionService(ABC):\n",
    "    @abstractmethod\n",
    "    def predict(self, features: np.ndarray) -> Dict:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def health_check(self) -> Dict:\n",
    "        pass\n",
    "\n",
    "\n",
    "class MonolithicService(PredictionService):\n",
    "    def __init__(self, model, preprocessor=None):\n",
    "        self.model = model\n",
    "        self.preprocessor = preprocessor\n",
    "        self.requests = 0\n",
    "        self.latencies = []\n",
    "    \n",
    "    def predict(self, features: np.ndarray) -> Dict:\n",
    "        start = time.time()\n",
    "        self.requests += 1\n",
    "        \n",
    "        if self.preprocessor:\n",
    "            features = self.preprocessor.transform(features.reshape(1, -1))\n",
    "        else:\n",
    "            features = features.reshape(1, -1)\n",
    "        \n",
    "        pred = self.model.predict(features)[0]\n",
    "        proba = self.model.predict_proba(features)[0] if hasattr(self.model, 'predict_proba') else None\n",
    "        \n",
    "        latency = time.time() - start\n",
    "        self.latencies.append(latency)\n",
    "        \n",
    "        return {\n",
    "            'prediction': int(pred),\n",
    "            'probability': proba.tolist() if proba is not None else None,\n",
    "            'latency_ms': latency * 1000\n",
    "        }\n",
    "    \n",
    "    def health_check(self) -> Dict:\n",
    "        return {'status': 'healthy', 'requests': self.requests}\n",
    "\n",
    "\n",
    "class MicroserviceGateway:\n",
    "    def __init__(self):\n",
    "        self.services: Dict[str, PredictionService] = {}\n",
    "        self.routes: Dict[str, str] = {}\n",
    "    \n",
    "    def register(self, name: str, service: PredictionService):\n",
    "        self.services[name] = service\n",
    "    \n",
    "    def set_route(self, endpoint: str, service_name: str):\n",
    "        self.routes[endpoint] = service_name\n",
    "    \n",
    "    def route(self, endpoint: str, features: np.ndarray) -> Dict:\n",
    "        if endpoint not in self.routes:\n",
    "            return {'error': 'Route not found'}\n",
    "        service_name = self.routes[endpoint]\n",
    "        if service_name not in self.services:\n",
    "            return {'error': 'Service not found'}\n",
    "        result = self.services[service_name].predict(features)\n",
    "        result['service'] = service_name\n",
    "        return result\n",
    "\n",
    "\n",
    "# Demo\n",
    "print(\"=\"*50)\n",
    "print(\"Service Architecture Demo\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Monolithic\n",
    "mono = MonolithicService(model_v1, scaler)\n",
    "for i in range(100):\n",
    "    mono.predict(X_test[i])\n",
    "print(f\"Monolithic: {mono.requests} requests, avg latency: {np.mean(mono.latencies)*1000:.3f}ms\")\n",
    "\n",
    "# Microservices\n",
    "gateway = MicroserviceGateway()\n",
    "gateway.register('v1', MonolithicService(model_v1, scaler))\n",
    "gateway.register('v2', MonolithicService(model_v2, scaler))\n",
    "gateway.set_route('/api/v1/predict', 'v1')\n",
    "gateway.set_route('/api/v2/predict', 'v2')\n",
    "\n",
    "r1 = gateway.route('/api/v1/predict', X_test[0])\n",
    "r2 = gateway.route('/api/v2/predict', X_test[0])\n",
    "print(f\"Gateway v1: pred={r1['prediction']}, Gateway v2: pred={r2['prediction']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Model Versioning and Registry <a name=\"3-versioning\"></a>\n",
    "\n",
    "Model versioning enables safe deployments and rollbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelVersion:\n",
    "    version: str\n",
    "    model: Any\n",
    "    created_at: datetime\n",
    "    metrics: Dict = field(default_factory=dict)\n",
    "    status: str = 'staging'\n",
    "\n",
    "\n",
    "class ModelRegistry:\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.versions: Dict[str, ModelVersion] = {}\n",
    "        self.production: Optional[str] = None\n",
    "        self.history: List[Dict] = []\n",
    "    \n",
    "    def register(self, version: str, model: Any, metrics: Dict = None):\n",
    "        mv = ModelVersion(version, model, datetime.now(), metrics or {})\n",
    "        self.versions[version] = mv\n",
    "        self.history.append({'action': 'register', 'version': version, 'time': datetime.now().isoformat()})\n",
    "        print(f\"Registered: {version}\")\n",
    "    \n",
    "    def promote(self, version: str):\n",
    "        if version not in self.versions:\n",
    "            raise ValueError(f\"Version {version} not found\")\n",
    "        if self.production:\n",
    "            self.versions[self.production].status = 'archived'\n",
    "        self.versions[version].status = 'production'\n",
    "        self.production = version\n",
    "        self.history.append({'action': 'promote', 'version': version, 'time': datetime.now().isoformat()})\n",
    "        print(f\"Promoted: {version}\")\n",
    "    \n",
    "    def rollback(self):\n",
    "        promotions = [h for h in self.history if h['action'] == 'promote']\n",
    "        if len(promotions) < 2:\n",
    "            return None\n",
    "        prev = promotions[-2]['version']\n",
    "        self.promote(prev)\n",
    "        return prev\n",
    "    \n",
    "    def get_production(self):\n",
    "        return self.versions[self.production].model if self.production else None\n",
    "    \n",
    "    def list_versions(self) -> pd.DataFrame:\n",
    "        data = [{'version': v, 'status': mv.status, 'accuracy': mv.metrics.get('accuracy', 'N/A')}\n",
    "                for v, mv in self.versions.items()]\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Demo registry\n",
    "print(\"=\"*50)\n",
    "print(\"Model Registry Demo\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "registry = ModelRegistry('classifier')\n",
    "registry.register('1.0.0', model_v1, {'accuracy': accuracy_score(y_test, model_v1.predict(X_test_scaled))})\n",
    "registry.register('2.0.0', model_v2, {'accuracy': accuracy_score(y_test, model_v2.predict(X_test_scaled))})\n",
    "\n",
    "registry.promote('1.0.0')\n",
    "registry.promote('2.0.0')\n",
    "registry.rollback()\n",
    "\n",
    "print(\"\\nVersions:\")\n",
    "print(registry.list_versions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Feature Stores <a name=\"4-feature-stores\"></a>\n",
    "\n",
    "Feature stores provide consistent feature serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureStore:\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.features: Dict[str, Dict] = {}  # feature definitions\n",
    "        self.online: Dict[str, Dict] = {}    # entity -> features\n",
    "        self.offline: List[Dict] = []        # historical data\n",
    "    \n",
    "    def register_feature(self, name: str, dtype: str, description: str = ''):\n",
    "        self.features[name] = {'dtype': dtype, 'description': description}\n",
    "    \n",
    "    def ingest(self, entity_id: str, features: Dict):\n",
    "        if entity_id not in self.online:\n",
    "            self.online[entity_id] = {}\n",
    "        self.online[entity_id].update(features)\n",
    "        self.online[entity_id]['_updated'] = datetime.now().isoformat()\n",
    "        self.offline.append({'entity_id': entity_id, 'timestamp': datetime.now().isoformat(), **features})\n",
    "    \n",
    "    def get_online(self, entity_id: str, feature_names: List[str] = None) -> Dict:\n",
    "        if entity_id not in self.online:\n",
    "            return {'error': 'Entity not found'}\n",
    "        features = self.online[entity_id]\n",
    "        if feature_names:\n",
    "            features = {k: v for k, v in features.items() if k in feature_names or k.startswith('_')}\n",
    "        return features\n",
    "    \n",
    "    def get_offline(self) -> pd.DataFrame:\n",
    "        return pd.DataFrame(self.offline)\n",
    "\n",
    "\n",
    "# Demo feature store\n",
    "print(\"=\"*50)\n",
    "print(\"Feature Store Demo\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "fs = FeatureStore('user_features')\n",
    "fs.register_feature('age', 'float', 'User age')\n",
    "fs.register_feature('income', 'float', 'Annual income')\n",
    "fs.register_feature('purchases', 'int', 'Purchase count')\n",
    "\n",
    "users = [\n",
    "    ('u001', {'age': 25, 'income': 50000, 'purchases': 10}),\n",
    "    ('u002', {'age': 35, 'income': 80000, 'purchases': 25}),\n",
    "    ('u003', {'age': 45, 'income': 120000, 'purchases': 50}),\n",
    "]\n",
    "\n",
    "for uid, feats in users:\n",
    "    fs.ingest(uid, feats)\n",
    "\n",
    "print(\"Online features for u001:\")\n",
    "print(fs.get_online('u001'))\n",
    "\n",
    "print(\"\\nOffline data:\")\n",
    "print(fs.get_offline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Prediction Pipelines <a name=\"5-pipelines\"></a>\n",
    "\n",
    "End-to-end pipelines with preprocessing and postprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineStage(ABC):\n",
    "    @abstractmethod\n",
    "    def process(self, data: Any) -> Any:\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def name(self) -> str:\n",
    "        pass\n",
    "\n",
    "\n",
    "class PreprocessingStage(PipelineStage):\n",
    "    def __init__(self, scaler):\n",
    "        self.scaler = scaler\n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'preprocessing'\n",
    "    \n",
    "    def process(self, data):\n",
    "        if data.ndim == 1:\n",
    "            data = data.reshape(1, -1)\n",
    "        return self.scaler.transform(data)\n",
    "\n",
    "\n",
    "class PredictionStage(PipelineStage):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'prediction'\n",
    "    \n",
    "    def process(self, data):\n",
    "        pred = self.model.predict(data)[0]\n",
    "        proba = self.model.predict_proba(data)[0] if hasattr(self.model, 'predict_proba') else None\n",
    "        return {'prediction': int(pred), 'probabilities': proba}\n",
    "\n",
    "\n",
    "class PostprocessingStage(PipelineStage):\n",
    "    def __init__(self, class_names: Dict[int, str]):\n",
    "        self.class_names = class_names\n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'postprocessing'\n",
    "    \n",
    "    def process(self, data):\n",
    "        pred = data['prediction']\n",
    "        proba = data['probabilities']\n",
    "        return {\n",
    "            'class_id': pred,\n",
    "            'class_name': self.class_names.get(pred, 'unknown'),\n",
    "            'confidence': float(max(proba)) if proba is not None else None\n",
    "        }\n",
    "\n",
    "\n",
    "class ServingPipeline:\n",
    "    def __init__(self):\n",
    "        self.stages: List[PipelineStage] = []\n",
    "        self.timings: Dict[str, List] = defaultdict(list)\n",
    "    \n",
    "    def add_stage(self, stage: PipelineStage):\n",
    "        self.stages.append(stage)\n",
    "    \n",
    "    def run(self, input_data):\n",
    "        total_start = time.time()\n",
    "        data = input_data\n",
    "        times = {}\n",
    "        \n",
    "        for stage in self.stages:\n",
    "            start = time.time()\n",
    "            data = stage.process(data)\n",
    "            elapsed = time.time() - start\n",
    "            times[stage.name] = elapsed\n",
    "            self.timings[stage.name].append(elapsed)\n",
    "        \n",
    "        if isinstance(data, dict):\n",
    "            data['_timing'] = times\n",
    "            data['_total_ms'] = (time.time() - total_start) * 1000\n",
    "        return data\n",
    "    \n",
    "    def get_stats(self):\n",
    "        return pd.DataFrame([{\n",
    "            'stage': s,\n",
    "            'avg_ms': np.mean(t) * 1000,\n",
    "            'p99_ms': np.percentile(t, 99) * 1000\n",
    "        } for s, t in self.timings.items()])\n",
    "\n",
    "\n",
    "# Demo pipeline\n",
    "print(\"=\"*50)\n",
    "print(\"Serving Pipeline Demo\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "pipeline = ServingPipeline()\n",
    "pipeline.add_stage(PreprocessingStage(scaler))\n",
    "pipeline.add_stage(PredictionStage(model_v2))\n",
    "pipeline.add_stage(PostprocessingStage({0: 'Low', 1: 'Medium', 2: 'High'}))\n",
    "\n",
    "for i in range(100):\n",
    "    pipeline.run(X_test[i])\n",
    "\n",
    "result = pipeline.run(X_test[0])\n",
    "print(\"Example output:\")\n",
    "print(json.dumps(result, indent=2, default=str))\n",
    "\n",
    "print(\"\\nPipeline Stats:\")\n",
    "print(pipeline.get_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize pipeline timing\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "stats = pipeline.get_stats()\n",
    "axes[0].barh(stats['stage'], stats['avg_ms'], color='steelblue')\n",
    "axes[0].set_xlabel('Avg Time (ms)')\n",
    "axes[0].set_title('Pipeline Stage Timing')\n",
    "\n",
    "# Total latency distribution\n",
    "total_times = [sum(pipeline.timings[s][i] for s in pipeline.timings) * 1000 \n",
    "               for i in range(len(list(pipeline.timings.values())[0]))]\n",
    "axes[1].hist(total_times, bins=30, color='coral', edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(np.mean(total_times), color='red', linestyle='--')\n",
    "axes[1].set_xlabel('Latency (ms)')\n",
    "axes[1].set_title('Total Latency Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Scaling and Load Balancing <a name=\"6-scaling\"></a>\n",
    "\n",
    "Handle high throughput with scaling and load balancing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadBalancer:\n",
    "    def __init__(self, strategy='round_robin'):\n",
    "        self.servers: List[PredictionService] = []\n",
    "        self.strategy = strategy\n",
    "        self.current = 0\n",
    "        self.counts = []\n",
    "    \n",
    "    def add_server(self, server: PredictionService):\n",
    "        self.servers.append(server)\n",
    "        self.counts.append(0)\n",
    "    \n",
    "    def get_server(self):\n",
    "        if not self.servers:\n",
    "            raise ValueError(\"No servers\")\n",
    "        \n",
    "        if self.strategy == 'round_robin':\n",
    "            idx = self.current\n",
    "            self.current = (self.current + 1) % len(self.servers)\n",
    "        elif self.strategy == 'least_connections':\n",
    "            idx = np.argmin(self.counts)\n",
    "        else:\n",
    "            idx = np.random.randint(len(self.servers))\n",
    "        \n",
    "        return idx, self.servers[idx]\n",
    "    \n",
    "    def route(self, features: np.ndarray) -> Dict:\n",
    "        idx, server = self.get_server()\n",
    "        self.counts[idx] += 1\n",
    "        result = server.predict(features)\n",
    "        result['server_id'] = idx\n",
    "        return result\n",
    "    \n",
    "    def stats(self) -> pd.DataFrame:\n",
    "        return pd.DataFrame([{'server': i, 'requests': c} for i, c in enumerate(self.counts)])\n",
    "\n",
    "\n",
    "# Demo load balancing\n",
    "print(\"=\"*50)\n",
    "print(\"Load Balancing Demo\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "lb = LoadBalancer('round_robin')\n",
    "for i in range(3):\n",
    "    lb.add_server(MonolithicService(model_v2, scaler))\n",
    "\n",
    "for i in range(300):\n",
    "    lb.route(X_test[i % len(X_test)])\n",
    "\n",
    "print(\"Load Distribution:\")\n",
    "print(lb.stats())\n",
    "\n",
    "# Compare strategies\n",
    "print(\"\\nStrategy Comparison:\")\n",
    "for strategy in ['round_robin', 'least_connections', 'random']:\n",
    "    lb_test = LoadBalancer(strategy)\n",
    "    for i in range(3):\n",
    "        lb_test.add_server(MonolithicService(model_v2, scaler))\n",
    "    for i in range(300):\n",
    "        lb_test.route(X_test[i % len(X_test)])\n",
    "    print(f\"  {strategy}: std={np.std(lb_test.counts):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize load balancing\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "strategies = ['round_robin', 'least_connections', 'random']\n",
    "distributions = []\n",
    "\n",
    "for strategy in strategies:\n",
    "    lb_test = LoadBalancer(strategy)\n",
    "    for i in range(3):\n",
    "        lb_test.add_server(MonolithicService(model_v2, scaler))\n",
    "    for i in range(300):\n",
    "        lb_test.route(X_test[i % len(X_test)])\n",
    "    distributions.append(lb_test.counts)\n",
    "\n",
    "x = np.arange(3)\n",
    "width = 0.25\n",
    "\n",
    "for i, (strategy, dist) in enumerate(zip(strategies, distributions)):\n",
    "    ax.bar(x + i*width, dist, width, label=strategy)\n",
    "\n",
    "ax.set_xlabel('Server')\n",
    "ax.set_ylabel('Requests')\n",
    "ax.set_title('Load Distribution by Strategy')\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(['Server 0', 'Server 1', 'Server 2'])\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Hands-on Exercises <a name=\"7-exercises\"></a>\n",
    "\n",
    "### Exercise 1: Build a Complete Serving System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Complete serving system\n",
    "print(\"Exercise 1: Complete Serving System\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create registry\n",
    "reg = ModelRegistry('production')\n",
    "reg.register('v1', model_v1, {'accuracy': 0.85})\n",
    "reg.register('v2', model_v2, {'accuracy': 0.90})\n",
    "reg.promote('v2')\n",
    "\n",
    "# Create pipeline with production model\n",
    "prod_pipeline = ServingPipeline()\n",
    "prod_pipeline.add_stage(PreprocessingStage(scaler))\n",
    "prod_pipeline.add_stage(PredictionStage(reg.get_production()))\n",
    "prod_pipeline.add_stage(PostprocessingStage({0: 'A', 1: 'B', 2: 'C'}))\n",
    "\n",
    "# Create load balancer with multiple instances\n",
    "final_lb = LoadBalancer('round_robin')\n",
    "for i in range(3):\n",
    "    final_lb.add_server(MonolithicService(reg.get_production(), scaler))\n",
    "\n",
    "# Run traffic\n",
    "for i in range(100):\n",
    "    final_lb.route(X_test[i % len(X_test)])\n",
    "\n",
    "print(f\"Production model: {reg.production}\")\n",
    "print(f\"Load distribution: {final_lb.counts}\")\n",
    "print(\"\\nSample prediction:\")\n",
    "print(prod_pipeline.run(X_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Feature Store Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Feature store integration\n",
    "print(\"\\nExercise 2: Feature Store Integration\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "class FeatureServingPipeline:\n",
    "    def __init__(self, feature_store, model, feature_names):\n",
    "        self.fs = feature_store\n",
    "        self.model = model\n",
    "        self.feature_names = feature_names\n",
    "    \n",
    "    def predict(self, entity_id):\n",
    "        # Get features\n",
    "        features = self.fs.get_online(entity_id, self.feature_names)\n",
    "        if 'error' in features:\n",
    "            return features\n",
    "        \n",
    "        # Build vector\n",
    "        vector = np.array([features.get(f, 0) for f in self.feature_names]).reshape(1, -1)\n",
    "        \n",
    "        # Predict\n",
    "        pred = self.model.predict(vector)[0]\n",
    "        return {'entity_id': entity_id, 'prediction': int(pred), 'features': self.feature_names}\n",
    "\n",
    "\n",
    "# Simple model for demo\n",
    "simple_model = LogisticRegression(random_state=42)\n",
    "demo_X = np.array([[25, 50000, 10], [35, 80000, 25], [45, 120000, 50]])\n",
    "demo_y = np.array([0, 1, 1])\n",
    "simple_model.fit(demo_X, demo_y)\n",
    "\n",
    "fs_pipeline = FeatureServingPipeline(fs, simple_model, ['age', 'income', 'purchases'])\n",
    "\n",
    "for uid, _ in users:\n",
    "    result = fs_pipeline.predict(uid)\n",
    "    print(f\"{uid}: prediction={result['prediction']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Summary <a name=\"8-summary\"></a>\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Serving Architectures**: Choose based on latency, scalability, and complexity needs\n",
    "\n",
    "2. **Model Versioning**: Essential for safe deployments and rollbacks\n",
    "\n",
    "3. **Feature Stores**: Provide consistent feature serving for training and inference\n",
    "\n",
    "4. **Prediction Pipelines**: Modular stages for preprocessing, prediction, postprocessing\n",
    "\n",
    "5. **Scaling**: Load balancing distributes traffic across servers\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- Use model registry for version management\n",
    "- Implement health checks for all services\n",
    "- Monitor latency percentiles (p50, p95, p99)\n",
    "- Design for horizontal scaling\n",
    "- Cache frequently accessed features\n",
    "\n",
    "### Production Checklist\n",
    "\n",
    "- [ ] Model versioning and registry\n",
    "- [ ] Feature store for online serving\n",
    "- [ ] Load balancing across servers\n",
    "- [ ] Health checks and monitoring\n",
    "- [ ] Rollback capability\n",
    "- [ ] Request logging\n",
    "\n",
    "### Module 6 Complete!\n",
    "\n",
    "You've learned:\n",
    "- Tutorial 14: Deployment Strategies\n",
    "- Tutorial 15: Model Compression\n",
    "- Tutorial 16: Serving and Prediction Pipelines\n",
    "\n",
    "Next: Module 7 - Monitoring and Infrastructure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
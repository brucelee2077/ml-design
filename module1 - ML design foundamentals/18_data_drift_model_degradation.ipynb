{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 18: Data Drift and Model Degradation\n",
    "\n",
    "## Module 7: Monitoring and Infrastructure\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Understand different types of data drift (covariate shift, label shift, concept drift)\n",
    "2. Implement statistical tests for drift detection (KS test, chi-square, PSI)\n",
    "3. Build automated drift detection pipelines\n",
    "4. Design model retraining strategies\n",
    "5. Monitor model degradation over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(42)\n",
    "print('Setup complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Types of Data Drift\n",
    "\n",
    "| Type | Definition | Detection |\n",
    "|------|------------|-----------|  \n",
    "| **Covariate Shift** | P(X) changes, P(Y|X) constant | Feature distribution tests |\n",
    "| **Label Shift** | P(Y) changes, P(X|Y) constant | Label distribution monitoring |\n",
    "| **Concept Drift** | P(Y|X) changes | Model performance monitoring |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DriftType(Enum):\n",
    "    COVARIATE = 'covariate_shift'\n",
    "    LABEL = 'label_shift'\n",
    "    CONCEPT = 'concept_drift'\n",
    "\n",
    "@dataclass\n",
    "class DriftInfo:\n",
    "    drift_type: DriftType\n",
    "    description: str\n",
    "    detection_method: str\n",
    "\n",
    "DRIFT_CATALOG = {\n",
    "    DriftType.COVARIATE: DriftInfo(\n",
    "        DriftType.COVARIATE,\n",
    "        'Input feature distribution changes while relationship stays same',\n",
    "        'Statistical tests on features (KS, PSI, chi-square)'\n",
    "    ),\n",
    "    DriftType.LABEL: DriftInfo(\n",
    "        DriftType.LABEL,\n",
    "        'Target distribution changes (class imbalance shifts)',\n",
    "        'Monitor label distribution over time'\n",
    "    ),\n",
    "    DriftType.CONCEPT: DriftInfo(\n",
    "        DriftType.CONCEPT,\n",
    "        'Relationship between features and target changes',\n",
    "        'Monitor model accuracy, compare to holdout'\n",
    "    )\n",
    "}\n",
    "\n",
    "print('TYPES OF DATA DRIFT')\n",
    "print('=' * 60)\n",
    "for dtype, info in DRIFT_CATALOG.items():\n",
    "    print(f'\\n{info.drift_type.value.upper()}')\n",
    "    print(f'  {info.description}')\n",
    "    print(f'  Detection: {info.detection_method}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize drift types\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Covariate Shift\n",
    "ax = axes[0, 0]\n",
    "x_train = np.random.normal(0, 1, 1000)\n",
    "x_prod = np.random.normal(1.5, 1.2, 1000)\n",
    "ax.hist(x_train, bins=50, alpha=0.5, label='Training', color='blue', density=True)\n",
    "ax.hist(x_prod, bins=50, alpha=0.5, label='Production', color='red', density=True)\n",
    "ax.set_title('Covariate Shift: P(X) Changes')\n",
    "ax.legend()\n",
    "\n",
    "# Label Shift\n",
    "ax = axes[0, 1]\n",
    "ax.bar(['Train 0', 'Train 1', 'Prod 0', 'Prod 1'], [0.9, 0.1, 0.7, 0.3],\n",
    "       color=['blue', 'blue', 'red', 'red'], alpha=0.7)\n",
    "ax.set_title('Label Shift: P(Y) Changes')\n",
    "\n",
    "# Concept Drift\n",
    "ax = axes[1, 0]\n",
    "x = np.linspace(0, 10, 100)\n",
    "ax.scatter(x, 2*x + 1 + np.random.normal(0, 1, 100), alpha=0.5, label='Training', color='blue')\n",
    "ax.scatter(x, 0.5*x + 5 + np.random.normal(0, 1, 100), alpha=0.5, label='Production', color='red')\n",
    "ax.set_title('Concept Drift: P(Y|X) Changes')\n",
    "ax.legend()\n",
    "\n",
    "# Performance over time\n",
    "ax = axes[1, 1]\n",
    "time = np.arange(100)\n",
    "perf = 0.95 - 0.002*time - 0.1*(time > 50) + np.random.normal(0, 0.02, 100)\n",
    "ax.plot(time, perf, color='green')\n",
    "ax.axvline(50, color='red', linestyle='--', label='Drift Point')\n",
    "ax.set_title('Model Performance Over Time')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Statistical Tests for Drift Detection\n",
    "\n",
    "- **KS Test**: Continuous features\n",
    "- **Chi-Square**: Categorical features  \n",
    "- **PSI**: Industry standard for drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DriftDetector:\n",
    "    def __init__(self, significance_level: float = 0.05):\n",
    "        self.significance_level = significance_level\n",
    "    \n",
    "    def ks_test(self, reference: np.ndarray, current: np.ndarray) -> Dict:\n",
    "        \"\"\"Kolmogorov-Smirnov test for continuous features.\"\"\"\n",
    "        statistic, p_value = stats.ks_2samp(reference, current)\n",
    "        return {\n",
    "            'test': 'ks_test',\n",
    "            'statistic': statistic,\n",
    "            'p_value': p_value,\n",
    "            'drift_detected': p_value < self.significance_level\n",
    "        }\n",
    "    \n",
    "    def psi(self, reference: np.ndarray, current: np.ndarray, bins: int = 10) -> Dict:\n",
    "        \"\"\"Population Stability Index.\"\"\"\n",
    "        bin_edges = np.percentile(reference, np.linspace(0, 100, bins + 1))\n",
    "        bin_edges[0], bin_edges[-1] = -np.inf, np.inf\n",
    "        \n",
    "        ref_counts, _ = np.histogram(reference, bins=bin_edges)\n",
    "        cur_counts, _ = np.histogram(current, bins=bin_edges)\n",
    "        \n",
    "        ref_pct = np.clip(ref_counts / len(reference), 0.0001, 1)\n",
    "        cur_pct = np.clip(cur_counts / len(current), 0.0001, 1)\n",
    "        \n",
    "        psi_value = np.sum((cur_pct - ref_pct) * np.log(cur_pct / ref_pct))\n",
    "        \n",
    "        severity = 'low' if psi_value < 0.1 else ('medium' if psi_value < 0.2 else 'high')\n",
    "        return {\n",
    "            'test': 'psi',\n",
    "            'statistic': psi_value,\n",
    "            'drift_detected': psi_value >= 0.1,\n",
    "            'severity': severity\n",
    "        }\n",
    "    \n",
    "    def chi_square_test(self, reference: np.ndarray, current: np.ndarray) -> Dict:\n",
    "        \"\"\"Chi-square test for categorical features.\"\"\"\n",
    "        categories = np.unique(np.concatenate([reference, current]))\n",
    "        ref_counts = np.array([np.sum(reference == c) for c in categories])\n",
    "        cur_counts = np.array([np.sum(current == c) for c in categories])\n",
    "        \n",
    "        expected = np.maximum((ref_counts / len(reference)) * len(current), 1)\n",
    "        statistic, p_value = stats.chisquare(cur_counts, expected)\n",
    "        \n",
    "        return {\n",
    "            'test': 'chi_square',\n",
    "            'statistic': statistic,\n",
    "            'p_value': p_value,\n",
    "            'drift_detected': p_value < self.significance_level\n",
    "        }\n",
    "\n",
    "detector = DriftDetector()\n",
    "print('DriftDetector created!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate drift detection\n",
    "np.random.seed(42)\n",
    "reference = np.random.normal(50, 10, 1000)\n",
    "no_drift = np.random.normal(50, 10, 1000)\n",
    "mild_drift = np.random.normal(55, 10, 1000)\n",
    "severe_drift = np.random.normal(65, 15, 1000)\n",
    "\n",
    "print('DRIFT DETECTION RESULTS')\n",
    "print('=' * 60)\n",
    "\n",
    "for name, data in [('No Drift', no_drift), ('Mild Drift', mild_drift), ('Severe Drift', severe_drift)]:\n",
    "    ks = detector.ks_test(reference, data)\n",
    "    psi = detector.psi(reference, data)\n",
    "    print(f'\\n{name}:')\n",
    "    print(f'  KS: {ks[\"statistic\"]:.4f}, drift={ks[\"drift_detected\"]}')\n",
    "    print(f'  PSI: {psi[\"statistic\"]:.4f} ({psi[\"severity\"]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize drift detection\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "datasets = [('No Drift', no_drift), ('Mild Drift', mild_drift), ('Severe Drift', severe_drift)]\n",
    "\n",
    "for idx, (name, data) in enumerate(datasets):\n",
    "    ax = axes[0, idx]\n",
    "    ax.hist(reference, bins=50, alpha=0.5, label='Reference', color='blue', density=True)\n",
    "    ax.hist(data, bins=50, alpha=0.5, label='Current', color='red', density=True)\n",
    "    psi_val = detector.psi(reference, data)['statistic']\n",
    "    ax.set_title(f'{name} (PSI: {psi_val:.3f})')\n",
    "    ax.legend()\n",
    "\n",
    "# PSI comparison\n",
    "ax = axes[1, 0]\n",
    "psi_vals = [detector.psi(reference, d)['statistic'] for _, d in datasets]\n",
    "colors = ['green' if p < 0.1 else 'orange' if p < 0.2 else 'red' for p in psi_vals]\n",
    "ax.bar([n for n, _ in datasets], psi_vals, color=colors)\n",
    "ax.axhline(0.1, color='orange', linestyle='--', label='Moderate')\n",
    "ax.axhline(0.2, color='red', linestyle='--', label='Severe')\n",
    "ax.set_title('PSI Comparison')\n",
    "ax.legend()\n",
    "\n",
    "# KS comparison\n",
    "ax = axes[1, 1]\n",
    "ks_vals = [detector.ks_test(reference, d)['statistic'] for _, d in datasets]\n",
    "ax.bar([n for n, _ in datasets], ks_vals, color='steelblue')\n",
    "ax.set_title('KS Statistic')\n",
    "\n",
    "# Thresholds guide\n",
    "ax = axes[1, 2]\n",
    "thresholds = ['PSI < 0.1', '0.1 <= PSI < 0.2', 'PSI >= 0.2']\n",
    "interpretations = ['No Drift', 'Moderate Drift', 'Severe Drift']\n",
    "colors = ['green', 'orange', 'red']\n",
    "ax.barh(thresholds, [1, 1, 1], color=colors)\n",
    "ax.set_title('PSI Interpretation Guide')\n",
    "ax.set_xlim(0, 1.5)\n",
    "for i, interp in enumerate(interpretations):\n",
    "    ax.text(1.1, i, interp, va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature-Level Drift Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureDriftMonitor:\n",
    "    def __init__(self, feature_names: List[str]):\n",
    "        self.feature_names = feature_names\n",
    "        self.detector = DriftDetector()\n",
    "        self.reference_data = None\n",
    "    \n",
    "    def set_reference(self, df: pd.DataFrame):\n",
    "        self.reference_data = df[self.feature_names].copy()\n",
    "    \n",
    "    def detect_drift(self, current_df: pd.DataFrame) -> Dict[str, Dict]:\n",
    "        results = {}\n",
    "        for feature in self.feature_names:\n",
    "            ref = self.reference_data[feature].values\n",
    "            cur = current_df[feature].values\n",
    "            \n",
    "            ks = self.detector.ks_test(ref, cur)\n",
    "            psi = self.detector.psi(ref, cur)\n",
    "            \n",
    "            results[feature] = {\n",
    "                'ks_statistic': ks['statistic'],\n",
    "                'psi': psi['statistic'],\n",
    "                'severity': psi['severity'],\n",
    "                'drift_detected': ks['drift_detected'] or psi['drift_detected']\n",
    "            }\n",
    "        return results\n",
    "\n",
    "# Create sample data\n",
    "np.random.seed(42)\n",
    "n = 2000\n",
    "\n",
    "train_df = pd.DataFrame({\n",
    "    'age': np.random.normal(35, 10, n),\n",
    "    'income': np.random.normal(50000, 20000, n),\n",
    "    'amount': np.random.exponential(100, n)\n",
    "})\n",
    "\n",
    "prod_df = pd.DataFrame({\n",
    "    'age': np.random.normal(40, 12, n),  # Drift\n",
    "    'income': np.random.normal(50000, 20000, n),  # No drift\n",
    "    'amount': np.random.exponential(150, n)  # Drift\n",
    "})\n",
    "\n",
    "monitor = FeatureDriftMonitor(['age', 'income', 'amount'])\n",
    "monitor.set_reference(train_df)\n",
    "results = monitor.detect_drift(prod_df)\n",
    "\n",
    "print('FEATURE DRIFT ANALYSIS')\n",
    "print('=' * 50)\n",
    "for feat, res in results.items():\n",
    "    status = 'DRIFT' if res['drift_detected'] else 'OK'\n",
    "    print(f\"{feat}: [{status}] PSI={res['psi']:.3f} ({res['severity']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature drift\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for idx, feat in enumerate(['age', 'income', 'amount']):\n",
    "    ax = axes[idx]\n",
    "    ax.hist(train_df[feat], bins=50, alpha=0.5, label='Train', color='blue', density=True)\n",
    "    ax.hist(prod_df[feat], bins=50, alpha=0.5, label='Prod', color='red', density=True)\n",
    "    status = 'DRIFT' if results[feat]['drift_detected'] else 'OK'\n",
    "    ax.set_title(f\"{feat} [{status}] - PSI: {results[feat]['psi']:.3f}\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Performance Degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceTracker:\n",
    "    def __init__(self, baseline: Dict[str, float], thresholds: Dict[str, float]):\n",
    "        self.baseline = baseline\n",
    "        self.thresholds = thresholds\n",
    "        self.history = []\n",
    "    \n",
    "    def record(self, timestamp: datetime, metrics: Dict[str, float]):\n",
    "        entry = {'timestamp': timestamp}\n",
    "        for metric, value in metrics.items():\n",
    "            entry[metric] = value\n",
    "            if metric in self.baseline:\n",
    "                entry[f'{metric}_drop'] = self.baseline[metric] - value\n",
    "                entry[f'{metric}_degraded'] = entry[f'{metric}_drop'] > self.thresholds.get(metric, 0.05)\n",
    "        self.history.append(entry)\n",
    "    \n",
    "    def get_report(self) -> str:\n",
    "        if not self.history:\n",
    "            return 'No data'\n",
    "        latest = self.history[-1]\n",
    "        lines = ['DEGRADATION REPORT', '=' * 40]\n",
    "        for metric in self.baseline:\n",
    "            if metric in latest:\n",
    "                status = 'DEGRADED' if latest.get(f'{metric}_degraded', False) else 'OK'\n",
    "                lines.append(f\"{metric}: {latest[metric]:.3f} (baseline: {self.baseline[metric]:.3f}) [{status}]\")\n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "# Simulate performance over time\n",
    "baseline = {'accuracy': 0.92, 'f1': 0.88}\n",
    "thresholds = {'accuracy': 0.03, 'f1': 0.05}\n",
    "tracker = PerformanceTracker(baseline, thresholds)\n",
    "\n",
    "start = datetime(2024, 1, 1)\n",
    "for day in range(90):\n",
    "    degradation = 0.001 * day + (0.1 if day > 45 else 0)\n",
    "    metrics = {\n",
    "        'accuracy': max(0.92 - degradation + np.random.normal(0, 0.01), 0.75),\n",
    "        'f1': max(0.88 - degradation * 1.2 + np.random.normal(0, 0.01), 0.70)\n",
    "    }\n",
    "    tracker.record(start + timedelta(days=day), metrics)\n",
    "\n",
    "print(tracker.get_report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize degradation\n",
    "df = pd.DataFrame(tracker.history)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(df['timestamp'], df['accuracy'], label='Accuracy', color='blue')\n",
    "ax.plot(df['timestamp'], df['f1'], label='F1', color='green')\n",
    "ax.axvline(datetime(2024, 2, 15), color='red', linestyle='--', label='Drift')\n",
    "ax.axhline(baseline['accuracy'] - thresholds['accuracy'], color='blue', linestyle=':', alpha=0.5)\n",
    "ax.set_title('Performance Over Time')\n",
    "ax.legend()\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.fill_between(df['timestamp'], baseline['accuracy'], baseline['accuracy'] - thresholds['accuracy'],\n",
    "                alpha=0.3, color='green', label='Acceptable')\n",
    "ax.fill_between(df['timestamp'], baseline['accuracy'] - thresholds['accuracy'], 0.7,\n",
    "                alpha=0.3, color='red', label='Degraded')\n",
    "ax.plot(df['timestamp'], df['accuracy'], color='blue', linewidth=2)\n",
    "ax.set_title('Accuracy with Thresholds')\n",
    "ax.legend()\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Retraining Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrainingStrategy(Enum):\n",
    "    SCHEDULED = 'scheduled'\n",
    "    PERFORMANCE_TRIGGERED = 'performance_triggered'\n",
    "    DRIFT_TRIGGERED = 'drift_triggered'\n",
    "\n",
    "@dataclass\n",
    "class RetrainingConfig:\n",
    "    strategy: RetrainingStrategy\n",
    "    schedule_days: Optional[int] = None\n",
    "    performance_threshold: Optional[float] = None\n",
    "    drift_threshold: Optional[float] = None\n",
    "\n",
    "class RetrainingManager:\n",
    "    def __init__(self, config: RetrainingConfig):\n",
    "        self.config = config\n",
    "        self.last_retrain = None\n",
    "        self.history = []\n",
    "    \n",
    "    def should_retrain(self, metrics: Dict, drift_scores: Dict, \n",
    "                       current_date: datetime) -> Tuple[bool, str]:\n",
    "        if self.config.strategy == RetrainingStrategy.SCHEDULED:\n",
    "            if self.last_retrain is None:\n",
    "                return True, 'Initial training'\n",
    "            days = (current_date - self.last_retrain).days\n",
    "            if days >= self.config.schedule_days:\n",
    "                return True, f'Scheduled ({days} days)'\n",
    "        \n",
    "        elif self.config.strategy == RetrainingStrategy.PERFORMANCE_TRIGGERED:\n",
    "            for metric, value in metrics.items():\n",
    "                if value < self.config.performance_threshold:\n",
    "                    return True, f'Performance ({metric}={value:.3f})'\n",
    "        \n",
    "        elif self.config.strategy == RetrainingStrategy.DRIFT_TRIGGERED:\n",
    "            for feature, score in drift_scores.items():\n",
    "                if score > self.config.drift_threshold:\n",
    "                    return True, f'Drift in {feature} (PSI={score:.3f})'\n",
    "        \n",
    "        return False, 'No retraining needed'\n",
    "    \n",
    "    def record_retrain(self, date: datetime, reason: str, before: Dict, after: Dict):\n",
    "        self.last_retrain = date\n",
    "        self.history.append({'date': date, 'reason': reason, 'before': before, 'after': after})\n",
    "\n",
    "# Demo\n",
    "config = RetrainingConfig(RetrainingStrategy.PERFORMANCE_TRIGGERED, performance_threshold=0.85)\n",
    "manager = RetrainingManager(config)\n",
    "manager.last_retrain = datetime(2024, 1, 1)\n",
    "\n",
    "should, reason = manager.should_retrain({'accuracy': 0.82}, {}, datetime(2024, 2, 1))\n",
    "print(f'Should retrain: {should}')\n",
    "print(f'Reason: {reason}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate retraining decisions\n",
    "manager.last_retrain = datetime(2024, 1, 1)\n",
    "decisions = []\n",
    "\n",
    "for day in range(90):\n",
    "    date = datetime(2024, 1, 1) + timedelta(days=day)\n",
    "    accuracy = max(0.92 - 0.001 * day - (0.1 if day > 45 else 0), 0.75)\n",
    "    \n",
    "    should, reason = manager.should_retrain({'accuracy': accuracy}, {}, date)\n",
    "    \n",
    "    if should and (not manager.history or (date - manager.history[-1]['date']).days > 7):\n",
    "        manager.record_retrain(date, reason, {'accuracy': accuracy}, {'accuracy': 0.92})\n",
    "    \n",
    "    decisions.append({'date': date, 'accuracy': accuracy, 'retrain': should})\n",
    "\n",
    "print('RETRAINING EVENTS')\n",
    "print('=' * 50)\n",
    "for event in manager.history:\n",
    "    print(f\"{event['date'].strftime('%Y-%m-%d')}: {event['reason']}\")\n",
    "    print(f\"  Accuracy: {event['before']['accuracy']:.3f} -> {event['after']['accuracy']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complete Drift Detection Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DriftPipeline:\n",
    "    def __init__(self, features: List[str], psi_threshold: float = 0.15):\n",
    "        self.monitor = FeatureDriftMonitor(features)\n",
    "        self.psi_threshold = psi_threshold\n",
    "        self.reference_labels = None\n",
    "    \n",
    "    def set_reference(self, df: pd.DataFrame, labels: np.ndarray):\n",
    "        self.monitor.set_reference(df)\n",
    "        self.reference_labels = labels\n",
    "        self.ref_positive_rate = np.mean(labels)\n",
    "    \n",
    "    def analyze(self, df: pd.DataFrame, labels: np.ndarray = None) -> Dict:\n",
    "        results = {'timestamp': datetime.now(), 'feature_drift': {}, 'alerts': []}\n",
    "        \n",
    "        # Feature drift\n",
    "        feature_results = self.monitor.detect_drift(df)\n",
    "        for feat, res in feature_results.items():\n",
    "            results['feature_drift'][feat] = res\n",
    "            if res['drift_detected']:\n",
    "                results['alerts'].append(f'Feature drift: {feat} (PSI={res[\"psi\"]:.3f})')\n",
    "        \n",
    "        # Label drift\n",
    "        if labels is not None and self.reference_labels is not None:\n",
    "            cur_rate = np.mean(labels)\n",
    "            shift = abs(cur_rate - self.ref_positive_rate)\n",
    "            results['label_drift'] = {'ref': self.ref_positive_rate, 'cur': cur_rate, 'shift': shift}\n",
    "            if shift > 0.05:\n",
    "                results['alerts'].append(f'Label drift: {self.ref_positive_rate:.2f} -> {cur_rate:.2f}')\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def report(self, results: Dict) -> str:\n",
    "        lines = ['=' * 50, 'DRIFT DETECTION REPORT', '=' * 50]\n",
    "        lines.append('\\nFeature Drift:')\n",
    "        for feat, res in results['feature_drift'].items():\n",
    "            status = 'DRIFT' if res['drift_detected'] else 'OK'\n",
    "            lines.append(f\"  {feat}: [{status}] PSI={res['psi']:.3f}\")\n",
    "        \n",
    "        if 'label_drift' in results:\n",
    "            ld = results['label_drift']\n",
    "            lines.append(f\"\\nLabel Drift: {ld['ref']:.2f} -> {ld['cur']:.2f} (shift: {ld['shift']:.2f})\")\n",
    "        \n",
    "        lines.append('\\nAlerts:')\n",
    "        for alert in results['alerts']:\n",
    "            lines.append(f\"  [!] {alert}\")\n",
    "        if not results['alerts']:\n",
    "            lines.append('  No alerts')\n",
    "        \n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "# Run pipeline\n",
    "pipeline = DriftPipeline(['age', 'income', 'amount'])\n",
    "ref_labels = np.random.choice([0, 1], 2000, p=[0.9, 0.1])\n",
    "pipeline.set_reference(train_df, ref_labels)\n",
    "\n",
    "cur_labels = np.random.choice([0, 1], 2000, p=[0.85, 0.15])\n",
    "results = pipeline.analyze(prod_df, cur_labels)\n",
    "print(pipeline.report(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hands-on Exercise\n",
    "\n",
    "Build a fraud detection drift monitor:\n",
    "1. Generate synthetic transaction data with drift\n",
    "2. Train a classifier and track performance\n",
    "3. Implement automatic drift alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudDriftMonitor:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.reference = None\n",
    "        self.detector = DriftDetector()\n",
    "        self.baseline_acc = None\n",
    "    \n",
    "    def train(self, X: pd.DataFrame, y: np.ndarray):\n",
    "        self.reference = X.copy()\n",
    "        self.model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "        self.model.fit(X, y)\n",
    "        self.baseline_acc = accuracy_score(y, self.model.predict(X))\n",
    "        print(f'Trained. Baseline accuracy: {self.baseline_acc:.3f}')\n",
    "    \n",
    "    def monitor(self, X: pd.DataFrame, y: np.ndarray) -> Dict:\n",
    "        results = {'drift': [], 'accuracy': None, 'alerts': []}\n",
    "        \n",
    "        # Check drift per feature\n",
    "        for col in self.reference.columns:\n",
    "            psi = self.detector.psi(self.reference[col].values, X[col].values)\n",
    "            results['drift'].append({'feature': col, 'psi': psi['statistic']})\n",
    "            if psi['drift_detected']:\n",
    "                results['alerts'].append(f'Drift in {col}: PSI={psi[\"statistic\"]:.3f}')\n",
    "        \n",
    "        # Check accuracy\n",
    "        pred = self.model.predict(X)\n",
    "        results['accuracy'] = accuracy_score(y, pred)\n",
    "        if results['accuracy'] < self.baseline_acc - 0.05:\n",
    "            results['alerts'].append(f'Accuracy drop: {self.baseline_acc:.3f} -> {results[\"accuracy\"]:.3f}')\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Generate data\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "X_train = pd.DataFrame({\n",
    "    'amount': np.random.exponential(100, n),\n",
    "    'hour': np.random.randint(0, 24, n),\n",
    "    'distance': np.random.exponential(50, n)\n",
    "})\n",
    "y_train = (X_train['amount'] > 150).astype(int) | (np.random.random(n) < 0.05)\n",
    "\n",
    "# Train\n",
    "monitor = FraudDriftMonitor()\n",
    "monitor.train(X_train, y_train)\n",
    "\n",
    "# Production data with drift\n",
    "X_prod = pd.DataFrame({\n",
    "    'amount': np.random.exponential(150, n),  # Drift!\n",
    "    'hour': np.random.randint(0, 24, n),\n",
    "    'distance': np.random.exponential(50, n)\n",
    "})\n",
    "y_prod = (X_prod['amount'] > 150).astype(int) | (np.random.random(n) < 0.1)\n",
    "\n",
    "results = monitor.monitor(X_prod, y_prod)\n",
    "print('\\nMonitoring Results:')\n",
    "print(f'Accuracy: {results[\"accuracy\"]:.3f}')\n",
    "print('\\nAlerts:')\n",
    "for alert in results['alerts']:\n",
    "    print(f'  [!] {alert}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Types of Drift**: Covariate (P(X) changes), Label (P(Y) changes), Concept (P(Y|X) changes)\n",
    "2. **Statistical Tests**: KS test for continuous, Chi-square for categorical, PSI for industry standard\n",
    "3. **PSI Thresholds**: <0.1 (no drift), 0.1-0.2 (moderate), >0.2 (severe)\n",
    "4. **Retraining Strategies**: Scheduled, performance-triggered, drift-triggered\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- Monitor ALL features, not just model inputs\n",
    "- Use multiple detection methods (KS + PSI)\n",
    "- Set up alerts before drift becomes critical\n",
    "- Automate retraining pipelines\n",
    "- Keep baseline/reference data versioned\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next module, we will explore **ML Infrastructure** including experiment tracking, feature stores, and MLOps practices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
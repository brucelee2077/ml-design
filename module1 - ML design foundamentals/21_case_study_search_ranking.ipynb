{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 21: Case Study - Search Ranking System\n",
    "\n",
    "## End-to-End ML System Design for Search Relevance\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "\n",
    "1. **Design a search ranking system** using the 7-step ML framework\n",
    "2. **Implement Learning-to-Rank models** (pointwise, pairwise, listwise)\n",
    "3. **Engineer query and document features** for search relevance\n",
    "4. **Apply ranking metrics** (NDCG, MRR, Precision@K)\n",
    "5. **Build a two-stage retrieval and ranking pipeline**\n",
    "6. **Design A/B testing** for search quality evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestRegressor\n",
    "from collections import defaultdict\n",
    "\n",
    "np.random.seed(42)\n",
    "print('All imports successful!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Problem Statement and Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchRequirements:\n",
    "    def __init__(self):\n",
    "        self.business = {'primary': 'Maximize relevant product discovery'}\n",
    "        self.scale = {'documents': '100M', 'qps': '100K'}\n",
    "        self.latency = {'total': 'p99 < 200ms'}\n",
    "        \n",
    "    def display(self):\n",
    "        print('SEARCH RANKING REQUIREMENTS')\n",
    "        print('=' * 40)\n",
    "        print(f\"Business: {self.business}\")\n",
    "        print(f\"Scale: {self.scale}\")\n",
    "        print(f\"Latency: {self.latency}\")\n",
    "\n",
    "req = SearchRequirements()\n",
    "req.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Two-Stage Architecture\n",
    "\n",
    "```\n",
    "Query -> Retrieval (100M -> 1000) -> Ranking (1000 -> 20) -> Results\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.set_xlim(0, 12)\n",
    "ax.set_ylim(0, 4)\n",
    "ax.axis('off')\n",
    "\n",
    "boxes = [\n",
    "    (0.5, 1, 1.5, 2, 'Query', '#3498db'),\n",
    "    (2.5, 0.5, 2.5, 3, 'Retrieval\\n100M->1K', '#2ecc71'),\n",
    "    (5.5, 0.5, 2.5, 3, 'Ranking\\n1K->20', '#e74c3c'),\n",
    "    (8.5, 0.5, 2.5, 3, 'Re-rank\\nBusiness', '#f39c12'),\n",
    "    (11.5, 1, 1, 2, 'Results', '#9b59b6')\n",
    "]\n",
    "\n",
    "for x, y, w, h, label, color in boxes:\n",
    "    rect = plt.Rectangle((x, y), w, h, facecolor=color, alpha=0.3, edgecolor=color, lw=2)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x + w/2, y + h/2, label, ha='center', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "for x1, x2 in [(2, 2.5), (5, 5.5), (8, 8.5), (11, 11.5)]:\n",
    "    ax.annotate('', xy=(x2, 2), xytext=(x1, 2), arrowprops=dict(arrowstyle='->', lw=2))\n",
    "\n",
    "ax.set_title('Two-Stage Search Architecture', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchDataGenerator:\n",
    "    def __init__(self, n_queries=2000, n_docs=3000):\n",
    "        self.n_queries = n_queries\n",
    "        self.n_docs = n_docs\n",
    "        self.categories = ['electronics', 'clothing', 'home', 'sports', 'books']\n",
    "        self.keywords = {\n",
    "            'electronics': ['laptop', 'phone', 'tablet'],\n",
    "            'clothing': ['shirt', 'pants', 'shoes'],\n",
    "            'home': ['furniture', 'lamp', 'rug'],\n",
    "            'sports': ['running', 'yoga', 'weights'],\n",
    "            'books': ['fiction', 'science', 'history']\n",
    "        }\n",
    "        \n",
    "    def generate_documents(self):\n",
    "        np.random.seed(42)\n",
    "        docs = []\n",
    "        for i in range(self.n_docs):\n",
    "            cat = np.random.choice(self.categories)\n",
    "            kw = np.random.choice(self.keywords[cat])\n",
    "            docs.append({\n",
    "                'doc_id': f'doc_{i}',\n",
    "                'title': f'{kw.title()} Product {i}',\n",
    "                'category': cat,\n",
    "                'price': round(np.random.exponential(50) + 10, 2),\n",
    "                'rating': round(np.clip(np.random.normal(4.0, 0.5), 1, 5), 1),\n",
    "                'num_reviews': int(np.random.exponential(100)),\n",
    "                'keywords': [kw]\n",
    "            })\n",
    "        return pd.DataFrame(docs)\n",
    "    \n",
    "    def generate_queries(self):\n",
    "        np.random.seed(43)\n",
    "        queries = []\n",
    "        for i in range(self.n_queries):\n",
    "            cat = np.random.choice(self.categories)\n",
    "            kw = np.random.choice(self.keywords[cat])\n",
    "            queries.append({\n",
    "                'query_id': f'q_{i}',\n",
    "                'query_text': kw,\n",
    "                'category_intent': cat\n",
    "            })\n",
    "        return pd.DataFrame(queries)\n",
    "    \n",
    "    def generate_pairs(self, queries, docs):\n",
    "        np.random.seed(44)\n",
    "        pairs = []\n",
    "        doc_by_cat = docs.groupby('category')['doc_id'].apply(list).to_dict()\n",
    "        \n",
    "        for _, query in queries.iterrows():\n",
    "            cat = query['category_intent']\n",
    "            qwords = set(query['query_text'].lower().split())\n",
    "            \n",
    "            relevant = doc_by_cat.get(cat, [])\n",
    "            others = [d for c, dl in doc_by_cat.items() if c != cat for d in dl]\n",
    "            \n",
    "            selected = list(np.random.choice(relevant, min(20, len(relevant)), replace=False))\n",
    "            selected += list(np.random.choice(others, min(10, len(others)), replace=False))\n",
    "            \n",
    "            for pos, doc_id in enumerate(selected, 1):\n",
    "                doc = docs[docs['doc_id'] == doc_id].iloc[0]\n",
    "                dwords = set(doc['title'].lower().split())\n",
    "                \n",
    "                kw_match = len(qwords & dwords) / len(qwords) if qwords else 0\n",
    "                cat_match = 1 if doc['category'] == cat else 0\n",
    "                relevance = int(np.clip(kw_match * 2 + cat_match * 2 + np.random.normal(0, 0.3), 0, 4))\n",
    "                clicked = np.random.random() < (relevance / 4) * (1 / np.log2(pos + 1))\n",
    "                \n",
    "                pairs.append({\n",
    "                    'query_id': query['query_id'],\n",
    "                    'doc_id': doc_id,\n",
    "                    'relevance': relevance,\n",
    "                    'clicked': clicked,\n",
    "                    'position': pos,\n",
    "                    'keyword_match': kw_match,\n",
    "                    'category_match': cat_match\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(pairs)\n",
    "    \n",
    "    def generate_all(self):\n",
    "        docs = self.generate_documents()\n",
    "        queries = self.generate_queries()\n",
    "        pairs = self.generate_pairs(queries, docs)\n",
    "        print(f'Generated: {len(docs)} docs, {len(queries)} queries, {len(pairs)} pairs')\n",
    "        return docs, queries, pairs\n",
    "\n",
    "gen = SearchDataGenerator()\n",
    "docs_df, queries_df, pairs_df = gen.generate_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Documents:')\n",
    "print(docs_df.head())\n",
    "print('\\nQueries:')\n",
    "print(queries_df.head())\n",
    "print('\\nPairs:')\n",
    "print(pairs_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "axes[0].hist(pairs_df['relevance'], bins=5, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Relevance Distribution')\n",
    "\n",
    "ctr = pairs_df.groupby('relevance')['clicked'].mean()\n",
    "axes[1].bar(ctr.index, ctr.values, color='green', alpha=0.7)\n",
    "axes[1].set_title('CTR by Relevance')\n",
    "\n",
    "pos_ctr = pairs_df.groupby('position')['clicked'].mean()\n",
    "axes[2].plot(pos_ctr.index[:15], pos_ctr.values[:15], marker='o')\n",
    "axes[2].set_title('Position Bias')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineering:\n",
    "    def __init__(self):\n",
    "        self.tfidf = TfidfVectorizer(max_features=500)\n",
    "        \n",
    "    def extract(self, pairs, queries, docs):\n",
    "        data = pairs.merge(queries, on='query_id')\n",
    "        data = data.merge(docs, on='doc_id')\n",
    "        \n",
    "        features = pd.DataFrame()\n",
    "        features['keyword_match'] = data['keyword_match']\n",
    "        features['category_match'] = data['category_match']\n",
    "        features['rating'] = data['rating']\n",
    "        features['log_reviews'] = np.log1p(data['num_reviews'])\n",
    "        features['log_price'] = np.log1p(data['price'])\n",
    "        features['query_len'] = data['query_text'].apply(lambda x: len(x.split()))\n",
    "        \n",
    "        return features, data['relevance'], data['query_id']\n",
    "\n",
    "fe = FeatureEngineering()\n",
    "X, y, qids = fe.extract(pairs_df, queries_df, docs_df)\n",
    "print(f'Features: {X.shape}')\n",
    "print(f'Columns: {list(X.columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_q = qids.unique()\n",
    "np.random.shuffle(unique_q)\n",
    "split = int(len(unique_q) * 0.8)\n",
    "\n",
    "train_q = set(unique_q[:split])\n",
    "test_q = set(unique_q[split:])\n",
    "\n",
    "train_mask = qids.isin(train_q)\n",
    "test_mask = qids.isin(test_q)\n",
    "\n",
    "X_train, X_test = X[train_mask], X[test_mask]\n",
    "y_train, y_test = y[train_mask].values, y[test_mask].values\n",
    "qids_train, qids_test = qids[train_mask].values, qids[test_mask].values\n",
    "\n",
    "print(f'Train: {len(X_train)}, Test: {len(X_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Model Development\n",
    "\n",
    "## 3.1 BM25 Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25:\n",
    "    def __init__(self):\n",
    "        self.tfidf = TfidfVectorizer()\n",
    "        self.doc_vecs = None\n",
    "        self.doc_ids = None\n",
    "        \n",
    "    def fit(self, docs):\n",
    "        print('Fitting BM25...')\n",
    "        self.doc_ids = docs['doc_id'].values\n",
    "        self.doc_vecs = self.tfidf.fit_transform(docs['title'])\n",
    "        \n",
    "    def rank(self, query, k=100):\n",
    "        q_vec = self.tfidf.transform([query])\n",
    "        scores = cosine_similarity(q_vec, self.doc_vecs)[0]\n",
    "        top_idx = np.argsort(scores)[::-1][:k]\n",
    "        return [(self.doc_ids[i], scores[i]) for i in top_idx]\n",
    "\n",
    "bm25 = BM25()\n",
    "bm25.fit(docs_df)\n",
    "\n",
    "print('Top results for \"laptop\":')\n",
    "for doc_id, score in bm25.rank('laptop', 5):\n",
    "    title = docs_df[docs_df['doc_id'] == doc_id]['title'].values[0]\n",
    "    print(f'  {title}: {score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Pointwise Ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointwiseRanker:\n",
    "    def __init__(self):\n",
    "        self.model = GradientBoostingClassifier(n_estimators=100, max_depth=5)\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        print('Training Pointwise Ranker...')\n",
    "        Xs = self.scaler.fit_transform(X)\n",
    "        yb = (y >= 2).astype(int)\n",
    "        self.model.fit(Xs, yb)\n",
    "        print(f'  Accuracy: {self.model.score(Xs, yb):.4f}')\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.model.predict_proba(self.scaler.transform(X))[:, 1]\n",
    "\n",
    "pointwise = PointwiseRanker()\n",
    "pointwise.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 LambdaMART-style Ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambdaMARTRanker:\n",
    "    def __init__(self):\n",
    "        self.model = RandomForestRegressor(n_estimators=100, max_depth=10)\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        print('Training LambdaMART Ranker...')\n",
    "        Xs = self.scaler.fit_transform(X)\n",
    "        self.model.fit(Xs, y)\n",
    "        print(f'  R2: {self.model.score(Xs, y):.4f}')\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(self.scaler.transform(X))\n",
    "    \n",
    "    def feature_importance(self, names):\n",
    "        return sorted(zip(names, self.model.feature_importances_), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "lambdamart = LambdaMARTRanker()\n",
    "lambdamart.fit(X_train, y_train)\n",
    "\n",
    "print('\\nFeature Importance:')\n",
    "for n, i in lambdamart.feature_importance(X.columns)[:5]:\n",
    "    print(f'  {n}: {i:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Evaluation\n",
    "\n",
    "## 4.1 Ranking Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankingMetrics:\n",
    "    @staticmethod\n",
    "    def dcg(rels, k):\n",
    "        rels = np.array(rels)[:k]\n",
    "        return np.sum(rels / np.log2(np.arange(2, len(rels) + 2)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def ndcg(rels, k):\n",
    "        dcg = RankingMetrics.dcg(rels, k)\n",
    "        idcg = RankingMetrics.dcg(sorted(rels, reverse=True), k)\n",
    "        return dcg / idcg if idcg > 0 else 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def mrr(rels, threshold=2):\n",
    "        for i, r in enumerate(rels):\n",
    "            if r >= threshold:\n",
    "                return 1 / (i + 1)\n",
    "        return 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def precision_at_k(rels, k, threshold=2):\n",
    "        return np.sum(np.array(rels)[:k] >= threshold) / k\n",
    "\n",
    "m = RankingMetrics()\n",
    "test_rels = [4, 3, 0, 1, 2]\n",
    "print(f'NDCG@5: {m.ndcg(test_rels, 5):.4f}')\n",
    "print(f'MRR: {m.mrr(test_rels):.4f}')\n",
    "print(f'P@5: {m.precision_at_k(test_rels, 5):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(ranker, X, y, qids, ks=[5, 10]):\n",
    "    results = {k: {'ndcg': [], 'mrr': [], 'prec': []} for k in ks}\n",
    "    groups = defaultdict(list)\n",
    "    for i, q in enumerate(qids):\n",
    "        groups[q].append(i)\n",
    "    \n",
    "    for qid, idx in groups.items():\n",
    "        Xq = X.iloc[idx]\n",
    "        yq = y[idx]\n",
    "        scores = ranker.predict(Xq)\n",
    "        sorted_rels = yq[np.argsort(scores)[::-1]]\n",
    "        \n",
    "        for k in ks:\n",
    "            results[k]['ndcg'].append(m.ndcg(sorted_rels, k))\n",
    "            results[k]['mrr'].append(m.mrr(sorted_rels))\n",
    "            results[k]['prec'].append(m.precision_at_k(sorted_rels, k))\n",
    "    \n",
    "    return {k: {m: np.mean(v) for m, v in d.items()} for k, d in results.items()}\n",
    "\n",
    "print('Pointwise Results:')\n",
    "for k, v in evaluate(pointwise, X_test, y_test, qids_test).items():\n",
    "    print(f'  @{k}: NDCG={v[\"ndcg\"]:.4f}, MRR={v[\"mrr\"]:.4f}')\n",
    "\n",
    "print('\\nLambdaMART Results:')\n",
    "for k, v in evaluate(lambdamart, X_test, y_test, qids_test).items():\n",
    "    print(f'  @{k}: NDCG={v[\"ndcg\"]:.4f}, MRR={v[\"mrr\"]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchService:\n",
    "    def __init__(self, retriever, ranker, docs, fe):\n",
    "        self.retriever = retriever\n",
    "        self.ranker = ranker\n",
    "        self.docs = docs\n",
    "        self.fe = fe\n",
    "        \n",
    "    def search(self, query, k=10):\n",
    "        import time\n",
    "        start = time.time()\n",
    "        \n",
    "        # Retrieval\n",
    "        candidates = self.retriever.rank(query, 50)\n",
    "        cand_ids = [c[0] for c in candidates]\n",
    "        \n",
    "        # Create features\n",
    "        qdf = pd.DataFrame([{'query_id': 'q', 'query_text': query, 'category_intent': 'unknown'}])\n",
    "        pdf = pd.DataFrame([{'query_id': 'q', 'doc_id': d, 'relevance': 0, 'clicked': False,\n",
    "                            'position': i, 'keyword_match': 0, 'category_match': 0}\n",
    "                           for i, d in enumerate(cand_ids, 1)])\n",
    "        \n",
    "        qwords = set(query.lower().split())\n",
    "        for i, did in enumerate(cand_ids):\n",
    "            doc = self.docs[self.docs['doc_id'] == did].iloc[0]\n",
    "            dwords = set(doc['title'].lower().split())\n",
    "            pdf.loc[i, 'keyword_match'] = len(qwords & dwords) / len(qwords) if qwords else 0\n",
    "        \n",
    "        Xr, _, _ = self.fe.extract(pdf, qdf, self.docs)\n",
    "        scores = self.ranker.predict(Xr)\n",
    "        \n",
    "        ranked = sorted(zip(cand_ids, scores), key=lambda x: x[1], reverse=True)[:k]\n",
    "        latency = (time.time() - start) * 1000\n",
    "        \n",
    "        results = []\n",
    "        for did, score in ranked:\n",
    "            doc = self.docs[self.docs['doc_id'] == did].iloc[0]\n",
    "            results.append({'doc_id': did, 'title': doc['title'], 'score': float(score)})\n",
    "        \n",
    "        return {'query': query, 'results': results, 'latency_ms': round(latency, 2)}\n",
    "\n",
    "service = SearchService(bm25, lambdamart, docs_df, fe)\n",
    "\n",
    "response = service.search('laptop', 5)\n",
    "print(f'Search: \"{response[\"query\"]}\" ({response[\"latency_ms\"]}ms)')\n",
    "for i, r in enumerate(response['results'], 1):\n",
    "    print(f\"  {i}. {r['title']} ({r['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 A/B Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABTest:\n",
    "    def __init__(self, control_ndcg=0.6, lift=0.05):\n",
    "        self.control = control_ndcg\n",
    "        self.treatment = control_ndcg * (1 + lift)\n",
    "        \n",
    "    def simulate(self, n=5000):\n",
    "        np.random.seed(42)\n",
    "        results = []\n",
    "        for i in range(n):\n",
    "            group = np.random.choice(['control', 'treatment'])\n",
    "            base = self.control if group == 'control' else self.treatment\n",
    "            ndcg = np.clip(np.random.normal(base, 0.15), 0, 1)\n",
    "            results.append({'group': group, 'ndcg': ndcg})\n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def analyze(self, df):\n",
    "        ctrl = df[df['group'] == 'control']['ndcg']\n",
    "        treat = df[df['group'] == 'treatment']['ndcg']\n",
    "        lift = (treat.mean() - ctrl.mean()) / ctrl.mean() * 100\n",
    "        from scipy.stats import ttest_ind\n",
    "        _, p = ttest_ind(ctrl, treat)\n",
    "        return {'control': ctrl.mean(), 'treatment': treat.mean(), 'lift': lift, 'p': p, 'sig': p < 0.05}\n",
    "\n",
    "ab = ABTest()\n",
    "res = ab.simulate()\n",
    "analysis = ab.analyze(res)\n",
    "\n",
    "print('A/B Test:')\n",
    "print(f\"  Control NDCG: {analysis['control']:.4f}\")\n",
    "print(f\"  Treatment NDCG: {analysis['treatment']:.4f}\")\n",
    "print(f\"  Lift: {analysis['lift']:.2f}%\")\n",
    "print(f\"  Significant: {analysis['sig']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchMonitor:\n",
    "    def __init__(self):\n",
    "        self.logs = []\n",
    "        \n",
    "    def log(self, query, n_results, latency, click_pos=None):\n",
    "        self.logs.append({\n",
    "            'ts': datetime.now(),\n",
    "            'query': query,\n",
    "            'n_results': n_results,\n",
    "            'latency': latency,\n",
    "            'click_pos': click_pos\n",
    "        })\n",
    "    \n",
    "    def metrics(self):\n",
    "        df = pd.DataFrame(self.logs)\n",
    "        clicked = df[df['click_pos'].notna()]\n",
    "        mrr = (1 / clicked['click_pos']).mean() if len(clicked) > 0 else 0\n",
    "        return {\n",
    "            'queries': len(df),\n",
    "            'latency_p50': df['latency'].quantile(0.5),\n",
    "            'latency_p99': df['latency'].quantile(0.99),\n",
    "            'ctr': len(clicked) / len(df),\n",
    "            'mrr': mrr\n",
    "        }\n",
    "\n",
    "monitor = SearchMonitor()\n",
    "\n",
    "for _ in range(50):\n",
    "    q = np.random.choice(['laptop', 'shoes', 'phone', 'book'])\n",
    "    r = service.search(q, 5)\n",
    "    click = np.random.choice([1, 2, 3, None], p=[0.2, 0.1, 0.05, 0.65])\n",
    "    monitor.log(q, len(r['results']), r['latency_ms'], click)\n",
    "\n",
    "met = monitor.metrics()\n",
    "print('Monitoring:')\n",
    "print(f\"  Queries: {met['queries']}\")\n",
    "print(f\"  Latency p50: {met['latency_p50']:.1f}ms\")\n",
    "print(f\"  CTR: {met['ctr']:.2%}\")\n",
    "print(f\"  MRR: {met['mrr']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "7-Step Framework for Search Ranking\n",
    "====================================\n",
    "\n",
    "Step 1: Requirements\n",
    "  - Scale: 100M docs, 100K QPS\n",
    "  - Latency: p99 < 200ms\n",
    "\n",
    "Step 2: Problem Framing  \n",
    "  - Two-stage: Retrieval + Ranking\n",
    "  - Learning-to-Rank approaches\n",
    "\n",
    "Step 3: Data\n",
    "  - Query-doc pairs with relevance\n",
    "  - Implicit feedback (clicks)\n",
    "\n",
    "Step 4: Models\n",
    "  - BM25 baseline\n",
    "  - Pointwise/Pairwise/Listwise\n",
    "  - LambdaMART\n",
    "\n",
    "Step 5: Evaluation\n",
    "  - NDCG, MRR, Precision@K\n",
    "\n",
    "Step 6: Deployment\n",
    "  - Retrieval -> Ranking pipeline\n",
    "  - A/B testing\n",
    "\n",
    "Step 7: Monitoring\n",
    "  - Latency, CTR, MRR tracking\n",
    "\"\"\")\n",
    "\n",
    "print('Tutorial 21 Complete!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}